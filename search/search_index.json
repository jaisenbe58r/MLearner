{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MLearner's documentation! MLearner is a Python library of useful tools for the day-to-day data science tasks. Links Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner Questions? Check out the Discord group MLearner Examples License MIT License Copyright (c) 2018-2022 Jaime Sendra Berenguer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contact I received a lot of feedback and questions about mlearner recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlearner, please consider posting it here since it can also be useful to others! Please join the Discord group MLearner If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Home"},{"location":"#welcome-to-mlearners-documentation","text":"MLearner is a Python library of useful tools for the day-to-day data science tasks.","title":"Welcome to MLearner's documentation!"},{"location":"#links","text":"Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner Questions? Check out the Discord group MLearner","title":"Links"},{"location":"#examples","text":"","title":"Examples"},{"location":"#license","text":"MIT License Copyright (c) 2018-2022 Jaime Sendra Berenguer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"#contact","text":"I received a lot of feedback and questions about mlearner recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlearner, please consider posting it here since it can also be useful to others! Please join the Discord group MLearner If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Contact"},{"location":"CHANGELOG/","text":"Release Notes The CHANGELOG for the current development version is available at https://github.com/jaisenbe58r/MLearner/blob/master/docs/sources/CHANGELOG.md . Version 0.1.0 (2018-2022-04-17) First Version","title":"Release Notes"},{"location":"CHANGELOG/#release-notes","text":"The CHANGELOG for the current development version is available at https://github.com/jaisenbe58r/MLearner/blob/master/docs/sources/CHANGELOG.md .","title":"Release Notes"},{"location":"CHANGELOG/#version-010-2018-2022-04-17","text":"First Version","title":"Version 0.1.0 (2018-2022-04-17)"},{"location":"CONTRIBUTING/","text":"How to Contribute I would be very happy about any kind of contributions that help to improve and extend the functionality of mlearner. Quick Contributor Checklist This is a quick checklist about the different steps of a typical contribution to mlearner (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go. [ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlearner repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlearner/*/tests [ ] Run PYTHONPATH='.' pytest ./mlearner -sv and make sure that all unit tests pass [ ] Check for style issues by running flake8 ./mlearner (you may want to run pytest again after you made modifications to the code) [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file [ ] Modify documentation in the appropriate location under mlearner/docs/sources/ [ ] Push the topic branch to the server and create a pull request [ ] Check the Travis-CI build passed at https://travis-ci.org/jaisenbe58r/mlearner [ ] Check/improve the unit test coverage at https://coveralls.io/github/jaisenbe58r/mlearner [ ] Check/improve the code health at https://landscape.io/github/jaisenbe58r/mlearner Tips for Contributors Getting Started - Creating a New Issue and Forking the Repository If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlearner repository from the GitHub web interface. Clone the mlearner repository to your local machine by executing git clone https://github.com/<your_username>/MLearner.git Syncing an Existing Fork If you already forked mlearner earlier, you can bring you \"Fork\" up to date with the master branch as follows: 1. Configuring a remote that points to the upstream repository on GitHub List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/jaisenbe58r/MLearner.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) upstream https://github.com/jaisenbe58r/MLearner.git (fetch) upstream https://github.com/jaisenbe58r/MLearner.git (push) 2. Syncing your Fork First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/jaisenbe58r/MLearner * [new branch] master -> upstream/master This means that the commits to the jaisenbe58r/mlearner master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+), *The Main Workflow - Making Changes in a New Topic Branch Listed below are the 9 typical steps of a contribution. 1. Discussing the Feature or Modification Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already. 2. Creating a new feature branch Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature> 3. Developing the new feature / bug fix Now it's time to modify existing code or to contribute new code to the project. 4. Testing your code Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlearner ---with-coverage 5. Documenting changes Please add an entry to the mlearner/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlearner/sources . 6. Committing changes When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>' 7. Optional: squashing commits If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions. 8. Uploading changes Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch> 9. Submitting a pull request Go to your GitHub repository online, select the new feature branch, and submit a new pull request: Notes for Developers Building the documentation The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlearner/docs directory. For example, ~/github/mlearner/docs$ mkdocs serve 1. Building the API documentation To build the API documentation, navigate to mlearner/docs and execute the make_api.py file from this directory via ~/github/mlearner/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlearner/docs/sources/api_modules mlearner/docs/sources/api_subpackes 2. Editing the User Guide The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlearner/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlearner/docs/mkdocs.yml file. 3. Building static HTML files of the documentation First, please check the documenation via localhost (http://127.0.0.1:8000/): ~/github/mlearner/docs$ mkdocs serve Next, build the static HTML files of the mlearner documentation via ~/github/mlearner/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlearner/docs$ mkdocs gh-deploy --clean 4. Generate a PDF of the documentation To generate a PDF version of the documentation, simply cd into the mlearner/docs directory and execute: python md2pdf.py Uploading a new version to PyPI 1. Creating a new testing environment Assuming we are using conda , create a new python environment via $ conda create -n 'mlearner-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlearner-testing 2. Installing the package from local files Test the installation by executing $ python setup.py install --record files.txt the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlearner; print(mlearner.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip install mlearner and uninstall it again $ pip uninstall mlearner 3. Deploying the package Consider deploying the package to the PyPI test server first. The setup instructions can be found here . $ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi Test if it can be installed from there by executing $ pip install -i https://testpypi.python.org/pypi mlearner and uninstall it $ pip uninstall mlearner After this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python setup.py sdist bdist_wheel upload 4. Removing the virtual environment Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlearner-testing' --all 5. Updating the conda-forge recipe Once a new version of mlearner has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlearner-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"How To Contribute"},{"location":"CONTRIBUTING/#how-to-contribute","text":"I would be very happy about any kind of contributions that help to improve and extend the functionality of mlearner.","title":"How to Contribute"},{"location":"CONTRIBUTING/#quick-contributor-checklist","text":"This is a quick checklist about the different steps of a typical contribution to mlearner (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go. [ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlearner repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlearner/*/tests [ ] Run PYTHONPATH='.' pytest ./mlearner -sv and make sure that all unit tests pass [ ] Check for style issues by running flake8 ./mlearner (you may want to run pytest again after you made modifications to the code) [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file [ ] Modify documentation in the appropriate location under mlearner/docs/sources/ [ ] Push the topic branch to the server and create a pull request [ ] Check the Travis-CI build passed at https://travis-ci.org/jaisenbe58r/mlearner [ ] Check/improve the unit test coverage at https://coveralls.io/github/jaisenbe58r/mlearner [ ] Check/improve the code health at https://landscape.io/github/jaisenbe58r/mlearner","title":"Quick Contributor Checklist"},{"location":"CONTRIBUTING/#tips-for-contributors","text":"","title":"Tips for Contributors"},{"location":"CONTRIBUTING/#getting-started-creating-a-new-issue-and-forking-the-repository","text":"If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlearner repository from the GitHub web interface. Clone the mlearner repository to your local machine by executing git clone https://github.com/<your_username>/MLearner.git","title":"Getting Started - Creating a New Issue and Forking the Repository"},{"location":"CONTRIBUTING/#syncing-an-existing-fork","text":"If you already forked mlearner earlier, you can bring you \"Fork\" up to date with the master branch as follows:","title":"Syncing an Existing Fork"},{"location":"CONTRIBUTING/#1-configuring-a-remote-that-points-to-the-upstream-repository-on-github","text":"List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/jaisenbe58r/MLearner.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) upstream https://github.com/jaisenbe58r/MLearner.git (fetch) upstream https://github.com/jaisenbe58r/MLearner.git (push)","title":"1. Configuring a remote that points to the upstream repository on GitHub"},{"location":"CONTRIBUTING/#2-syncing-your-fork","text":"First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/jaisenbe58r/MLearner * [new branch] master -> upstream/master This means that the commits to the jaisenbe58r/mlearner master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+),","title":"2. Syncing your Fork"},{"location":"CONTRIBUTING/#the-main-workflow-making-changes-in-a-new-topic-branch","text":"Listed below are the 9 typical steps of a contribution.","title":"*The Main Workflow - Making Changes in a New Topic Branch"},{"location":"CONTRIBUTING/#1-discussing-the-feature-or-modification","text":"Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already.","title":"1. Discussing the Feature or Modification"},{"location":"CONTRIBUTING/#2-creating-a-new-feature-branch","text":"Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature>","title":"2. Creating a new feature branch"},{"location":"CONTRIBUTING/#3-developing-the-new-feature-bug-fix","text":"Now it's time to modify existing code or to contribute new code to the project.","title":"3. Developing the new feature / bug fix"},{"location":"CONTRIBUTING/#4-testing-your-code","text":"Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlearner ---with-coverage","title":"4. Testing your code"},{"location":"CONTRIBUTING/#5-documenting-changes","text":"Please add an entry to the mlearner/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlearner/sources .","title":"5. Documenting changes"},{"location":"CONTRIBUTING/#6-committing-changes","text":"When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>'","title":"6. Committing changes"},{"location":"CONTRIBUTING/#7-optional-squashing-commits","text":"If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions.","title":"7. Optional: squashing commits"},{"location":"CONTRIBUTING/#8-uploading-changes","text":"Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch>","title":"8. Uploading changes"},{"location":"CONTRIBUTING/#9-submitting-a-pull-request","text":"Go to your GitHub repository online, select the new feature branch, and submit a new pull request:","title":"9. Submitting a pull request"},{"location":"CONTRIBUTING/#notes-for-developers","text":"","title":"Notes for Developers"},{"location":"CONTRIBUTING/#building-the-documentation","text":"The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlearner/docs directory. For example, ~/github/mlearner/docs$ mkdocs serve","title":"Building the documentation"},{"location":"CONTRIBUTING/#1-building-the-api-documentation","text":"To build the API documentation, navigate to mlearner/docs and execute the make_api.py file from this directory via ~/github/mlearner/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlearner/docs/sources/api_modules mlearner/docs/sources/api_subpackes","title":"1. Building the API documentation"},{"location":"CONTRIBUTING/#2-editing-the-user-guide","text":"The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlearner/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlearner/docs/mkdocs.yml file.","title":"2. Editing the User Guide"},{"location":"CONTRIBUTING/#3-building-static-html-files-of-the-documentation","text":"First, please check the documenation via localhost (http://127.0.0.1:8000/): ~/github/mlearner/docs$ mkdocs serve Next, build the static HTML files of the mlearner documentation via ~/github/mlearner/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlearner/docs$ mkdocs gh-deploy --clean","title":"3. Building static HTML files of the documentation"},{"location":"CONTRIBUTING/#4-generate-a-pdf-of-the-documentation","text":"To generate a PDF version of the documentation, simply cd into the mlearner/docs directory and execute: python md2pdf.py","title":"4. Generate a PDF of the documentation"},{"location":"CONTRIBUTING/#uploading-a-new-version-to-pypi","text":"","title":"Uploading a new version to PyPI"},{"location":"CONTRIBUTING/#1-creating-a-new-testing-environment","text":"Assuming we are using conda , create a new python environment via $ conda create -n 'mlearner-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlearner-testing","title":"1. Creating a new testing environment"},{"location":"CONTRIBUTING/#2-installing-the-package-from-local-files","text":"Test the installation by executing $ python setup.py install --record files.txt the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlearner; print(mlearner.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip install mlearner and uninstall it again $ pip uninstall mlearner","title":"2. Installing the package from local files"},{"location":"CONTRIBUTING/#3-deploying-the-package","text":"Consider deploying the package to the PyPI test server first. The setup instructions can be found here . $ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi Test if it can be installed from there by executing $ pip install -i https://testpypi.python.org/pypi mlearner and uninstall it $ pip uninstall mlearner After this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python setup.py sdist bdist_wheel upload","title":"3. Deploying the package"},{"location":"CONTRIBUTING/#4-removing-the-virtual-environment","text":"Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlearner-testing' --all","title":"4. Removing the virtual environment"},{"location":"CONTRIBUTING/#5-updating-the-conda-forge-recipe","text":"Once a new version of mlearner has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlearner-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"5. Updating the conda-forge recipe"},{"location":"USER_GUIDE_INDEX/","text":"User Guide Index load DataLoad data wine_data data_normal data_gamma data_uniform create_dataset preprocessing MeanCenterer minmax_scaling FeatureDropper FillNaTransformer_median FillNaTransformer_mean FillNaTransformer_idmax FillNaTransformer_any FillNaTransformer_all FillNaTransformer_value FillNaTransformer_backward FillNaTransformer_forward FixSkewness OneHotEncoder DropOutliers ExtractCategories ReplaceMulticlass ReplaceTransformer DataCleaner DataAnalyst feature selections FeatureSelection models modelXGBoost modelLightBoost modelCatBoost clasifier PipelineClasificators TrainingUtilities training Training evaluation EvaluationModels utils ParamsManager","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#user-guide-index","text":"","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#load","text":"DataLoad","title":"load"},{"location":"USER_GUIDE_INDEX/#data","text":"wine_data data_normal data_gamma data_uniform create_dataset","title":"data"},{"location":"USER_GUIDE_INDEX/#preprocessing","text":"MeanCenterer minmax_scaling FeatureDropper FillNaTransformer_median FillNaTransformer_mean FillNaTransformer_idmax FillNaTransformer_any FillNaTransformer_all FillNaTransformer_value FillNaTransformer_backward FillNaTransformer_forward FixSkewness OneHotEncoder DropOutliers ExtractCategories ReplaceMulticlass ReplaceTransformer DataCleaner DataAnalyst","title":"preprocessing"},{"location":"USER_GUIDE_INDEX/#feature-selections","text":"FeatureSelection","title":"feature selections"},{"location":"USER_GUIDE_INDEX/#models","text":"modelXGBoost modelLightBoost modelCatBoost","title":"models"},{"location":"USER_GUIDE_INDEX/#clasifier","text":"PipelineClasificators TrainingUtilities","title":"clasifier"},{"location":"USER_GUIDE_INDEX/#training","text":"Training","title":"training"},{"location":"USER_GUIDE_INDEX/#evaluation","text":"EvaluationModels","title":"evaluation"},{"location":"USER_GUIDE_INDEX/#utils","text":"ParamsManager","title":"utils"},{"location":"cite/","text":"Citing mlearner","title":"Citing mlearner"},{"location":"cite/#citing-mlearner","text":"","title":"Citing mlearner"},{"location":"contributors/","text":"Contributors For the current list of contributors to mlearner, please see the GitHub contributor page at [https://github.com/jaisenbe58r/MLearner/graphs/contributors].","title":"Contributors"},{"location":"contributors/#contributors","text":"For the current list of contributors to mlearner, please see the GitHub contributor page at [https://github.com/jaisenbe58r/MLearner/graphs/contributors].","title":"Contributors"},{"location":"discuss/","text":"Discuss Any questions or comments about mlearner? Join the mlearner mailing list on Google Groups!","title":"Discuss"},{"location":"discuss/#discuss","text":"Any questions or comments about mlearner? Join the mlearner mailing list on Google Groups!","title":"Discuss"},{"location":"installation/","text":"Installing mlearner PyPI To install mlearner, just execute pip install mlearner Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlearner , unzip it, navigate into the package, and use the command: python setup.py install Upgrading via pip To upgrade an existing version of mlearner from PyPI, execute pip install mlearner --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this. Installing mlearner from the source distribution In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlearner from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlearner from the source distribution instead via pip install --no-binary :all: mlearner Also, I would appreciate it if you could report any issues that occur when using pip install mlearner in hope that we can fix these in future releases. Conda The mlearner package is also available through conda forge . To install mlearner using conda, use the following command: conda install mlearner --channel conda-forge or simply conda install mlearner if you added conda-forge to your channels ( conda config --add channels conda-forge ). Dev Version The mlearner version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/jaisenbe58r/MLearner.git Or, you can fork the GitHub repository from https://github.com/jaisenbe58r/MLearner and install mlearner from your local drive via python setup.py install","title":"Installation"},{"location":"installation/#installing-mlearner","text":"","title":"Installing mlearner"},{"location":"installation/#pypi","text":"To install mlearner, just execute pip install mlearner Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlearner , unzip it, navigate into the package, and use the command: python setup.py install","title":"PyPI"},{"location":"installation/#upgrading-via-pip","text":"To upgrade an existing version of mlearner from PyPI, execute pip install mlearner --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this.","title":"Upgrading via pip"},{"location":"installation/#installing-mlearner-from-the-source-distribution","text":"In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlearner from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlearner from the source distribution instead via pip install --no-binary :all: mlearner Also, I would appreciate it if you could report any issues that occur when using pip install mlearner in hope that we can fix these in future releases.","title":"Installing mlearner from the source distribution"},{"location":"installation/#conda","text":"The mlearner package is also available through conda forge . To install mlearner using conda, use the following command: conda install mlearner --channel conda-forge or simply conda install mlearner if you added conda-forge to your channels ( conda config --add channels conda-forge ).","title":"Conda"},{"location":"installation/#dev-version","text":"The mlearner version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/jaisenbe58r/MLearner.git Or, you can fork the GitHub repository from https://github.com/jaisenbe58r/MLearner and install mlearner from your local drive via python setup.py install","title":"Dev Version"},{"location":"license/","text":"This project is released under a permissive new BSD open source license and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose. In addition, you may use, copy, modify, and redistribute all artistic creative works (figures and images) included in this distribution under the directory according to the terms and conditions of the Creative Commons Attribution 4.0 International License. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above). new BSD License New BSD License Copyright (c) 2014-2018-2022, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlearner nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Creative Commons Attribution 4.0 International License mlearner documentation figures are licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by-sa/4.0/ . You are free to: Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"License"},{"location":"license/#new-bsd-license","text":"New BSD License Copyright (c) 2014-2018-2022, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlearner nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"new BSD License"},{"location":"license/#creative-commons-attribution-40-international-license","text":"mlearner documentation figures are licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by-sa/4.0/ .","title":"Creative Commons Attribution 4.0 International License"},{"location":"license/#you-are-free-to","text":"Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"license/#under-the-following-terms","text":"Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms:"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/","text":"PipelineClasificators PipelineClasificators(random_state=99) None Methods Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"PipelineClasificators"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/#pipelineclasificators","text":"PipelineClasificators(random_state=99) None","title":"PipelineClasificators"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/#methods","text":"Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/","text":"TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"TrainingUtilities"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/#methods","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.data/create_dataset/","text":"create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"Create dataset"},{"location":"api_modules/mlearner.data/create_dataset/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"api_modules/mlearner.data/data_gamma/","text":"data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"Data gamma"},{"location":"api_modules/mlearner.data/data_gamma/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"api_modules/mlearner.data/data_normal/","text":"data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"Data normal"},{"location":"api_modules/mlearner.data/data_normal/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"api_modules/mlearner.data/data_uniform/","text":"data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"Data uniform"},{"location":"api_modules/mlearner.data/data_uniform/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"api_modules/mlearner.data/wine_data/","text":"wine_data wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Wine data"},{"location":"api_modules/mlearner.data/wine_data/#wine_data","text":"wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"wine_data"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/","text":"EvaluationModels EvaluationModels(model, random_state=99) None Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"EvaluationModels"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/#evaluationmodels","text":"EvaluationModels(model, random_state=99) None","title":"EvaluationModels"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/","text":"FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"FeatureSelection"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/#methods","text":"LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"api_modules/mlearner.load/DataLoad/","text":"DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"DataLoad"},{"location":"api_modules/mlearner.load/DataLoad/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"api_modules/mlearner.load/DataLoad/#methods","text":"load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"api_modules/mlearner.models/modelCatBoost/","text":"modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"modelCatBoost"},{"location":"api_modules/mlearner.models/modelCatBoost/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"api_modules/mlearner.models/modelCatBoost/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"api_modules/mlearner.models/modelLightBoost/","text":"modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelLightBoost"},{"location":"api_modules/mlearner.models/modelLightBoost/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"api_modules/mlearner.models/modelLightBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.models/modelXGBoost/","text":"modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelXGBoost"},{"location":"api_modules/mlearner.models/modelXGBoost/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"api_modules/mlearner.models/modelXGBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.plotly/FeatureAnalyst/","text":"FeatureAnalyst FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_modules/mlearner.plotly/FeatureAnalyst/#featureanalyst","text":"FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_modules/mlearner.plotly/plot_LDA/","text":"plot_LDA plot_LDA(data, features) None","title":"plot LDA"},{"location":"api_modules/mlearner.plotly/plot_LDA/#plot_lda","text":"plot_LDA(data, features) None","title":"plot_LDA"},{"location":"api_modules/mlearner.plotly/plot_PCA/","text":"plot_PCA plot_PCA(data, features) None","title":"plot PCA"},{"location":"api_modules/mlearner.plotly/plot_PCA/#plot_pca","text":"plot_PCA(data, features) None","title":"plot_PCA"},{"location":"api_modules/mlearner.plotly/plotly_histogram2/","text":"plotly_histogram2 plotly_histogram2(X, columns, target) None","title":"Plotly histogram2"},{"location":"api_modules/mlearner.plotly/plotly_histogram2/#plotly_histogram2","text":"plotly_histogram2(X, columns, target) None","title":"plotly_histogram2"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/","text":"CategoricalEncoder CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings. Methods fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"CategoricalEncoder"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/#categoricalencoder","text":"CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings.","title":"CategoricalEncoder"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/#methods","text":"fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/","text":"CopyFeatures CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"CopyFeatures"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/#copyfeatures","text":"CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CopyFeatures"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/","text":"DataAnalyst DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/ Methods Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/') Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataAnalyst"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/#dataanalyst","text":"DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/","title":"DataAnalyst"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/#methods","text":"Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/') Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/","text":"DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataCleaner"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/#methods","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/","text":"DataExploratory DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataExploratory"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/#dataexploratory","text":"DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataExploratory"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/#methods","text":"categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/","text":"DataFrameSelector DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"DataFrameSelector"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/#dataframeselector","text":"DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"DataFrameSelector"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/","text":"DropFeatures DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"DropFeatures"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/#dropfeatures","text":"DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/","title":"DropFeatures"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/","text":"DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"DropOutliers"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/","text":"ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ExtractCategories"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/","text":"FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"FeatureDropper"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/","text":"FeatureSelector FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FeatureSelector"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/#featureselector","text":"FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/","title":"FeatureSelector"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/","text":"FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer all"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/","text":"FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer any"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/","text":"FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer backward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/","text":"FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer forward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/","text":"FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer idmax"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/","text":"FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer mean"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/","text":"FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer median"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/","text":"FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer value"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/#methods","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/","text":"FixSkewness FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"FixSkewness"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/#fixskewness","text":"FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/#methods","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LDA_add/","text":"LDA_add LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"LDA add"},{"location":"api_modules/mlearner.preprocessing/LDA_add/#lda_add","text":"LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_add"},{"location":"api_modules/mlearner.preprocessing/LDA_add/#methods","text":"fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/","text":"LDA_selector LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"LDA selector"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/#lda_selector","text":"LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_selector"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/#methods","text":"fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/","text":"LabelEncoder LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array. Methods fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"LabelEncoder"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/#labelencoder","text":"LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array.","title":"LabelEncoder"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/#methods","text":"fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/","text":"MeanCenterer MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"MeanCenterer"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/#meancenterer","text":"MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/#methods","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/","text":"OneHotEncoder OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"OneHotEncoder"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/#methods","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/PCA_add/","text":"PCA_add PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"PCA add"},{"location":"api_modules/mlearner.preprocessing/PCA_add/#pca_add","text":"PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_add"},{"location":"api_modules/mlearner.preprocessing/PCA_add/#methods","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/","text":"PCA_selector PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"PCA selector"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/#pca_selector","text":"PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_selector"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/#methods","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/","text":"ReplaceMulticlass ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceMulticlass"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/#replacemulticlass","text":"ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/","text":"ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceTransformer"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/","text":"StandardScaler StandardScaler(copy=True, with_mean=True, with_std=True) Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`. Methods fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"StandardScaler"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/#standardscaler","text":"StandardScaler(copy=True, with_mean=True, with_std=True) Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.","title":"StandardScaler"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/#methods","text":"fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/minmax_scaling/","text":"minmax_scaling minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Minmax scaling"},{"location":"api_modules/mlearner.preprocessing/minmax_scaling/#minmax_scaling","text":"minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"api_modules/mlearner.training/Training/","text":"Training Training(model, random_state=99) None Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Training"},{"location":"api_modules/mlearner.training/Training/#training","text":"Training(model, random_state=99) None","title":"Training"},{"location":"api_modules/mlearner.training/Training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.utils/ParamsManager/","text":"ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes matodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None","title":"ParamsManager"},{"location":"api_modules/mlearner.utils/ParamsManager/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes matodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"api_modules/mlearner.utils/ParamsManager/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"},{"location":"api_subpackages/mlearner.classifier/","text":"mlearner version: 0.1.3dev0 PipelineClasificators PipelineClasificators(random_state=99) None Methods Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones. TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.classifier"},{"location":"api_subpackages/mlearner.classifier/#pipelineclasificators","text":"PipelineClasificators(random_state=99) None","title":"PipelineClasificators"},{"location":"api_subpackages/mlearner.classifier/#methods","text":"Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.classifier/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"api_subpackages/mlearner.classifier/#methods_1","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.data/","text":"mlearner version: 0.1.3dev0 create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/ data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/ data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/ data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/ wine_data wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Mlearner.data"},{"location":"api_subpackages/mlearner.data/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"api_subpackages/mlearner.data/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"api_subpackages/mlearner.data/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"api_subpackages/mlearner.data/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"api_subpackages/mlearner.data/#wine_data","text":"wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"wine_data"},{"location":"api_subpackages/mlearner.evaluation/","text":"mlearner version: 0.1.3dev0 EvaluationModels EvaluationModels(model, random_state=99) None Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.evaluation"},{"location":"api_subpackages/mlearner.evaluation/#evaluationmodels","text":"EvaluationModels(model, random_state=99) None","title":"EvaluationModels"},{"location":"api_subpackages/mlearner.evaluation/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.feature_selection/","text":"mlearner version: 0.1.3dev0 FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Mlearner.feature selection"},{"location":"api_subpackages/mlearner.feature_selection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"api_subpackages/mlearner.feature_selection/#methods","text":"LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"api_subpackages/mlearner.load/","text":"mlearner version: 0.1.3dev0 DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Mlearner.load"},{"location":"api_subpackages/mlearner.load/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"api_subpackages/mlearner.load/#methods","text":"load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"api_subpackages/mlearner.models/","text":"mlearner version: 0.1.3dev0 modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones. modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.models"},{"location":"api_subpackages/mlearner.models/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"api_subpackages/mlearner.models/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"api_subpackages/mlearner.models/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"api_subpackages/mlearner.models/#methods_1","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.models/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"api_subpackages/mlearner.models/#methods_2","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.plotly/","text":"mlearner version: 0.1.3dev0 FeatureAnalyst FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias. plot_LDA plot_LDA(data, features) None plot_PCA plot_PCA(data, features) None plotly_histogram2 plotly_histogram2(X, columns, target) None","title":"Mlearner.plotly"},{"location":"api_subpackages/mlearner.plotly/#featureanalyst","text":"FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_subpackages/mlearner.plotly/#plot_lda","text":"plot_LDA(data, features) None","title":"plot_LDA"},{"location":"api_subpackages/mlearner.plotly/#plot_pca","text":"plot_PCA(data, features) None","title":"plot_PCA"},{"location":"api_subpackages/mlearner.plotly/#plotly_histogram2","text":"plotly_histogram2(X, columns, target) None","title":"plotly_histogram2"},{"location":"api_subpackages/mlearner.preprocessing/","text":"mlearner version: 0.1.3dev0 CategoricalEncoder CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings. Methods fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input. CopyFeatures CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None DataAnalyst DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/ Methods Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/') Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataExploratory DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataFrameSelector DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None DropFeatures DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped. ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped. FeatureSelector FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FixSkewness FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. LDA_add LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. LDA_selector LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. LabelEncoder LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array. Methods fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] MeanCenterer MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. OneHotEncoder OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder. PCA_add PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. PCA_selector PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. ReplaceMulticlass ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. StandardScaler StandardScaler(copy=True, with_mean=True, with_std=True) Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`. Methods fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. minmax_scaling minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Mlearner.preprocessing"},{"location":"api_subpackages/mlearner.preprocessing/#categoricalencoder","text":"CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings.","title":"CategoricalEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods","text":"fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#copyfeatures","text":"CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CopyFeatures"},{"location":"api_subpackages/mlearner.preprocessing/#methods_1","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataanalyst","text":"DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/","title":"DataAnalyst"},{"location":"api_subpackages/mlearner.preprocessing/#methods_2","text":"Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/') Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"api_subpackages/mlearner.preprocessing/#methods_3","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataexploratory","text":"DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataExploratory"},{"location":"api_subpackages/mlearner.preprocessing/#methods_4","text":"categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataframeselector","text":"DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"DataFrameSelector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_5","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dropfeatures","text":"DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/","title":"DropFeatures"},{"location":"api_subpackages/mlearner.preprocessing/#methods_6","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"api_subpackages/mlearner.preprocessing/#methods_7","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"api_subpackages/mlearner.preprocessing/#methods_8","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"api_subpackages/mlearner.preprocessing/#methods_9","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#featureselector","text":"FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/","title":"FeatureSelector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_10","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"api_subpackages/mlearner.preprocessing/#methods_11","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"api_subpackages/mlearner.preprocessing/#methods_12","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"api_subpackages/mlearner.preprocessing/#methods_13","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"api_subpackages/mlearner.preprocessing/#methods_14","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"api_subpackages/mlearner.preprocessing/#methods_15","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"api_subpackages/mlearner.preprocessing/#methods_16","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"api_subpackages/mlearner.preprocessing/#methods_17","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"api_subpackages/mlearner.preprocessing/#methods_18","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fixskewness","text":"FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"api_subpackages/mlearner.preprocessing/#methods_19","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#lda_add","text":"LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_add"},{"location":"api_subpackages/mlearner.preprocessing/#methods_20","text":"fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#lda_selector","text":"LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_selector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_21","text":"fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#labelencoder","text":"LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array.","title":"LabelEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods_22","text":"fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#meancenterer","text":"MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_23","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods_24","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#pca_add","text":"PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_add"},{"location":"api_subpackages/mlearner.preprocessing/#methods_25","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#pca_selector","text":"PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_selector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_26","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#replacemulticlass","text":"ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"api_subpackages/mlearner.preprocessing/#methods_27","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_28","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#standardscaler","text":"StandardScaler(copy=True, with_mean=True, with_std=True) Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.","title":"StandardScaler"},{"location":"api_subpackages/mlearner.preprocessing/#methods_29","text":"fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. **fit_params : dict Additional fit parameters. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#minmax_scaling","text":"minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"api_subpackages/mlearner.training/","text":"mlearner version: 0.1.3dev0 Training Training(model, random_state=99) None Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.training"},{"location":"api_subpackages/mlearner.training/#training","text":"Training(model, random_state=99) None","title":"Training"},{"location":"api_subpackages/mlearner.training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.utils/","text":"mlearner version: 0.1.3dev0 ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes matodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None","title":"Mlearner.utils"},{"location":"api_subpackages/mlearner.utils/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes matodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"api_subpackages/mlearner.utils/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"},{"location":"user_guide/clasifier/PipelineClasificators/","text":"PipelineClasificators PipelineClasificators(random_state=99) None Methods CatBoost(name='CBT') None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_GridSearch() None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5, select='XGBoost') None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine() XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(display=True) None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"PipelineClasificators"},{"location":"user_guide/clasifier/PipelineClasificators/#pipelineclasificators","text":"PipelineClasificators(random_state=99) None","title":"PipelineClasificators"},{"location":"user_guide/clasifier/PipelineClasificators/#methods","text":"CatBoost(name='CBT') None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_GridSearch() None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5, select='XGBoost') None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine() XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(display=True) None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/clasifier/TrainingUtilities/","text":"TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"TrainingUtilities"},{"location":"user_guide/clasifier/TrainingUtilities/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"user_guide/clasifier/TrainingUtilities/#methods","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/data/create_dataset/","text":"create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"Create dataset"},{"location":"user_guide/data/create_dataset/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"user_guide/data/data_gamma/","text":"data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"Data gamma"},{"location":"user_guide/data/data_gamma/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"user_guide/data/data_normal/","text":"data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"Data normal"},{"location":"user_guide/data/data_normal/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"user_guide/data/data_uniform/","text":"data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"Data uniform"},{"location":"user_guide/data/data_uniform/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"user_guide/data/wine_data/","text":"wine_data wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data","title":"Wine data"},{"location":"user_guide/data/wine_data/#wine_data","text":"wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data","title":"wine_data"},{"location":"user_guide/evaluation/EvaluationModels/","text":"EvaluationModels EvaluationModels(model, random_state=99) None Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"EvaluationModels"},{"location":"user_guide/evaluation/EvaluationModels/#evaluationmodels","text":"EvaluationModels(model, random_state=99) None","title":"EvaluationModels"},{"location":"user_guide/evaluation/EvaluationModels/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/feature_selections/FeatureSelection/","text":"FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"FeatureSelection"},{"location":"user_guide/feature_selections/FeatureSelection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"user_guide/feature_selections/FeatureSelection/#methods","text":"LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"user_guide/load/DataLoad/","text":"DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: 'str, path object or file-like object' Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: 'str' Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: 'str, default None' Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"DataLoad"},{"location":"user_guide/load/DataLoad/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"user_guide/load/DataLoad/#methods","text":"load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: 'str, path object or file-like object' Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: 'str' Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: 'str, default None' Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"user_guide/models/modelCatBoost/","text":"modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"modelCatBoost"},{"location":"user_guide/models/modelCatBoost/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"user_guide/models/modelCatBoost/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"user_guide/models/modelLightBoost/","text":"modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelLightBoost"},{"location":"user_guide/models/modelLightBoost/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"user_guide/models/modelLightBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/models/modelXGBoost/","text":"modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelXGBoost"},{"location":"user_guide/models/modelXGBoost/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"user_guide/models/modelXGBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/preprocessing/DataAnalyst/","text":"Preprocessing Data analysis - EDA Dataset preprocessing examples from mlearner.preprocessing import DataAnalyst import numpy as np import pandas as pd from mlearner.preprocessing import DataAnalyst %matplotlib inline Load data in base_preprocess class filename = \"mlearner/data/data/breast-cancer-wisconsin-data.txt\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sample code number Clump Thickness Uniformity of Cell Size Uniformity of Cell Shape Marginal Adhesion Single Epithelial Cell Size Bare Nuclei Bland Chromatin Normal Nucleoli Mitoses Class 0 1000025 5 1 1 1 2 1 3 1 1 2 1 1002945 5 4 4 5 7 10 3 2 1 2 2 1015425 3 1 1 1 2 2 3 1 1 2 3 1016277 6 8 8 1 3 4 3 7 1 2 4 1017023 4 1 1 3 2 1 3 1 1 2 dataset.dtypes() Sample code number int64 Clump Thickness int64 Uniformity of Cell Size int64 Uniformity of Cell Shape int64 Marginal Adhesion int64 Single Epithelial Cell Size int64 Bare Nuclei object Bland Chromatin int64 Normal Nucleoli int64 Mitoses int64 Class int64 dtype: object target = [\"Class\"] Method: boxplot dataset.boxplot(features=[\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: dispersion_categoria dataset.dispersion_categoria(target=target) Method: sns_jointplot dataset.sns_jointplot(feature1=[\"Bland Chromatin\"], feature2=[\"Clump Thickness\"], target=target, categoria1 = [2]) Method: sns_pairplot dataset.sns_pairplot(features= [\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: distribution_targets dataset.distribution_targets(target=target) Method: corr_matrix dataset.corr_matrix() Method: not_type_object API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataAnalyst/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataAnalyst/#data-analysis-eda","text":"Dataset preprocessing examples from mlearner.preprocessing import DataAnalyst import numpy as np import pandas as pd from mlearner.preprocessing import DataAnalyst %matplotlib inline Load data in base_preprocess class filename = \"mlearner/data/data/breast-cancer-wisconsin-data.txt\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sample code number Clump Thickness Uniformity of Cell Size Uniformity of Cell Shape Marginal Adhesion Single Epithelial Cell Size Bare Nuclei Bland Chromatin Normal Nucleoli Mitoses Class 0 1000025 5 1 1 1 2 1 3 1 1 2 1 1002945 5 4 4 5 7 10 3 2 1 2 2 1015425 3 1 1 1 2 2 3 1 1 2 3 1016277 6 8 8 1 3 4 3 7 1 2 4 1017023 4 1 1 3 2 1 3 1 1 2 dataset.dtypes() Sample code number int64 Clump Thickness int64 Uniformity of Cell Size int64 Uniformity of Cell Shape int64 Marginal Adhesion int64 Single Epithelial Cell Size int64 Bare Nuclei object Bland Chromatin int64 Normal Nucleoli int64 Mitoses int64 Class int64 dtype: object target = [\"Class\"] Method: boxplot dataset.boxplot(features=[\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: dispersion_categoria dataset.dispersion_categoria(target=target) Method: sns_jointplot dataset.sns_jointplot(feature1=[\"Bland Chromatin\"], feature2=[\"Clump Thickness\"], target=target, categoria1 = [2]) Method: sns_pairplot dataset.sns_pairplot(features= [\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: distribution_targets dataset.distribution_targets(target=target) Method: corr_matrix dataset.corr_matrix() Method: not_type_object","title":"Data analysis - EDA"},{"location":"user_guide/preprocessing/DataAnalyst/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/","text":"Preprocessing Data Cleaner - Transformers Dataset preprocessing examples import numpy as np import pandas as pd from mlearner.preprocessing import minmax_scaling, MeanCenterer, FeatureDropper from mlearner.preprocessing import FillNaTransformer_median, FillNaTransformer_mean from mlearner.preprocessing import FillNaTransformer_idmax, FillNaTransformer_any from mlearner.preprocessing import FillNaTransformer_all, FillNaTransformer_value from mlearner.preprocessing import FillNaTransformer_backward, FillNaTransformer_forward from mlearner.preprocessing import FixSkewness, OneHotEncoder, DropOutliers, ReplaceTransformer from mlearner.preprocessing import ReplaceMulticlass, ExtractCategories, DataAnalyst, DataExploratory Load data in DataAnalyst class from mlearner.preprocessing import DataAnalyst filename = \"mlearner/data/data/titanic3.csv\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object dataset.data[\"age\"] = dataset.data[\"age\"].astype(np.float32) dataset.Xy_dataset(target=[\"survived\"]) Apply Transformers Drop Nan dataset.X.shape (1309, 13) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Transformer FillNaTransformer_median class from mlearner.preprocessing import FillNaTransformer_median mc = FillNaTransformer_median(columns=[\"body\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\pandas\\core\\frame.py:3140: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy self[k1] = value[k2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_mean class from mlearner.preprocessing import FillNaTransformer_mean mc = FillNaTransformer_median(columns=[\"age\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 embarked 2 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_idmax class from mlearner.preprocessing import FillNaTransformer_idmax mc = FillNaTransformer_idmax(columns=[\"embarked\"]).fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) C:\\Users\\AUTIS\\Google Drive\\10_MachineLearning_JS\\MachineLearning\\05_MLearner\\MLearner\\mlearner\\preprocessing\\replace_na.py:244: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy X[col] = X[col].fillna(X[col].value_counts().idxmax()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_value class from mlearner.preprocessing import FillNaTransformer_value mc = FillNaTransformer_value(columns=[\"home.dest\"]).fit(dataset.X, value=\"Empty\") dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_backward class from mlearner.preprocessing import FillNaTransformer_backward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_backward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 59 home.dest 27 boat 9 body 2 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 embarked 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_forward class from mlearner.preprocessing import FillNaTransformer_forward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_forward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 3 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_all class from mlearner.preprocessing import FillNaTransformer_all mc = FillNaTransformer_median().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_any class from mlearner.preprocessing import FillNaTransformer_any mc = FillNaTransformer_any().fit(dataset.X) AUX = mc.transform(dataset.X) dataset.missing_values(AUX) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Extract targets Transformer ExtractCategories class from mlearner.preprocessing import ExtractCategories Extracting subsets of data from a selected category mc = ExtractCategories(categories=[1], target=[\"survived\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) data_survived.shape (500, 14) Drop Features Transformer FeatureDropper class from mlearner.preprocessing import FeatureDropper mc = FeatureDropper(drop=[\"name\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape, data_survived.shape ((1309, 14), (1309, 13)) Drop Outliers Transformer DropOutliers class from mlearner.preprocessing import DropOutliers mc = DropOutliers(features=[\"age\", \"body\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape[0], data_survived.shape[0] C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal keep = (tmp_a >= first_edge) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal keep &= (tmp_a <= last_edge) (1309, 116) Scaling Transformer minmax_scaling class from mlearner.preprocessing import minmax_scaling dataset.X[\"age\"].max(), dataset.X[\"age\"].min() (80.0, 0.1667) new = minmax_scaling(dataset.X, columns=[\"age\"]) new.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.361169 1 0.009395 2 0.022964 3 0.373695 4 0.311064 Fix Skewnesss Transformer FixSkewness class from mlearner.preprocessing import FixSkewness import matplotlib.pyplot as plt mc = FixSkewness(columns=[\"age\"]).fit(dataset.X) plt.hist(dataset.data[\"age\"]) plt.hist(mc.transform(dataset.X)[\"age\"]) (array([ 76., 85., 336., 319., 208., 148., 71., 51., 11., 4.]), array([ 0.1667 , 8.15003, 16.13336, 24.11669, 32.10002, 40.08335, 48.06668, 56.05001, 64.03334, 72.01667, 80. ], dtype=float32), <a list of 10 Patch objects>) One-Hot-Encoder Transformer OneHotEncoder class from mlearner.preprocessing import OneHotEncoder mc = OneHotEncoder(columns=[\"pclass\"]).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[[\"pclass_1\", \"pclass_2\", \"pclass_3\"]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass_1 pclass_2 pclass_3 0 1 0 0 1 1 0 0 2 1 0 0 3 1 0 0 4 1 0 0 Replace map Transformer ReplaceTransformer class from mlearner.preprocessing import ReplaceTransformer mapping = {\"C\": 0, \"S\": 1, \"Q\": 3} mc = ReplaceTransformer(columns=[\"embarked\"], mapping=mapping).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[\"embarked\"].unique() array([1, 0, 3], dtype=int64) ### Replace Multiclass Transformer ReplaceMulticlass class from mlearner.preprocessing import ReplaceMulticlass dataset.X.columns.tolist() ['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] col = ['pclass', 'sex', 'sibsp', 'parch', 'ticket'] mc = ReplaceMulticlass(columns=col).fit(dataset.X) data_X = mc.transform(dataset.X) data_X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 0 Allen, Miss. Elisabeth Walton 0 29.0000 0 0 0 211.3375 B5 S 2 171.0 St Louis, MO 1 0 Allison, Master. Hudson Trevor 1 0.9167 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 2 0 Allison, Miss. Helen Loraine 0 2.0000 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 3 0 Allison, Mr. Hudson Joshua Creighton 1 30.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON 4 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) 0 25.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#data-cleaner-transformers","text":"Dataset preprocessing examples import numpy as np import pandas as pd from mlearner.preprocessing import minmax_scaling, MeanCenterer, FeatureDropper from mlearner.preprocessing import FillNaTransformer_median, FillNaTransformer_mean from mlearner.preprocessing import FillNaTransformer_idmax, FillNaTransformer_any from mlearner.preprocessing import FillNaTransformer_all, FillNaTransformer_value from mlearner.preprocessing import FillNaTransformer_backward, FillNaTransformer_forward from mlearner.preprocessing import FixSkewness, OneHotEncoder, DropOutliers, ReplaceTransformer from mlearner.preprocessing import ReplaceMulticlass, ExtractCategories, DataAnalyst, DataExploratory Load data in DataAnalyst class from mlearner.preprocessing import DataAnalyst filename = \"mlearner/data/data/titanic3.csv\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object dataset.data[\"age\"] = dataset.data[\"age\"].astype(np.float32) dataset.Xy_dataset(target=[\"survived\"])","title":"Data Cleaner - Transformers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#apply-transformers","text":"","title":"Apply Transformers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-nan","text":"dataset.X.shape (1309, 13) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Transformer FillNaTransformer_median class from mlearner.preprocessing import FillNaTransformer_median mc = FillNaTransformer_median(columns=[\"body\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\pandas\\core\\frame.py:3140: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy self[k1] = value[k2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_mean class from mlearner.preprocessing import FillNaTransformer_mean mc = FillNaTransformer_median(columns=[\"age\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 embarked 2 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_idmax class from mlearner.preprocessing import FillNaTransformer_idmax mc = FillNaTransformer_idmax(columns=[\"embarked\"]).fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) C:\\Users\\AUTIS\\Google Drive\\10_MachineLearning_JS\\MachineLearning\\05_MLearner\\MLearner\\mlearner\\preprocessing\\replace_na.py:244: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy X[col] = X[col].fillna(X[col].value_counts().idxmax()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_value class from mlearner.preprocessing import FillNaTransformer_value mc = FillNaTransformer_value(columns=[\"home.dest\"]).fit(dataset.X, value=\"Empty\") dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_backward class from mlearner.preprocessing import FillNaTransformer_backward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_backward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 59 home.dest 27 boat 9 body 2 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 embarked 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_forward class from mlearner.preprocessing import FillNaTransformer_forward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_forward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 3 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_all class from mlearner.preprocessing import FillNaTransformer_all mc = FillNaTransformer_median().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_any class from mlearner.preprocessing import FillNaTransformer_any mc = FillNaTransformer_any().fit(dataset.X) AUX = mc.transform(dataset.X) dataset.missing_values(AUX) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13)","title":"Drop Nan"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#extract-targets","text":"Transformer ExtractCategories class from mlearner.preprocessing import ExtractCategories Extracting subsets of data from a selected category mc = ExtractCategories(categories=[1], target=[\"survived\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) data_survived.shape (500, 14)","title":"Extract targets"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-features","text":"Transformer FeatureDropper class from mlearner.preprocessing import FeatureDropper mc = FeatureDropper(drop=[\"name\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape, data_survived.shape ((1309, 14), (1309, 13))","title":"Drop Features"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-outliers","text":"Transformer DropOutliers class from mlearner.preprocessing import DropOutliers mc = DropOutliers(features=[\"age\", \"body\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape[0], data_survived.shape[0] C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal keep = (tmp_a >= first_edge) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal keep &= (tmp_a <= last_edge) (1309, 116)","title":"Drop Outliers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#scaling","text":"Transformer minmax_scaling class from mlearner.preprocessing import minmax_scaling dataset.X[\"age\"].max(), dataset.X[\"age\"].min() (80.0, 0.1667) new = minmax_scaling(dataset.X, columns=[\"age\"]) new.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.361169 1 0.009395 2 0.022964 3 0.373695 4 0.311064","title":"Scaling"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#fix-skewnesss","text":"Transformer FixSkewness class from mlearner.preprocessing import FixSkewness import matplotlib.pyplot as plt mc = FixSkewness(columns=[\"age\"]).fit(dataset.X) plt.hist(dataset.data[\"age\"]) plt.hist(mc.transform(dataset.X)[\"age\"]) (array([ 76., 85., 336., 319., 208., 148., 71., 51., 11., 4.]), array([ 0.1667 , 8.15003, 16.13336, 24.11669, 32.10002, 40.08335, 48.06668, 56.05001, 64.03334, 72.01667, 80. ], dtype=float32), <a list of 10 Patch objects>)","title":"Fix Skewnesss"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#one-hot-encoder","text":"Transformer OneHotEncoder class from mlearner.preprocessing import OneHotEncoder mc = OneHotEncoder(columns=[\"pclass\"]).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[[\"pclass_1\", \"pclass_2\", \"pclass_3\"]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass_1 pclass_2 pclass_3 0 1 0 0 1 1 0 0 2 1 0 0 3 1 0 0 4 1 0 0","title":"One-Hot-Encoder"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#replace-map","text":"Transformer ReplaceTransformer class from mlearner.preprocessing import ReplaceTransformer mapping = {\"C\": 0, \"S\": 1, \"Q\": 3} mc = ReplaceTransformer(columns=[\"embarked\"], mapping=mapping).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[\"embarked\"].unique() array([1, 0, 3], dtype=int64) ### Replace Multiclass Transformer ReplaceMulticlass class from mlearner.preprocessing import ReplaceMulticlass dataset.X.columns.tolist() ['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] col = ['pclass', 'sex', 'sibsp', 'parch', 'ticket'] mc = ReplaceMulticlass(columns=col).fit(dataset.X) data_X = mc.transform(dataset.X) data_X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 0 Allen, Miss. Elisabeth Walton 0 29.0000 0 0 0 211.3375 B5 S 2 171.0 St Louis, MO 1 0 Allison, Master. Hudson Trevor 1 0.9167 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 2 0 Allison, Miss. Helen Loraine 0 2.0000 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 3 0 Allison, Mr. Hudson Joshua Creighton 1 30.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON 4 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) 0 25.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON","title":"Replace map"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DataCleaner/","text":"DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataCleaner"},{"location":"user_guide/preprocessing/DataCleaner/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"user_guide/preprocessing/DataCleaner/#methods","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"user_guide/preprocessing/DataExploratory/","text":"Preprocessing Data exploratory - EDA Dataset preprocessing examples from mlearner.preprocessing import DataExploratory import numpy as np import pandas as pd from mlearner.preprocessing import DataExploratory Load data in DataExploratory class filename = \"mlearner/data/data/titanic3.csv\" dataset = DataExploratory.load_data(filename, sep=\",\") dataset.data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 5 1 1 Anderson, Mr. Harry male 48 0 0 19952 26.5500 E12 S 3 NaN New York, NY 6 1 1 Andrews, Miss. Kornelia Theodosia female 63 1 0 13502 77.9583 D7 S 10 NaN Hudson, NY 7 1 0 Andrews, Mr. Thomas Jr male 39 0 0 112050 0.0000 A36 S NaN NaN Belfast, NI 8 1 1 Appleton, Mrs. Edward Dale (Charlotte Lamson) female 53 2 0 11769 51.4792 C101 S D NaN Bayside, Queens, NY 9 1 0 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5042 NaN C NaN 22.0 Montevideo, Uruguay 10 1 0 Astor, Col. John Jacob male 47 1 0 PC 17757 227.5250 C62 C64 C NaN 124.0 New York, NY 11 1 1 Astor, Mrs. John Jacob (Madeleine Talmadge Force) female 18 1 0 PC 17757 227.5250 C62 C64 C 4 NaN New York, NY 12 1 1 Aubart, Mme. Leontine Pauline female 24 0 0 PC 17477 69.3000 B35 C 9 NaN Paris, France 13 1 1 Barber, Miss. Ellen \"Nellie\" female 26 0 0 19877 78.8500 NaN S 6 NaN NaN 14 1 1 Barkworth, Mr. Algernon Henry Wilson male 80 0 0 27042 30.0000 A23 S B NaN Hessle, Yorks 15 1 0 Baumann, Mr. John D male NaN 0 0 PC 17318 25.9250 NaN S NaN NaN New York, NY 16 1 0 Baxter, Mr. Quigg Edmond male 24 0 1 PC 17558 247.5208 B58 B60 C NaN NaN Montreal, PQ 17 1 1 Baxter, Mrs. James (Helene DeLaudeniere Chaput) female 50 0 1 PC 17558 247.5208 B58 B60 C 6 NaN Montreal, PQ 18 1 1 Bazzani, Miss. Albina female 32 0 0 11813 76.2917 D15 C 8 NaN NaN 19 1 0 Beattie, Mr. Thomson male 36 0 0 13050 75.2417 C6 C A NaN Winnipeg, MN 20 1 1 Beckwith, Mr. Richard Leonard male 37 1 1 11751 52.5542 D35 S 5 NaN New York, NY 21 1 1 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) female 47 1 1 11751 52.5542 D35 S 5 NaN New York, NY 22 1 1 Behr, Mr. Karl Howell male 26 0 0 111369 30.0000 C148 C 5 NaN New York, NY 23 1 1 Bidois, Miss. Rosalie female 42 0 0 PC 17757 227.5250 NaN C 4 NaN NaN 24 1 1 Bird, Miss. Ellen female 29 0 0 PC 17483 221.7792 C97 S 8 NaN NaN 25 1 0 Birnbaum, Mr. Jakob male 25 0 0 13905 26.0000 NaN C NaN 148.0 San Francisco, CA 26 1 1 Bishop, Mr. Dickinson H male 25 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 27 1 1 Bishop, Mrs. Dickinson H (Helen Walton) female 19 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 28 1 1 Bissette, Miss. Amelia female 35 0 0 PC 17760 135.6333 C99 S 8 NaN NaN 29 1 1 Bjornstrom-Steffansson, Mr. Mauritz Hakan male 28 0 0 110564 26.5500 C52 S D NaN Stockholm, Sweden / Washington, DC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1279 3 0 Vestrom, Miss. Hulda Amanda Adolfina female 14 0 0 350406 7.8542 NaN S NaN NaN NaN 1280 3 0 Vovk, Mr. Janko male 22 0 0 349252 7.8958 NaN S NaN NaN NaN 1281 3 0 Waelens, Mr. Achille male 22 0 0 345767 9.0000 NaN S NaN NaN Antwerp, Belgium / Stanton, OH 1282 3 0 Ware, Mr. Frederick male NaN 0 0 359309 8.0500 NaN S NaN NaN NaN 1283 3 0 Warren, Mr. Charles William male NaN 0 0 C.A. 49867 7.5500 NaN S NaN NaN NaN 1284 3 0 Webber, Mr. James male NaN 0 0 SOTON/OQ 3101316 8.0500 NaN S NaN NaN NaN 1285 3 0 Wenzel, Mr. Linhart male 32.5 0 0 345775 9.5000 NaN S NaN 298.0 NaN 1286 3 1 Whabee, Mrs. George Joseph (Shawneene Abi-Saab) female 38 0 0 2688 7.2292 NaN C C NaN NaN 1287 3 0 Widegren, Mr. Carl/Charles Peter male 51 0 0 347064 7.7500 NaN S NaN NaN NaN 1288 3 0 Wiklund, Mr. Jakob Alfred male 18 1 0 3101267 6.4958 NaN S NaN 314.0 NaN 1289 3 0 Wiklund, Mr. Karl Johan male 21 1 0 3101266 6.4958 NaN S NaN NaN NaN 1290 3 1 Wilkes, Mrs. James (Ellen Needs) female 47 1 0 363272 7.0000 NaN S NaN NaN NaN 1291 3 0 Willer, Mr. Aaron (\"Abi Weller\") male NaN 0 0 3410 8.7125 NaN S NaN NaN NaN 1292 3 0 Willey, Mr. Edward male NaN 0 0 S.O./P.P. 751 7.5500 NaN S NaN NaN NaN 1293 3 0 Williams, Mr. Howard Hugh \"Harry\" male NaN 0 0 A/5 2466 8.0500 NaN S NaN NaN NaN 1294 3 0 Williams, Mr. Leslie male 28.5 0 0 54636 16.1000 NaN S NaN 14.0 NaN 1295 3 0 Windelov, Mr. Einar male 21 0 0 SOTON/OQ 3101317 7.2500 NaN S NaN NaN NaN 1296 3 0 Wirz, Mr. Albert male 27 0 0 315154 8.6625 NaN S NaN 131.0 NaN 1297 3 0 Wiseman, Mr. Phillippe male NaN 0 0 A/4. 34244 7.2500 NaN S NaN NaN NaN 1298 3 0 Wittevrongel, Mr. Camille male 36 0 0 345771 9.5000 NaN S NaN NaN NaN 1299 3 0 Yasbeck, Mr. Antoni male 27 1 0 2659 14.4542 NaN C C NaN NaN 1300 3 1 Yasbeck, Mrs. Antoni (Selini Alexander) female 15 1 0 2659 14.4542 NaN C NaN NaN NaN 1301 3 0 Youseff, Mr. Gerious male 45.5 0 0 2628 7.2250 NaN C NaN 312.0 NaN 1302 3 0 Yousif, Mr. Wazli male NaN 0 0 2647 7.2250 NaN C NaN NaN NaN 1303 3 0 Yousseff, Mr. Gerious male NaN 0 0 2627 14.4583 NaN C NaN NaN NaN 1304 3 0 Zabour, Miss. Hileni female 14.5 1 0 2665 14.4542 NaN C NaN 328.0 NaN 1305 3 0 Zabour, Miss. Thamine female NaN 1 0 2665 14.4542 NaN C NaN NaN NaN 1306 3 0 Zakarian, Mr. Mapriededer male 26.5 0 0 2656 7.2250 NaN C NaN 304.0 NaN 1307 3 0 Zakarian, Mr. Ortin male 27 0 0 2670 7.2250 NaN C NaN NaN NaN 1308 3 0 Zimmerman, Mr. Leo male 29 0 0 315082 7.8750 NaN S NaN NaN NaN 1309 rows \u00d7 14 columns Method: dtypes dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object Method: missing_values dataset.missing_values() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 survived 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Method: isNull dataset.isNull() Cuidado que existen valores nulos True Method: view_features dataset.view_features() ['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] Method: categorical_vs_numerical _, _ = dataset.categorical_vs_numerical() Number of categorical features: 9 Number of numerical features: 5 Method: type_object dataset.type_object() ['name', 'sex', 'age', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'home.dest'] Method: not_type_object dataset.not_type_object() ['pclass', 'survived', 'sibsp', 'parch', 'body'] API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataExploratory/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataExploratory/#data-exploratory-eda","text":"Dataset preprocessing examples from mlearner.preprocessing import DataExploratory import numpy as np import pandas as pd from mlearner.preprocessing import DataExploratory Load data in DataExploratory class filename = \"mlearner/data/data/titanic3.csv\" dataset = DataExploratory.load_data(filename, sep=\",\") dataset.data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 5 1 1 Anderson, Mr. Harry male 48 0 0 19952 26.5500 E12 S 3 NaN New York, NY 6 1 1 Andrews, Miss. Kornelia Theodosia female 63 1 0 13502 77.9583 D7 S 10 NaN Hudson, NY 7 1 0 Andrews, Mr. Thomas Jr male 39 0 0 112050 0.0000 A36 S NaN NaN Belfast, NI 8 1 1 Appleton, Mrs. Edward Dale (Charlotte Lamson) female 53 2 0 11769 51.4792 C101 S D NaN Bayside, Queens, NY 9 1 0 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5042 NaN C NaN 22.0 Montevideo, Uruguay 10 1 0 Astor, Col. John Jacob male 47 1 0 PC 17757 227.5250 C62 C64 C NaN 124.0 New York, NY 11 1 1 Astor, Mrs. John Jacob (Madeleine Talmadge Force) female 18 1 0 PC 17757 227.5250 C62 C64 C 4 NaN New York, NY 12 1 1 Aubart, Mme. Leontine Pauline female 24 0 0 PC 17477 69.3000 B35 C 9 NaN Paris, France 13 1 1 Barber, Miss. Ellen \"Nellie\" female 26 0 0 19877 78.8500 NaN S 6 NaN NaN 14 1 1 Barkworth, Mr. Algernon Henry Wilson male 80 0 0 27042 30.0000 A23 S B NaN Hessle, Yorks 15 1 0 Baumann, Mr. John D male NaN 0 0 PC 17318 25.9250 NaN S NaN NaN New York, NY 16 1 0 Baxter, Mr. Quigg Edmond male 24 0 1 PC 17558 247.5208 B58 B60 C NaN NaN Montreal, PQ 17 1 1 Baxter, Mrs. James (Helene DeLaudeniere Chaput) female 50 0 1 PC 17558 247.5208 B58 B60 C 6 NaN Montreal, PQ 18 1 1 Bazzani, Miss. Albina female 32 0 0 11813 76.2917 D15 C 8 NaN NaN 19 1 0 Beattie, Mr. Thomson male 36 0 0 13050 75.2417 C6 C A NaN Winnipeg, MN 20 1 1 Beckwith, Mr. Richard Leonard male 37 1 1 11751 52.5542 D35 S 5 NaN New York, NY 21 1 1 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) female 47 1 1 11751 52.5542 D35 S 5 NaN New York, NY 22 1 1 Behr, Mr. Karl Howell male 26 0 0 111369 30.0000 C148 C 5 NaN New York, NY 23 1 1 Bidois, Miss. Rosalie female 42 0 0 PC 17757 227.5250 NaN C 4 NaN NaN 24 1 1 Bird, Miss. Ellen female 29 0 0 PC 17483 221.7792 C97 S 8 NaN NaN 25 1 0 Birnbaum, Mr. Jakob male 25 0 0 13905 26.0000 NaN C NaN 148.0 San Francisco, CA 26 1 1 Bishop, Mr. Dickinson H male 25 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 27 1 1 Bishop, Mrs. Dickinson H (Helen Walton) female 19 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 28 1 1 Bissette, Miss. Amelia female 35 0 0 PC 17760 135.6333 C99 S 8 NaN NaN 29 1 1 Bjornstrom-Steffansson, Mr. Mauritz Hakan male 28 0 0 110564 26.5500 C52 S D NaN Stockholm, Sweden / Washington, DC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1279 3 0 Vestrom, Miss. Hulda Amanda Adolfina female 14 0 0 350406 7.8542 NaN S NaN NaN NaN 1280 3 0 Vovk, Mr. Janko male 22 0 0 349252 7.8958 NaN S NaN NaN NaN 1281 3 0 Waelens, Mr. Achille male 22 0 0 345767 9.0000 NaN S NaN NaN Antwerp, Belgium / Stanton, OH 1282 3 0 Ware, Mr. Frederick male NaN 0 0 359309 8.0500 NaN S NaN NaN NaN 1283 3 0 Warren, Mr. Charles William male NaN 0 0 C.A. 49867 7.5500 NaN S NaN NaN NaN 1284 3 0 Webber, Mr. James male NaN 0 0 SOTON/OQ 3101316 8.0500 NaN S NaN NaN NaN 1285 3 0 Wenzel, Mr. Linhart male 32.5 0 0 345775 9.5000 NaN S NaN 298.0 NaN 1286 3 1 Whabee, Mrs. George Joseph (Shawneene Abi-Saab) female 38 0 0 2688 7.2292 NaN C C NaN NaN 1287 3 0 Widegren, Mr. Carl/Charles Peter male 51 0 0 347064 7.7500 NaN S NaN NaN NaN 1288 3 0 Wiklund, Mr. Jakob Alfred male 18 1 0 3101267 6.4958 NaN S NaN 314.0 NaN 1289 3 0 Wiklund, Mr. Karl Johan male 21 1 0 3101266 6.4958 NaN S NaN NaN NaN 1290 3 1 Wilkes, Mrs. James (Ellen Needs) female 47 1 0 363272 7.0000 NaN S NaN NaN NaN 1291 3 0 Willer, Mr. Aaron (\"Abi Weller\") male NaN 0 0 3410 8.7125 NaN S NaN NaN NaN 1292 3 0 Willey, Mr. Edward male NaN 0 0 S.O./P.P. 751 7.5500 NaN S NaN NaN NaN 1293 3 0 Williams, Mr. Howard Hugh \"Harry\" male NaN 0 0 A/5 2466 8.0500 NaN S NaN NaN NaN 1294 3 0 Williams, Mr. Leslie male 28.5 0 0 54636 16.1000 NaN S NaN 14.0 NaN 1295 3 0 Windelov, Mr. Einar male 21 0 0 SOTON/OQ 3101317 7.2500 NaN S NaN NaN NaN 1296 3 0 Wirz, Mr. Albert male 27 0 0 315154 8.6625 NaN S NaN 131.0 NaN 1297 3 0 Wiseman, Mr. Phillippe male NaN 0 0 A/4. 34244 7.2500 NaN S NaN NaN NaN 1298 3 0 Wittevrongel, Mr. Camille male 36 0 0 345771 9.5000 NaN S NaN NaN NaN 1299 3 0 Yasbeck, Mr. Antoni male 27 1 0 2659 14.4542 NaN C C NaN NaN 1300 3 1 Yasbeck, Mrs. Antoni (Selini Alexander) female 15 1 0 2659 14.4542 NaN C NaN NaN NaN 1301 3 0 Youseff, Mr. Gerious male 45.5 0 0 2628 7.2250 NaN C NaN 312.0 NaN 1302 3 0 Yousif, Mr. Wazli male NaN 0 0 2647 7.2250 NaN C NaN NaN NaN 1303 3 0 Yousseff, Mr. Gerious male NaN 0 0 2627 14.4583 NaN C NaN NaN NaN 1304 3 0 Zabour, Miss. Hileni female 14.5 1 0 2665 14.4542 NaN C NaN 328.0 NaN 1305 3 0 Zabour, Miss. Thamine female NaN 1 0 2665 14.4542 NaN C NaN NaN NaN 1306 3 0 Zakarian, Mr. Mapriededer male 26.5 0 0 2656 7.2250 NaN C NaN 304.0 NaN 1307 3 0 Zakarian, Mr. Ortin male 27 0 0 2670 7.2250 NaN C NaN NaN NaN 1308 3 0 Zimmerman, Mr. Leo male 29 0 0 315082 7.8750 NaN S NaN NaN NaN 1309 rows \u00d7 14 columns Method: dtypes dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object Method: missing_values dataset.missing_values() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 survived 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Method: isNull dataset.isNull() Cuidado que existen valores nulos True Method: view_features dataset.view_features() ['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] Method: categorical_vs_numerical _, _ = dataset.categorical_vs_numerical() Number of categorical features: 9 Number of numerical features: 5 Method: type_object dataset.type_object() ['name', 'sex', 'age', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'home.dest'] Method: not_type_object dataset.not_type_object() ['pclass', 'survived', 'sibsp', 'parch', 'body']","title":"Data exploratory - EDA"},{"location":"user_guide/preprocessing/DataExploratory/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DropOutliers/","text":"DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"DropOutliers"},{"location":"user_guide/preprocessing/DropOutliers/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"user_guide/preprocessing/DropOutliers/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"user_guide/preprocessing/ExtractCategories/","text":"ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ExtractCategories"},{"location":"user_guide/preprocessing/ExtractCategories/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"user_guide/preprocessing/ExtractCategories/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FeatureDropper/","text":"FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"FeatureDropper"},{"location":"user_guide/preprocessing/FeatureDropper/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"user_guide/preprocessing/FeatureDropper/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_all/","text":"FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer all"},{"location":"user_guide/preprocessing/FillNaTransformer_all/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"user_guide/preprocessing/FillNaTransformer_all/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_any/","text":"FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer any"},{"location":"user_guide/preprocessing/FillNaTransformer_any/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"user_guide/preprocessing/FillNaTransformer_any/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/","text":"FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer backward"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/","text":"FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer forward"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/","text":"FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer idmax"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/","text":"FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer mean"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_median/","text":"FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer median"},{"location":"user_guide/preprocessing/FillNaTransformer_median/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"user_guide/preprocessing/FillNaTransformer_median/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_value/","text":"FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer value"},{"location":"user_guide/preprocessing/FillNaTransformer_value/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"user_guide/preprocessing/FillNaTransformer_value/#methods","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FixSkewness/","text":"FixSkewness FixSkewness(columns=None) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"FixSkewness"},{"location":"user_guide/preprocessing/FixSkewness/#fixskewness","text":"FixSkewness(columns=None) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"user_guide/preprocessing/FixSkewness/#methods","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"user_guide/preprocessing/MeanCenterer/","text":"MeanCenterer MeanCenterer() Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"MeanCenterer"},{"location":"user_guide/preprocessing/MeanCenterer/#meancenterer","text":"MeanCenterer() Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"user_guide/preprocessing/MeanCenterer/#methods","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"user_guide/preprocessing/OneHotEncoder/","text":"OneHotEncoder OneHotEncoder(columns=None, numerical=[]) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"OneHotEncoder"},{"location":"user_guide/preprocessing/OneHotEncoder/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[]) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"user_guide/preprocessing/OneHotEncoder/#methods","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"user_guide/preprocessing/ReplaceMulticlass/","text":"ReplaceMulticlass ReplaceMulticlass(columns=None, mapping=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceMulticlass"},{"location":"user_guide/preprocessing/ReplaceMulticlass/#replacemulticlass","text":"ReplaceMulticlass(columns=None, mapping=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"user_guide/preprocessing/ReplaceMulticlass/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/ReplaceTransformer/","text":"ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceTransformer"},{"location":"user_guide/preprocessing/ReplaceTransformer/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"user_guide/preprocessing/ReplaceTransformer/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/minmax_scaling/","text":"minmax_scaling minmax_scaling(array, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Minmax scaling"},{"location":"user_guide/preprocessing/minmax_scaling/#minmax_scaling","text":"minmax_scaling(array, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"user_guide/training/Training/","text":"Training Training(model, random_state=99) None Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Training"},{"location":"user_guide/training/Training/#training","text":"Training(model, random_state=99) None","title":"Training"},{"location":"user_guide/training/Training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/utils/ParamsManager/","text":"ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes mtodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None","title":"ParamsManager"},{"location":"user_guide/utils/ParamsManager/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes mtodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"user_guide/utils/ParamsManager/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"}]}