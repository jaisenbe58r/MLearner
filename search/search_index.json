{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MLearner's documentation! MLearner is a Python library of useful tools for the day-to-day data science tasks. Links Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner Questions? Check out the Discord group MLearner Examples License MIT License Copyright (c) 2018-2022 Jaime Sendra Berenguer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contact I received a lot of feedback and questions about mlearner recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlearner, please consider posting it here since it can also be useful to others! Please join the Discord group MLearner If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Home"},{"location":"#welcome-to-mlearners-documentation","text":"MLearner is a Python library of useful tools for the day-to-day data science tasks.","title":"Welcome to MLearner's documentation!"},{"location":"#links","text":"Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner Questions? Check out the Discord group MLearner","title":"Links"},{"location":"#examples","text":"","title":"Examples"},{"location":"#license","text":"MIT License Copyright (c) 2018-2022 Jaime Sendra Berenguer Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"#contact","text":"I received a lot of feedback and questions about mlearner recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlearner, please consider posting it here since it can also be useful to others! Please join the Discord group MLearner If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Contact"},{"location":"CHANGELOG/","text":"Release Notes The CHANGELOG for the current development version is available at https://github.com/jaisenbe58r/MLearner/blob/master/docs/sources/CHANGELOG.md . Version 0.1.0 (2018-2022-04-17) First Version","title":"Release Notes"},{"location":"CHANGELOG/#release-notes","text":"The CHANGELOG for the current development version is available at https://github.com/jaisenbe58r/MLearner/blob/master/docs/sources/CHANGELOG.md .","title":"Release Notes"},{"location":"CHANGELOG/#version-010-2018-2022-04-17","text":"First Version","title":"Version 0.1.0 (2018-2022-04-17)"},{"location":"CONTRIBUTING/","text":"How to Contribute I would be very happy about any kind of contributions that help to improve and extend the functionality of mlearner. Quick Contributor Checklist This is a quick checklist about the different steps of a typical contribution to mlearner (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go. [ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlearner repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlearner/*/tests [ ] Run PYTHONPATH='.' pytest ./mlearner -sv and make sure that all unit tests pass [ ] Check for style issues by running flake8 ./mlearner (you may want to run pytest again after you made modifications to the code) [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file [ ] Modify documentation in the appropriate location under mlearner/docs/sources/ [ ] Push the topic branch to the server and create a pull request [ ] Check the Travis-CI build passed at https://travis-ci.org/jaisenbe58r/mlearner [ ] Check/improve the unit test coverage at https://coveralls.io/github/jaisenbe58r/mlearner [ ] Check/improve the code health at https://landscape.io/github/jaisenbe58r/mlearner Tips for Contributors Getting Started - Creating a New Issue and Forking the Repository If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlearner repository from the GitHub web interface. Clone the mlearner repository to your local machine by executing git clone https://github.com/<your_username>/MLearner.git Syncing an Existing Fork If you already forked mlearner earlier, you can bring you \"Fork\" up to date with the master branch as follows: 1. Configuring a remote that points to the upstream repository on GitHub List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/jaisenbe58r/MLearner.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) upstream https://github.com/jaisenbe58r/MLearner.git (fetch) upstream https://github.com/jaisenbe58r/MLearner.git (push) 2. Syncing your Fork First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/jaisenbe58r/MLearner * [new branch] master -> upstream/master This means that the commits to the jaisenbe58r/mlearner master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+), *The Main Workflow - Making Changes in a New Topic Branch Listed below are the 9 typical steps of a contribution. 1. Discussing the Feature or Modification Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already. 2. Creating a new feature branch Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature> 3. Developing the new feature / bug fix Now it's time to modify existing code or to contribute new code to the project. 4. Testing your code Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlearner ---with-coverage 5. Documenting changes Please add an entry to the mlearner/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlearner/sources . 6. Committing changes When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>' 7. Optional: squashing commits If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions. 8. Uploading changes Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch> 9. Submitting a pull request Go to your GitHub repository online, select the new feature branch, and submit a new pull request: Notes for Developers Building the documentation The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlearner/docs directory. For example, ~/github/mlearner/docs$ mkdocs serve 1. Building the API documentation To build the API documentation, navigate to mlearner/docs and execute the make_api.py file from this directory via ~/github/mlearner/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlearner/docs/sources/api_modules mlearner/docs/sources/api_subpackes 2. Editing the User Guide The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlearner/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlearner/docs/mkdocs.yml file. 3. Building static HTML files of the documentation First, please check the documenation via localhost (http://127.0.0.1:8000/): ~/github/mlearner/docs$ mkdocs serve Next, build the static HTML files of the mlearner documentation via ~/github/mlearner/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlearner/docs$ mkdocs gh-deploy --clean 4. Generate a PDF of the documentation To generate a PDF version of the documentation, simply cd into the mlearner/docs directory and execute: python md2pdf.py Uploading a new version to PyPI 1. Creating a new testing environment Assuming we are using conda , create a new python environment via $ conda create -n 'mlearner-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlearner-testing 2. Installing the package from local files Test the installation by executing $ python setup.py install --record files.txt the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlearner; print(mlearner.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip install mlearner and uninstall it again $ pip uninstall mlearner 3. Deploying the package Consider deploying the package to the PyPI test server first. The setup instructions can be found here . $ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi Test if it can be installed from there by executing $ pip install -i https://testpypi.python.org/pypi mlearner and uninstall it $ pip uninstall mlearner After this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python setup.py sdist bdist_wheel upload 4. Removing the virtual environment Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlearner-testing' --all 5. Updating the conda-forge recipe Once a new version of mlearner has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlearner-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"How To Contribute"},{"location":"CONTRIBUTING/#how-to-contribute","text":"I would be very happy about any kind of contributions that help to improve and extend the functionality of mlearner.","title":"How to Contribute"},{"location":"CONTRIBUTING/#quick-contributor-checklist","text":"This is a quick checklist about the different steps of a typical contribution to mlearner (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go. [ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlearner repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlearner/*/tests [ ] Run PYTHONPATH='.' pytest ./mlearner -sv and make sure that all unit tests pass [ ] Check for style issues by running flake8 ./mlearner (you may want to run pytest again after you made modifications to the code) [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file [ ] Modify documentation in the appropriate location under mlearner/docs/sources/ [ ] Push the topic branch to the server and create a pull request [ ] Check the Travis-CI build passed at https://travis-ci.org/jaisenbe58r/mlearner [ ] Check/improve the unit test coverage at https://coveralls.io/github/jaisenbe58r/mlearner [ ] Check/improve the code health at https://landscape.io/github/jaisenbe58r/mlearner","title":"Quick Contributor Checklist"},{"location":"CONTRIBUTING/#tips-for-contributors","text":"","title":"Tips for Contributors"},{"location":"CONTRIBUTING/#getting-started-creating-a-new-issue-and-forking-the-repository","text":"If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlearner repository from the GitHub web interface. Clone the mlearner repository to your local machine by executing git clone https://github.com/<your_username>/MLearner.git","title":"Getting Started - Creating a New Issue and Forking the Repository"},{"location":"CONTRIBUTING/#syncing-an-existing-fork","text":"If you already forked mlearner earlier, you can bring you \"Fork\" up to date with the master branch as follows:","title":"Syncing an Existing Fork"},{"location":"CONTRIBUTING/#1-configuring-a-remote-that-points-to-the-upstream-repository-on-github","text":"List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/jaisenbe58r/MLearner.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/MLearner.git (fetch) origin https://github.com/<your username>/MLearner.git (push) upstream https://github.com/jaisenbe58r/MLearner.git (fetch) upstream https://github.com/jaisenbe58r/MLearner.git (push)","title":"1. Configuring a remote that points to the upstream repository on GitHub"},{"location":"CONTRIBUTING/#2-syncing-your-fork","text":"First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/jaisenbe58r/MLearner * [new branch] master -> upstream/master This means that the commits to the jaisenbe58r/mlearner master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+),","title":"2. Syncing your Fork"},{"location":"CONTRIBUTING/#the-main-workflow-making-changes-in-a-new-topic-branch","text":"Listed below are the 9 typical steps of a contribution.","title":"*The Main Workflow - Making Changes in a New Topic Branch"},{"location":"CONTRIBUTING/#1-discussing-the-feature-or-modification","text":"Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already.","title":"1. Discussing the Feature or Modification"},{"location":"CONTRIBUTING/#2-creating-a-new-feature-branch","text":"Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature>","title":"2. Creating a new feature branch"},{"location":"CONTRIBUTING/#3-developing-the-new-feature-bug-fix","text":"Now it's time to modify existing code or to contribute new code to the project.","title":"3. Developing the new feature / bug fix"},{"location":"CONTRIBUTING/#4-testing-your-code","text":"Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlearner ---with-coverage","title":"4. Testing your code"},{"location":"CONTRIBUTING/#5-documenting-changes","text":"Please add an entry to the mlearner/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlearner/sources .","title":"5. Documenting changes"},{"location":"CONTRIBUTING/#6-committing-changes","text":"When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>'","title":"6. Committing changes"},{"location":"CONTRIBUTING/#7-optional-squashing-commits","text":"If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: jaisenbe58r <jaisenberafel@gmail.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions.","title":"7. Optional: squashing commits"},{"location":"CONTRIBUTING/#8-uploading-changes","text":"Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch>","title":"8. Uploading changes"},{"location":"CONTRIBUTING/#9-submitting-a-pull-request","text":"Go to your GitHub repository online, select the new feature branch, and submit a new pull request:","title":"9. Submitting a pull request"},{"location":"CONTRIBUTING/#notes-for-developers","text":"","title":"Notes for Developers"},{"location":"CONTRIBUTING/#building-the-documentation","text":"The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlearner/docs directory. For example, ~/github/mlearner/docs$ mkdocs serve","title":"Building the documentation"},{"location":"CONTRIBUTING/#1-building-the-api-documentation","text":"To build the API documentation, navigate to mlearner/docs and execute the make_api.py file from this directory via ~/github/mlearner/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlearner/docs/sources/api_modules mlearner/docs/sources/api_subpackes","title":"1. Building the API documentation"},{"location":"CONTRIBUTING/#2-editing-the-user-guide","text":"The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlearner/docs$ python ipynb2markdown.py --ipynb_path ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlearner/docs/mkdocs.yml file.","title":"2. Editing the User Guide"},{"location":"CONTRIBUTING/#3-building-static-html-files-of-the-documentation","text":"First, please check the documenation via localhost (http://127.0.0.1:8000/): ~/github/mlearner/docs$ mkdocs serve Next, build the static HTML files of the mlearner documentation via ~/github/mlearner/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlearner/docs$ mkdocs gh-deploy --clean","title":"3. Building static HTML files of the documentation"},{"location":"CONTRIBUTING/#4-generate-a-pdf-of-the-documentation","text":"To generate a PDF version of the documentation, simply cd into the mlearner/docs directory and execute: python md2pdf.py","title":"4. Generate a PDF of the documentation"},{"location":"CONTRIBUTING/#uploading-a-new-version-to-pypi","text":"","title":"Uploading a new version to PyPI"},{"location":"CONTRIBUTING/#1-creating-a-new-testing-environment","text":"Assuming we are using conda , create a new python environment via $ conda create -n 'mlearner-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlearner-testing","title":"1. Creating a new testing environment"},{"location":"CONTRIBUTING/#2-installing-the-package-from-local-files","text":"Test the installation by executing $ python setup.py install --record files.txt the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlearner; print(mlearner.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip install mlearner and uninstall it again $ pip uninstall mlearner","title":"2. Installing the package from local files"},{"location":"CONTRIBUTING/#3-deploying-the-package","text":"Consider deploying the package to the PyPI test server first. The setup instructions can be found here . $ python setup.py sdist bdist_wheel upload -r https://testpypi.python.org/pypi Test if it can be installed from there by executing $ pip install -i https://testpypi.python.org/pypi mlearner and uninstall it $ pip uninstall mlearner After this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python setup.py sdist bdist_wheel upload","title":"3. Deploying the package"},{"location":"CONTRIBUTING/#4-removing-the-virtual-environment","text":"Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlearner-testing' --all","title":"4. Removing the virtual environment"},{"location":"CONTRIBUTING/#5-updating-the-conda-forge-recipe","text":"Once a new version of mlearner has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlearner-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"5. Updating the conda-forge recipe"},{"location":"USER_GUIDE_INDEX/","text":"User Guide Index load DataLoad data wine_data data_normal data_gamma data_uniform create_dataset preprocessing MeanCenterer minmax_scaling FeatureDropper FillNaTransformer_median FillNaTransformer_mean FillNaTransformer_idmax FillNaTransformer_any FillNaTransformer_all FillNaTransformer_value FillNaTransformer_backward FillNaTransformer_forward FixSkewness OneHotEncoder DropOutliers ExtractCategories ReplaceMulticlass ReplaceTransformer DataCleaner DataAnalyst feature selections FeatureSelection models modelXGBoost modelLightBoost modelCatBoost clasifier PipelineClasificators TrainingUtilities training Training evaluation EvaluationModels utils ParamsManager nlp boundary CountVectorizer DCNN find_at find_capital find_dates find_dollar find_domain find_email find_emoji find_hash find_nonalp find_number find_phone_number find_punct find_url find_year ip_add lat_lon mac_add neg_look_ahead neg_look_behind ngrams_top num_great num_less only_words open_txt pick_only_key_sentence pick_unique_sentence pos_look_ahead pos_look_behind Processor_data remove_emoji remove_tag search_string subword unique_char","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#user-guide-index","text":"","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#load","text":"DataLoad","title":"load"},{"location":"USER_GUIDE_INDEX/#data","text":"wine_data data_normal data_gamma data_uniform create_dataset","title":"data"},{"location":"USER_GUIDE_INDEX/#preprocessing","text":"MeanCenterer minmax_scaling FeatureDropper FillNaTransformer_median FillNaTransformer_mean FillNaTransformer_idmax FillNaTransformer_any FillNaTransformer_all FillNaTransformer_value FillNaTransformer_backward FillNaTransformer_forward FixSkewness OneHotEncoder DropOutliers ExtractCategories ReplaceMulticlass ReplaceTransformer DataCleaner DataAnalyst","title":"preprocessing"},{"location":"USER_GUIDE_INDEX/#feature-selections","text":"FeatureSelection","title":"feature selections"},{"location":"USER_GUIDE_INDEX/#models","text":"modelXGBoost modelLightBoost modelCatBoost","title":"models"},{"location":"USER_GUIDE_INDEX/#clasifier","text":"PipelineClasificators TrainingUtilities","title":"clasifier"},{"location":"USER_GUIDE_INDEX/#training","text":"Training","title":"training"},{"location":"USER_GUIDE_INDEX/#evaluation","text":"EvaluationModels","title":"evaluation"},{"location":"USER_GUIDE_INDEX/#utils","text":"ParamsManager","title":"utils"},{"location":"USER_GUIDE_INDEX/#nlp","text":"boundary CountVectorizer DCNN find_at find_capital find_dates find_dollar find_domain find_email find_emoji find_hash find_nonalp find_number find_phone_number find_punct find_url find_year ip_add lat_lon mac_add neg_look_ahead neg_look_behind ngrams_top num_great num_less only_words open_txt pick_only_key_sentence pick_unique_sentence pos_look_ahead pos_look_behind Processor_data remove_emoji remove_tag search_string subword unique_char","title":"nlp"},{"location":"cite/","text":"Citing mlearner","title":"Citing mlearner"},{"location":"cite/#citing-mlearner","text":"","title":"Citing mlearner"},{"location":"contributors/","text":"Contributors For the current list of contributors to mlearner, please see the GitHub contributor page at [https://github.com/jaisenbe58r/MLearner/graphs/contributors].","title":"Contributors"},{"location":"contributors/#contributors","text":"For the current list of contributors to mlearner, please see the GitHub contributor page at [https://github.com/jaisenbe58r/MLearner/graphs/contributors].","title":"Contributors"},{"location":"discuss/","text":"Discuss Any questions or comments about mlearner? Join the mlearner mailing list on Google Groups!","title":"Discuss"},{"location":"discuss/#discuss","text":"Any questions or comments about mlearner? Join the mlearner mailing list on Google Groups!","title":"Discuss"},{"location":"installation/","text":"Installing mlearner PyPI To install mlearner, just execute pip install mlearner Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlearner , unzip it, navigate into the package, and use the command: python setup.py install Upgrading via pip To upgrade an existing version of mlearner from PyPI, execute pip install mlearner --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this. Installing mlearner from the source distribution In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlearner from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlearner from the source distribution instead via pip install --no-binary :all: mlearner Also, I would appreciate it if you could report any issues that occur when using pip install mlearner in hope that we can fix these in future releases. Conda The mlearner package is also available through conda forge . To install mlearner using conda, use the following command: conda install mlearner --channel conda-forge or simply conda install mlearner if you added conda-forge to your channels ( conda config --add channels conda-forge ). Dev Version The mlearner version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/jaisenbe58r/MLearner.git Or, you can fork the GitHub repository from https://github.com/jaisenbe58r/MLearner and install mlearner from your local drive via python setup.py install","title":"Installation"},{"location":"installation/#installing-mlearner","text":"","title":"Installing mlearner"},{"location":"installation/#pypi","text":"To install mlearner, just execute pip install mlearner Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlearner , unzip it, navigate into the package, and use the command: python setup.py install","title":"PyPI"},{"location":"installation/#upgrading-via-pip","text":"To upgrade an existing version of mlearner from PyPI, execute pip install mlearner --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this.","title":"Upgrading via pip"},{"location":"installation/#installing-mlearner-from-the-source-distribution","text":"In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlearner from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlearner from the source distribution instead via pip install --no-binary :all: mlearner Also, I would appreciate it if you could report any issues that occur when using pip install mlearner in hope that we can fix these in future releases.","title":"Installing mlearner from the source distribution"},{"location":"installation/#conda","text":"The mlearner package is also available through conda forge . To install mlearner using conda, use the following command: conda install mlearner --channel conda-forge or simply conda install mlearner if you added conda-forge to your channels ( conda config --add channels conda-forge ).","title":"Conda"},{"location":"installation/#dev-version","text":"The mlearner version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/jaisenbe58r/MLearner.git Or, you can fork the GitHub repository from https://github.com/jaisenbe58r/MLearner and install mlearner from your local drive via python setup.py install","title":"Dev Version"},{"location":"license/","text":"This project is released under a permissive new BSD open source license and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose. In addition, you may use, copy, modify, and redistribute all artistic creative works (figures and images) included in this distribution under the directory according to the terms and conditions of the Creative Commons Attribution 4.0 International License. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above). new BSD License New BSD License Copyright (c) 2014-2020, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlearner nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Creative Commons Attribution 4.0 International License mlearner documentation figures are licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by-sa/4.0/ . You are free to: Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"License"},{"location":"license/#new-bsd-license","text":"New BSD License Copyright (c) 2014-2020, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlearner nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"new BSD License"},{"location":"license/#creative-commons-attribution-40-international-license","text":"mlearner documentation figures are licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by-sa/4.0/ .","title":"Creative Commons Attribution 4.0 International License"},{"location":"license/#you-are-free-to","text":"Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"license/#under-the-following-terms","text":"Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms:"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/","text":"PipelineClasificators PipelineClasificators(random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"PipelineClasificators"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/#pipelineclasificators","text":"PipelineClasificators(random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"PipelineClasificators"},{"location":"api_modules/mlearner.classifier/PipelineClasificators/#methods","text":"Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/","text":"TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"TrainingUtilities"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"api_modules/mlearner.classifier/TrainingUtilities/#methods","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.classifier/wrapper_model/","text":"wrapper_model wrapper_model(clf, pipeline_preprocess, random_state=99, name='model', select=False) Wrapper for Estimator. Methods Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True, report=False) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None buid_Pipeline(pipeline_preprocess, threshold='median', select=True) None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None restore_pipeline_v1(filename, random_state=99, name='Pipeline_model') None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Wrapper model"},{"location":"api_modules/mlearner.classifier/wrapper_model/#wrapper_model","text":"wrapper_model(clf, pipeline_preprocess, random_state=99, name='model', select=False) Wrapper for Estimator.","title":"wrapper_model"},{"location":"api_modules/mlearner.classifier/wrapper_model/#methods","text":"Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True, report=False) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None buid_Pipeline(pipeline_preprocess, threshold='median', select=True) None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None restore_pipeline_v1(filename, random_state=99, name='Pipeline_model') None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_modules/mlearner.classifier/wrapper_pipeline/","text":"wrapper_pipeline wrapper_pipeline(filename, name='model', random_state=99) Wrapper for Estimator. Methods Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Wrapper pipeline"},{"location":"api_modules/mlearner.classifier/wrapper_pipeline/#wrapper_pipeline","text":"wrapper_pipeline(filename, name='model', random_state=99) Wrapper for Estimator.","title":"wrapper_pipeline"},{"location":"api_modules/mlearner.classifier/wrapper_pipeline/#methods","text":"Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_modules/mlearner.data/create_dataset/","text":"create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"Create dataset"},{"location":"api_modules/mlearner.data/create_dataset/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"api_modules/mlearner.data/data_gamma/","text":"data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"Data gamma"},{"location":"api_modules/mlearner.data/data_gamma/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"api_modules/mlearner.data/data_normal/","text":"data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"Data normal"},{"location":"api_modules/mlearner.data/data_normal/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"api_modules/mlearner.data/data_uniform/","text":"data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"Data uniform"},{"location":"api_modules/mlearner.data/data_uniform/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"api_modules/mlearner.data/wine_data/","text":"wine_data wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Wine data"},{"location":"api_modules/mlearner.data/wine_data/#wine_data","text":"wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"wine_data"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/","text":"EvaluationModels EvaluationModels(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"EvaluationModels"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/#evaluationmodels","text":"EvaluationModels(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"EvaluationModels"},{"location":"api_modules/mlearner.evaluation/EvaluationModels/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.externals/check_is_fitted/","text":"check_is_fitted check_is_fitted(estimator, attributes, msg=None, all_or_any= ) Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of \"all_or_any\" of the passed attributes and raises a NotFittedError with the given message. Parameters estimator : estimator instance. estimator instance for which the check is performed. attributes : attribute name(s) given as string or a list/tuple of strings Eg.: [\"coef_\", \"estimator_\", ...], \"coef_\" msg : string The default error message is, \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" For custom messages if \"%(name)s\" is present in the message string, it is substituted for the estimator name. Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\". all_or_any : callable, {all, any}, default all Specify whether all or any of the given attributes must exist. Returns None Raises NotFittedError If the attributes are not found.","title":"Check is fitted"},{"location":"api_modules/mlearner.externals/check_is_fitted/#check_is_fitted","text":"check_is_fitted(estimator, attributes, msg=None, all_or_any= ) Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of \"all_or_any\" of the passed attributes and raises a NotFittedError with the given message. Parameters estimator : estimator instance. estimator instance for which the check is performed. attributes : attribute name(s) given as string or a list/tuple of strings Eg.: [\"coef_\", \"estimator_\", ...], \"coef_\" msg : string The default error message is, \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" For custom messages if \"%(name)s\" is present in the message string, it is substituted for the estimator name. Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\". all_or_any : callable, {all, any}, default all Specify whether all or any of the given attributes must exist. Returns None Raises NotFittedError If the attributes are not found.","title":"check_is_fitted"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/","text":"FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y, n_estimators=100) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"FeatureSelection"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"api_modules/mlearner.feature_selection/FeatureSelection/#methods","text":"LightGBM(X, y, n_estimators=100) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"api_modules/mlearner.images/load_image/","text":"load_image load_image(filename) None","title":"Load image"},{"location":"api_modules/mlearner.images/load_image/#load_image","text":"load_image(filename) None","title":"load_image"},{"location":"api_modules/mlearner.images/plot_image/","text":"plot_image plot_image(img, name='Image', save=False, logdir_report='/images') Grafico Imagen.","title":"Plot image"},{"location":"api_modules/mlearner.images/plot_image/#plot_image","text":"plot_image(img, name='Image', save=False, logdir_report='/images') Grafico Imagen.","title":"plot_image"},{"location":"api_modules/mlearner.images/plot_image2/","text":"plot_image2 plot_image2(img1, img2, title='Images', save=False, logdir_report='/images') Grafico Imagen.","title":"Plot image2"},{"location":"api_modules/mlearner.images/plot_image2/#plot_image2","text":"plot_image2(img1, img2, title='Images', save=False, logdir_report='/images') Grafico Imagen.","title":"plot_image2"},{"location":"api_modules/mlearner.load/DataLoad/","text":"DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"DataLoad"},{"location":"api_modules/mlearner.load/DataLoad/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"api_modules/mlearner.load/DataLoad/#methods","text":"load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"api_modules/mlearner.models/modelCatBoost/","text":"modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"modelCatBoost"},{"location":"api_modules/mlearner.models/modelCatBoost/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"api_modules/mlearner.models/modelCatBoost/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"api_modules/mlearner.models/modelLightBoost/","text":"modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', params=None, args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelLightBoost"},{"location":"api_modules/mlearner.models/modelLightBoost/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', params=None, args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"api_modules/mlearner.models/modelLightBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.models/modelXGBoost/","text":"modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', params=None, args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelXGBoost"},{"location":"api_modules/mlearner.models/modelXGBoost/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', params=None, args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"api_modules/mlearner.models/modelXGBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.neural/Neural/","text":"Neural Neural(clf, random_state=99, name='Neural_model') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods Evaluation_model(X_train, X_test, y_train, y_test, clases=[0, 1], save=True, n_splits=10, ROC=True, path='checkpoints/', params) None Pipeline_train(X, y, X_train, X_test, y_train, y_test, n_splits=10, clases=[0, 1], ROC=True, path='checkpoints/', params) None cuarentena(X, y, IDs) None fit(X, y, params) None fit_cv(X, y, n_splits=10, shuffle=True, random_state=99, mute=False, display=True, params) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None plot_history(n_history) Grafica Resultados. predict(X, y=None) None predict_proba(X, y=None) None restore(filename, random_state=99, name='Neural_model') None save_general(path, X_train, y_train) None save_model(path, name=None) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Neural"},{"location":"api_modules/mlearner.neural/Neural/#neural","text":"Neural(clf, random_state=99, name='Neural_model') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"Neural"},{"location":"api_modules/mlearner.neural/Neural/#methods","text":"Evaluation_model(X_train, X_test, y_train, y_test, clases=[0, 1], save=True, n_splits=10, ROC=True, path='checkpoints/', params) None Pipeline_train(X, y, X_train, X_test, y_train, y_test, n_splits=10, clases=[0, 1], ROC=True, path='checkpoints/', params) None cuarentena(X, y, IDs) None fit(X, y, params) None fit_cv(X, y, n_splits=10, shuffle=True, random_state=99, mute=False, display=True, params) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None plot_history(n_history) Grafica Resultados. predict(X, y=None) None predict_proba(X, y=None) None restore(filename, random_state=99, name='Neural_model') None save_general(path, X_train, y_train) None save_model(path, name=None) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_modules/mlearner.neural/Neural_sklearn/","text":"Neural_sklearn Neural_sklearn(random_state=99, name='Neural_model_sklearn') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods Evaluation_model(X, y, clases=[0, 1], save=True, n_splits=10, ROC=True, binary=True, path='checkpoints/', params) None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None buid_Pipeline(fn_clf, pipeline_preprocess, params) Example: def fn_clf(optimizer=tf.keras.optimizers.Adam(1e-3), kernel_initializer='glorot_uniform', dropout=0.2): model = tf.keras.models.Sequential() model.add(tf.keras.Input(shape=(8,))) model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_initializer=kernel_initializer)) model.add(tf.keras.layers.Dropout(dropout)) model.add(tf.keras.layers.Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer)) model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model params: { nb_epoch:100, batch_size:32, verbose:0 } build_param_grid(param_grid) None cuarentena(X, y, IDs) None fit(X, y) None fit_cv(X, y, n_splits=10, shuffle=True, scoring='accuracy', mute=False, display=True) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X, y=None, binary=True) None predict_proba(X, y=None) None restore_pipeline(filename, random_state=99, name='Neural_model_sklearn') None save(path, name=None) None save_general(path, X_train, y_train) None score(X, y, binary=True) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Neural sklearn"},{"location":"api_modules/mlearner.neural/Neural_sklearn/#neural_sklearn","text":"Neural_sklearn(random_state=99, name='Neural_model_sklearn') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"Neural_sklearn"},{"location":"api_modules/mlearner.neural/Neural_sklearn/#methods","text":"Evaluation_model(X, y, clases=[0, 1], save=True, n_splits=10, ROC=True, binary=True, path='checkpoints/', params) None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None buid_Pipeline(fn_clf, pipeline_preprocess, params) Example: def fn_clf(optimizer=tf.keras.optimizers.Adam(1e-3), kernel_initializer='glorot_uniform', dropout=0.2): model = tf.keras.models.Sequential() model.add(tf.keras.Input(shape=(8,))) model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_initializer=kernel_initializer)) model.add(tf.keras.layers.Dropout(dropout)) model.add(tf.keras.layers.Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer)) model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model params: { nb_epoch:100, batch_size:32, verbose:0 } build_param_grid(param_grid) None cuarentena(X, y, IDs) None fit(X, y) None fit_cv(X, y, n_splits=10, shuffle=True, scoring='accuracy', mute=False, display=True) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X, y=None, binary=True) None predict_proba(X, y=None) None restore_pipeline(filename, random_state=99, name='Neural_model_sklearn') None save(path, name=None) None save_general(path, X_train, y_train) None score(X, y, binary=True) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_modules/mlearner.nlp/CountVectorizer/","text":"CountVectorizer CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling. Methods build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix.","title":"CountVectorizer"},{"location":"api_modules/mlearner.nlp/CountVectorizer/#countvectorizer","text":"CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.","title":"CountVectorizer"},{"location":"api_modules/mlearner.nlp/CountVectorizer/#methods","text":"build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix.","title":"Methods"},{"location":"api_modules/mlearner.nlp/DCNN/","text":"DCNN DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results) Methods add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. Properties activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables.","title":"DCNN"},{"location":"api_modules/mlearner.nlp/DCNN/#dcnn","text":"DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results)","title":"DCNN"},{"location":"api_modules/mlearner.nlp/DCNN/#methods","text":"add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope.","title":"Methods"},{"location":"api_modules/mlearner.nlp/DCNN/#properties","text":"activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables.","title":"Properties"},{"location":"api_modules/mlearner.nlp/Processor_data/","text":"Processor_data Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data) Methods apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Padding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding","title":"Processor data"},{"location":"api_modules/mlearner.nlp/Processor_data/#processor_data","text":"Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data)","title":"Processor_data"},{"location":"api_modules/mlearner.nlp/Processor_data/#methods","text":"apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Padding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding","title":"Methods"},{"location":"api_modules/mlearner.nlp/boundary/","text":"boundary boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral'","title":"Boundary"},{"location":"api_modules/mlearner.nlp/boundary/#boundary","text":"boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral'","title":"boundary"},{"location":"api_modules/mlearner.nlp/find_at/","text":"find_at find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David'","title":"Find at"},{"location":"api_modules/mlearner.nlp/find_at/#find_at","text":"find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David'","title":"find_at"},{"location":"api_modules/mlearner.nlp/find_capital/","text":"find_capital find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God']","title":"Find capital"},{"location":"api_modules/mlearner.nlp/find_capital/#find_capital","text":"find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God']","title":"find_capital"},{"location":"api_modules/mlearner.nlp/find_coin/","text":"find_coin find_coin(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"Find coin"},{"location":"api_modules/mlearner.nlp/find_coin/#find_coin","text":"find_coin(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"find_coin"},{"location":"api_modules/mlearner.nlp/find_dates/","text":"find_dates find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')]","title":"Find dates"},{"location":"api_modules/mlearner.nlp/find_dates/#find_dates","text":"find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')]","title":"find_dates"},{"location":"api_modules/mlearner.nlp/find_dollar/","text":"find_dollar find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"Find dollar"},{"location":"api_modules/mlearner.nlp/find_dollar/#find_dollar","text":"find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"find_dollar"},{"location":"api_modules/mlearner.nlp/find_domain/","text":"find_domain find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org']","title":"Find domain"},{"location":"api_modules/mlearner.nlp/find_domain/#find_domain","text":"find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org']","title":"find_domain"},{"location":"api_modules/mlearner.nlp/find_email/","text":"find_email find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com'","title":"Find email"},{"location":"api_modules/mlearner.nlp/find_email/#find_email","text":"find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com'","title":"find_email"},{"location":"api_modules/mlearner.nlp/find_emoji/","text":"find_emoji find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ())\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes']","title":"Find emoji"},{"location":"api_modules/mlearner.nlp/find_emoji/#find_emoji","text":"find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ())\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes']","title":"find_emoji"},{"location":"api_modules/mlearner.nlp/find_hash/","text":"find_hash find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona'","title":"Find hash"},{"location":"api_modules/mlearner.nlp/find_hash/#find_hash","text":"find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona'","title":"find_hash"},{"location":"api_modules/mlearner.nlp/find_nonalp/","text":"find_nonalp find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')']","title":"Find nonalp"},{"location":"api_modules/mlearner.nlp/find_nonalp/#find_nonalp","text":"find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')']","title":"find_nonalp"},{"location":"api_modules/mlearner.nlp/find_number/","text":"find_number find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047'","title":"Find number"},{"location":"api_modules/mlearner.nlp/find_number/#find_number","text":"find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047'","title":"find_number"},{"location":"api_modules/mlearner.nlp/find_phone_number/","text":"find_phone_number find_phone_number(text) Spain Mobile numbers have ten digit. I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"698887776 is a phone number of Mark from 210,North Avenue\") >>> '698887776'","title":"Find phone number"},{"location":"api_modules/mlearner.nlp/find_phone_number/#find_phone_number","text":"find_phone_number(text) Spain Mobile numbers have ten digit. I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"698887776 is a phone number of Mark from 210,North Avenue\") >>> '698887776'","title":"find_phone_number"},{"location":"api_modules/mlearner.nlp/find_punct/","text":"find_punct find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')']","title":"Find punct"},{"location":"api_modules/mlearner.nlp/find_punct/#find_punct","text":"find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')']","title":"find_punct"},{"location":"api_modules/mlearner.nlp/find_url/","text":"find_url find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence)","title":"Find url"},{"location":"api_modules/mlearner.nlp/find_url/#find_url","text":"find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence)","title":"find_url"},{"location":"api_modules/mlearner.nlp/find_year/","text":"find_year find_year(text) Extract year from 1940 till 2040. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947']","title":"Find year"},{"location":"api_modules/mlearner.nlp/find_year/#find_year","text":"find_year(text) Extract year from 1940 till 2040. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947']","title":"find_year"},{"location":"api_modules/mlearner.nlp/ip_add/","text":"ip_add ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1']","title":"Ip add"},{"location":"api_modules/mlearner.nlp/ip_add/#ip_add","text":"ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1']","title":"ip_add"},{"location":"api_modules/mlearner.nlp/lat_lon/","text":"lat_lon lat_lon(string, display=False) valid latitude & longitude Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude","title":"Lat lon"},{"location":"api_modules/mlearner.nlp/lat_lon/#lat_lon","text":"lat_lon(string, display=False) valid latitude & longitude Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude","title":"lat_lon"},{"location":"api_modules/mlearner.nlp/mac_add/","text":"mac_add mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc']","title":"Mac add"},{"location":"api_modules/mlearner.nlp/mac_add/#mac_add","text":"mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc']","title":"mac_add"},{"location":"api_modules/mlearner.nlp/neg_look_ahead/","text":"neg_look_ahead neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love","title":"Neg look ahead"},{"location":"api_modules/mlearner.nlp/neg_look_ahead/#neg_look_ahead","text":"neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love","title":"neg_look_ahead"},{"location":"api_modules/mlearner.nlp/neg_look_behind/","text":"neg_look_behind neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp","title":"Neg look behind"},{"location":"api_modules/mlearner.nlp/neg_look_behind/#neg_look_behind","text":"neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp","title":"neg_look_behind"},{"location":"api_modules/mlearner.nlp/ngrams_top/","text":"ngrams_top ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176","title":"Ngrams top"},{"location":"api_modules/mlearner.nlp/ngrams_top/#ngrams_top","text":"ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176","title":"ngrams_top"},{"location":"api_modules/mlearner.nlp/num_great/","text":"num_great num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974'","title":"Num great"},{"location":"api_modules/mlearner.nlp/num_great/#num_great","text":"num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974'","title":"num_great"},{"location":"api_modules/mlearner.nlp/num_less/","text":"num_less num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920'","title":"Num less"},{"location":"api_modules/mlearner.nlp/num_less/#num_less","text":"num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920'","title":"num_less"},{"location":"api_modules/mlearner.nlp/only_words/","text":"only_words only_words(text) Only Words - Discard Numbers. Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000'","title":"Only words"},{"location":"api_modules/mlearner.nlp/only_words/#only_words","text":"only_words(text) Only Words - Discard Numbers. Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000'","title":"only_words"},{"location":"api_modules/mlearner.nlp/open_txt/","text":"open_txt open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings.","title":"Open txt"},{"location":"api_modules/mlearner.nlp/open_txt/#open_txt","text":"open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings.","title":"open_txt"},{"location":"api_modules/mlearner.nlp/pick_only_key_sentence/","text":"pick_only_key_sentence pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid']","title":"Pick only key sentence"},{"location":"api_modules/mlearner.nlp/pick_only_key_sentence/#pick_only_key_sentence","text":"pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid']","title":"pick_only_key_sentence"},{"location":"api_modules/mlearner.nlp/pick_unique_sentence/","text":"pick_unique_sentence pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors']","title":"Pick unique sentence"},{"location":"api_modules/mlearner.nlp/pick_unique_sentence/#pick_unique_sentence","text":"pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors']","title":"pick_unique_sentence"},{"location":"api_modules/mlearner.nlp/pos_look_ahead/","text":"pos_look_ahead pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love","title":"Pos look ahead"},{"location":"api_modules/mlearner.nlp/pos_look_ahead/#pos_look_ahead","text":"pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love","title":"pos_look_ahead"},{"location":"api_modules/mlearner.nlp/pos_look_behind/","text":"pos_look_behind pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp","title":"Pos look behind"},{"location":"api_modules/mlearner.nlp/pos_look_behind/#pos_look_behind","text":"pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp","title":"pos_look_behind"},{"location":"api_modules/mlearner.nlp/remove_emoji/","text":"","title":"Remove emoji"},{"location":"api_modules/mlearner.nlp/remove_tag/","text":"remove_tag remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics'","title":"Remove tag"},{"location":"api_modules/mlearner.nlp/remove_tag/#remove_tag","text":"remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics'","title":"remove_tag"},{"location":"api_modules/mlearner.nlp/search_string/","text":"search_string search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True","title":"Search string"},{"location":"api_modules/mlearner.nlp/search_string/#search_string","text":"search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True","title":"search_string"},{"location":"api_modules/mlearner.nlp/subword/","text":"subword subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2","title":"Subword"},{"location":"api_modules/mlearner.nlp/subword/#subword","text":"subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2","title":"subword"},{"location":"api_modules/mlearner.nlp/unique_char/","text":"unique_char unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(sentence) >>> 'hey this is long text son'","title":"Unique char"},{"location":"api_modules/mlearner.nlp/unique_char/#unique_char","text":"unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(sentence) >>> 'hey this is long text son'","title":"unique_char"},{"location":"api_modules/mlearner.plotly/FeatureAnalyst/","text":"FeatureAnalyst FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_modules/mlearner.plotly/FeatureAnalyst/#featureanalyst","text":"FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_modules/mlearner.plotly/plot_LDA/","text":"plot_LDA plot_LDA(data, features) None","title":"plot LDA"},{"location":"api_modules/mlearner.plotly/plot_LDA/#plot_lda","text":"plot_LDA(data, features) None","title":"plot_LDA"},{"location":"api_modules/mlearner.plotly/plot_PCA/","text":"plot_PCA plot_PCA(data, features) None","title":"plot PCA"},{"location":"api_modules/mlearner.plotly/plot_PCA/#plot_pca","text":"plot_PCA(data, features) None","title":"plot_PCA"},{"location":"api_modules/mlearner.plotly/plotly_histogram2/","text":"plotly_histogram2 plotly_histogram2(X, columns, target) None","title":"Plotly histogram2"},{"location":"api_modules/mlearner.plotly/plotly_histogram2/#plotly_histogram2","text":"plotly_histogram2(X, columns, target) None","title":"plotly_histogram2"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/","text":"CategoricalEncoder CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings. Methods fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"CategoricalEncoder"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/#categoricalencoder","text":"CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings.","title":"CategoricalEncoder"},{"location":"api_modules/mlearner.preprocessing/CategoricalEncoder/#methods","text":"fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ClassTransformer_value/","text":"ClassTransformer_value ClassTransformer_value(columns, name='A/AH_cat', value=100) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"ClassTransformer value"},{"location":"api_modules/mlearner.preprocessing/ClassTransformer_value/#classtransformer_value","text":"ClassTransformer_value(columns, name='A/AH_cat', value=100) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"ClassTransformer_value"},{"location":"api_modules/mlearner.preprocessing/ClassTransformer_value/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/","text":"CopyFeatures CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"CopyFeatures"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/#copyfeatures","text":"CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CopyFeatures"},{"location":"api_modules/mlearner.preprocessing/CopyFeatures/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/","text":"DataAnalyst DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/ Methods Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/', width=2) Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, width=2, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataAnalyst"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/#dataanalyst","text":"DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/","title":"DataAnalyst"},{"location":"api_modules/mlearner.preprocessing/DataAnalyst/#methods","text":"Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/', width=2) Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, width=2, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/","text":"DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataCleaner"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"api_modules/mlearner.preprocessing/DataCleaner/#methods","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/","text":"DataExploratory DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataExploratory"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/#dataexploratory","text":"DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataExploratory"},{"location":"api_modules/mlearner.preprocessing/DataExploratory/#methods","text":"categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/","text":"DataFrameSelector DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"DataFrameSelector"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/#dataframeselector","text":"DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"DataFrameSelector"},{"location":"api_modules/mlearner.preprocessing/DataFrameSelector/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/","text":"DropFeatures DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"DropFeatures"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/#dropfeatures","text":"DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/","title":"DropFeatures"},{"location":"api_modules/mlearner.preprocessing/DropFeatures/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/","text":"DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"DropOutliers"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"api_modules/mlearner.preprocessing/DropOutliers/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/","text":"ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ExtractCategories"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"api_modules/mlearner.preprocessing/ExtractCategories/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/","text":"FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"FeatureDropper"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"api_modules/mlearner.preprocessing/FeatureDropper/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/","text":"FeatureSelector FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FeatureSelector"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/#featureselector","text":"FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/","title":"FeatureSelector"},{"location":"api_modules/mlearner.preprocessing/FeatureSelector/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/","text":"FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer all"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_all/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/","text":"FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer any"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_any/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/","text":"FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer backward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_backward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/","text":"FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer forward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_forward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/","text":"FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer idmax"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_idmax/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/","text":"FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer mean"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_mean/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/","text":"FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer median"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_median/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/","text":"FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer value"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"api_modules/mlearner.preprocessing/FillNaTransformer_value/#methods","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/","text":"FixSkewness FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"FixSkewness"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/#fixskewness","text":"FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"api_modules/mlearner.preprocessing/FixSkewness/#methods","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/Keep/","text":"Keep Keep() Mantener columnas. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Keep"},{"location":"api_modules/mlearner.preprocessing/Keep/#keep","text":"Keep() Mantener columnas.","title":"Keep"},{"location":"api_modules/mlearner.preprocessing/Keep/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LDA_add/","text":"LDA_add LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"LDA add"},{"location":"api_modules/mlearner.preprocessing/LDA_add/#lda_add","text":"LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_add"},{"location":"api_modules/mlearner.preprocessing/LDA_add/#methods","text":"fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/","text":"LDA_selector LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"LDA selector"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/#lda_selector","text":"LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_selector"},{"location":"api_modules/mlearner.preprocessing/LDA_selector/#methods","text":"fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/","text":"LabelEncoder LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array. Methods fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"LabelEncoder"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/#labelencoder","text":"LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array.","title":"LabelEncoder"},{"location":"api_modules/mlearner.preprocessing/LabelEncoder/#methods","text":"fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/MFD_OrientationClassTransformer/","text":"MFD_OrientationClassTransformer MFD_OrientationClassTransformer(columns, name='MFDOCT', a=120, b=60, c=30, d=150) Transformer MFD Orientation. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"MFD OrientationClassTransformer"},{"location":"api_modules/mlearner.preprocessing/MFD_OrientationClassTransformer/#mfd_orientationclasstransformer","text":"MFD_OrientationClassTransformer(columns, name='MFDOCT', a=120, b=60, c=30, d=150) Transformer MFD Orientation.","title":"MFD_OrientationClassTransformer"},{"location":"api_modules/mlearner.preprocessing/MFD_OrientationClassTransformer/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/","text":"MeanCenterer MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"MeanCenterer"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/#meancenterer","text":"MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"api_modules/mlearner.preprocessing/MeanCenterer/#methods","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/","text":"OneHotEncoder OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"OneHotEncoder"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"api_modules/mlearner.preprocessing/OneHotEncoder/#methods","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/OrientationClassTransformer/","text":"OrientationClassTransformer OrientationClassTransformer(columns, name='OCT', a=135, b=45) Transformer Orientation. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"OrientationClassTransformer"},{"location":"api_modules/mlearner.preprocessing/OrientationClassTransformer/#orientationclasstransformer","text":"OrientationClassTransformer(columns, name='OCT', a=135, b=45) Transformer Orientation.","title":"OrientationClassTransformer"},{"location":"api_modules/mlearner.preprocessing/OrientationClassTransformer/#methods","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/PCA_add/","text":"PCA_add PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"PCA add"},{"location":"api_modules/mlearner.preprocessing/PCA_add/#pca_add","text":"PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_add"},{"location":"api_modules/mlearner.preprocessing/PCA_add/#methods","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/","text":"PCA_selector PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"PCA selector"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/#pca_selector","text":"PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_selector"},{"location":"api_modules/mlearner.preprocessing/PCA_selector/#methods","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/","text":"ReplaceMulticlass ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceMulticlass"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/#replacemulticlass","text":"ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"api_modules/mlearner.preprocessing/ReplaceMulticlass/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/","text":"ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceTransformer"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"api_modules/mlearner.preprocessing/ReplaceTransformer/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/","text":"StandardScaler StandardScaler( , copy=True, with_mean=True, with_std=True)* Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`. Methods fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"StandardScaler"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/#standardscaler","text":"StandardScaler( , copy=True, with_mean=True, with_std=True)* Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.","title":"StandardScaler"},{"location":"api_modules/mlearner.preprocessing/StandardScaler/#methods","text":"fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"Methods"},{"location":"api_modules/mlearner.preprocessing/minmax_scaling/","text":"minmax_scaling minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Minmax scaling"},{"location":"api_modules/mlearner.preprocessing/minmax_scaling/#minmax_scaling","text":"minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"api_modules/mlearner.training/Training/","text":"Training Training(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Training"},{"location":"api_modules/mlearner.training/Training/#training","text":"Training(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"Training"},{"location":"api_modules/mlearner.training/Training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_modules/mlearner.utils/ParamsManager/","text":"ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes metodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None","title":"ParamsManager"},{"location":"api_modules/mlearner.utils/ParamsManager/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes metodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"api_modules/mlearner.utils/ParamsManager/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"},{"location":"api_modules/mlearner.utils/keras_checkpoint/","text":"keras_checkpoint keras_checkpoint(model, checkpoint_path='', max_to_keep=5) Manantiene una save_counter para numerar los puestos de control. Parameters model: tf.keras.Model. Input model checkpoint_path: str Input model max_to_keep: int Maximo numero de checkpoints guardados Returns: ckpt_manager: tf.train.Checkpoint. Clase Checkpoint","title":"Keras checkpoint"},{"location":"api_modules/mlearner.utils/keras_checkpoint/#keras_checkpoint","text":"keras_checkpoint(model, checkpoint_path='', max_to_keep=5) Manantiene una save_counter para numerar los puestos de control. Parameters model: tf.keras.Model. Input model checkpoint_path: str Input model max_to_keep: int Maximo numero de checkpoints guardados Returns: ckpt_manager: tf.train.Checkpoint. Clase Checkpoint","title":"keras_checkpoint"},{"location":"api_subpackages/mlearner.classifier/","text":"mlearner version: 0.1.8 PipelineClasificators PipelineClasificators(random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones. TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones. wrapper_model wrapper_model(clf, pipeline_preprocess, random_state=99, name='model', select=False) Wrapper for Estimator. Methods Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True, report=False) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None buid_Pipeline(pipeline_preprocess, threshold='median', select=True) None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None restore_pipeline_v1(filename, random_state=99, name='Pipeline_model') None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None wrapper_pipeline wrapper_pipeline(filename, name='model', random_state=99) Wrapper for Estimator. Methods Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Mlearner.classifier"},{"location":"api_subpackages/mlearner.classifier/#pipelineclasificators","text":"PipelineClasificators(random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"PipelineClasificators"},{"location":"api_subpackages/mlearner.classifier/#methods","text":"Ablacion_relativa(pipeline, X, y, n_splits=10, mute=False, std=True, scoring='accuracy', display=True, save_image=False, path='/') None AdaBoostClassifier( params) None CatBoost(name='CBT') None ExtraTreesClassifier( params) None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GradientBoostingClassifier( params) None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_FeatureSelect(X, y, n_splits=10, mute=False, scoring='accuracy', n_features=20, display=True, save_image=False, path='/') None Pipeline_GridSearch() None Pipeline_SelectEmsembleModel(X, y, n_splits=10, mute=False, scoring='accuracy', display=True, save_image=False, path='/', AB=True) None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5) None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine( params) XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(clf, X, y, display=True, save_image=False, path='/') None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.classifier/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"api_subpackages/mlearner.classifier/#methods_1","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.classifier/#wrapper_model","text":"wrapper_model(clf, pipeline_preprocess, random_state=99, name='model', select=False) Wrapper for Estimator.","title":"wrapper_model"},{"location":"api_subpackages/mlearner.classifier/#methods_2","text":"Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True, report=False) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None buid_Pipeline(pipeline_preprocess, threshold='median', select=True) None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None restore_pipeline_v1(filename, random_state=99, name='Pipeline_model') None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_subpackages/mlearner.classifier/#wrapper_pipeline","text":"wrapper_pipeline(filename, name='model', random_state=99) Wrapper for Estimator.","title":"wrapper_pipeline"},{"location":"api_subpackages/mlearner.classifier/#methods_3","text":"Evaluation_model(X, y, clases=[0, 1], save=True, ROC=True, n_splits=10, path='checkpoints/') None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None Restore_Pipeline(filename='checkpoints/Pipeline_model.pkl') None Restore_model(filename='checkpoints/model.pkl') None build_param_grid(param_grid) None cuarentena(X, y) None fit(X, y) None fit_cv(X, y, n_splits=10, scoring='accuracy', shuffle=False) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X) None predict_proba(X) None save_general(path, X_train, y_train) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_subpackages/mlearner.data/","text":"mlearner version: 0.1.8 create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/ data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/ data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/ data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/ wine_data wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Mlearner.data"},{"location":"api_subpackages/mlearner.data/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"api_subpackages/mlearner.data/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"api_subpackages/mlearner.data/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"api_subpackages/mlearner.data/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"api_subpackages/mlearner.data/#wine_data","text":"wine_data() Wine dataset. Source: https://archive.ics.uci.edu/ml/datasets/Wine Number of samples: 178 Class labels: {0, 1, 2}, distribution: [59, 71, 48] Data Set Information: These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it) 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline In a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/wine_data adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/data/wine.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"wine_data"},{"location":"api_subpackages/mlearner.evaluation/","text":"mlearner version: 0.1.8 EvaluationModels EvaluationModels(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.evaluation"},{"location":"api_subpackages/mlearner.evaluation/#evaluationmodels","text":"EvaluationModels(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"EvaluationModels"},{"location":"api_subpackages/mlearner.evaluation/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.externals/","text":"mlearner version: 0.1.8 check_is_fitted check_is_fitted(estimator, attributes, msg=None, all_or_any= ) Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of \"all_or_any\" of the passed attributes and raises a NotFittedError with the given message. Parameters estimator : estimator instance. estimator instance for which the check is performed. attributes : attribute name(s) given as string or a list/tuple of strings Eg.: [\"coef_\", \"estimator_\", ...], \"coef_\" msg : string The default error message is, \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" For custom messages if \"%(name)s\" is present in the message string, it is substituted for the estimator name. Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\". all_or_any : callable, {all, any}, default all Specify whether all or any of the given attributes must exist. Returns None Raises NotFittedError If the attributes are not found.","title":"Mlearner.externals"},{"location":"api_subpackages/mlearner.externals/#check_is_fitted","text":"check_is_fitted(estimator, attributes, msg=None, all_or_any= ) Perform is_fitted validation for estimator. Checks if the estimator is fitted by verifying the presence of \"all_or_any\" of the passed attributes and raises a NotFittedError with the given message. Parameters estimator : estimator instance. estimator instance for which the check is performed. attributes : attribute name(s) given as string or a list/tuple of strings Eg.: [\"coef_\", \"estimator_\", ...], \"coef_\" msg : string The default error message is, \"This %(name)s instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\" For custom messages if \"%(name)s\" is present in the message string, it is substituted for the estimator name. Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\". all_or_any : callable, {all, any}, default all Specify whether all or any of the given attributes must exist. Returns None Raises NotFittedError If the attributes are not found.","title":"check_is_fitted"},{"location":"api_subpackages/mlearner.feature_selection/","text":"mlearner version: 0.1.8 FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y, n_estimators=100) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Mlearner.feature selection"},{"location":"api_subpackages/mlearner.feature_selection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"api_subpackages/mlearner.feature_selection/#methods","text":"LightGBM(X, y, n_estimators=100) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, k='all', cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"api_subpackages/mlearner.images/","text":"mlearner version: 0.1.8 load_image load_image(filename) None plot_image plot_image(img, name='Image', save=False, logdir_report='/images') Grafico Imagen. plot_image2 plot_image2(img1, img2, title='Images', save=False, logdir_report='/images') Grafico Imagen.","title":"Mlearner.images"},{"location":"api_subpackages/mlearner.images/#load_image","text":"load_image(filename) None","title":"load_image"},{"location":"api_subpackages/mlearner.images/#plot_image","text":"plot_image(img, name='Image', save=False, logdir_report='/images') Grafico Imagen.","title":"plot_image"},{"location":"api_subpackages/mlearner.images/#plot_image2","text":"plot_image2(img1, img2, title='Images', save=False, logdir_report='/images') Grafico Imagen.","title":"plot_image2"},{"location":"api_subpackages/mlearner.load/","text":"mlearner version: 0.1.8 DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Mlearner.load"},{"location":"api_subpackages/mlearner.load/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"api_subpackages/mlearner.load/#methods","text":"load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"api_subpackages/mlearner.models/","text":"mlearner version: 0.1.8 modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', params=None, args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones. modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', params=None, args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.models"},{"location":"api_subpackages/mlearner.models/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"api_subpackages/mlearner.models/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"api_subpackages/mlearner.models/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', params=None, args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"api_subpackages/mlearner.models/#methods_1","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/LGM_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='LGM_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.models/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', params=None, args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"api_subpackages/mlearner.models/#methods_2","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, params=None, params_finetune=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=-4, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True, max_num_features=20) None get_model() None get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_params_json() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints/XGB_model.txt', file_model='.txt') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None pred_multiclass(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='XGB_model', file_model='.txt') Save the model to disk score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X. sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) wrt. y. set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.neural/","text":"mlearner version: 0.1.8 Neural Neural(clf, random_state=99, name='Neural_model') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods Evaluation_model(X_train, X_test, y_train, y_test, clases=[0, 1], save=True, n_splits=10, ROC=True, path='checkpoints/', params) None Pipeline_train(X, y, X_train, X_test, y_train, y_test, n_splits=10, clases=[0, 1], ROC=True, path='checkpoints/', params) None cuarentena(X, y, IDs) None fit(X, y, params) None fit_cv(X, y, n_splits=10, shuffle=True, random_state=99, mute=False, display=True, params) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None plot_history(n_history) Grafica Resultados. predict(X, y=None) None predict_proba(X, y=None) None restore(filename, random_state=99, name='Neural_model') None save_general(path, X_train, y_train) None save_model(path, name=None) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None Neural_sklearn Neural_sklearn(random_state=99, name='Neural_model_sklearn') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods Evaluation_model(X, y, clases=[0, 1], save=True, n_splits=10, ROC=True, binary=True, path='checkpoints/', params) None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None buid_Pipeline(fn_clf, pipeline_preprocess, params) Example: def fn_clf(optimizer=tf.keras.optimizers.Adam(1e-3), kernel_initializer='glorot_uniform', dropout=0.2): model = tf.keras.models.Sequential() model.add(tf.keras.Input(shape=(8,))) model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_initializer=kernel_initializer)) model.add(tf.keras.layers.Dropout(dropout)) model.add(tf.keras.layers.Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer)) model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model params: { nb_epoch:100, batch_size:32, verbose:0 } build_param_grid(param_grid) None cuarentena(X, y, IDs) None fit(X, y) None fit_cv(X, y, n_splits=10, shuffle=True, scoring='accuracy', mute=False, display=True) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X, y=None, binary=True) None predict_proba(X, y=None) None restore_pipeline(filename, random_state=99, name='Neural_model_sklearn') None save(path, name=None) None save_general(path, X_train, y_train) None score(X, y, binary=True) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Mlearner.neural"},{"location":"api_subpackages/mlearner.neural/#neural","text":"Neural(clf, random_state=99, name='Neural_model') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"Neural"},{"location":"api_subpackages/mlearner.neural/#methods","text":"Evaluation_model(X_train, X_test, y_train, y_test, clases=[0, 1], save=True, n_splits=10, ROC=True, path='checkpoints/', params) None Pipeline_train(X, y, X_train, X_test, y_train, y_test, n_splits=10, clases=[0, 1], ROC=True, path='checkpoints/', params) None cuarentena(X, y, IDs) None fit(X, y, params) None fit_cv(X, y, n_splits=10, shuffle=True, random_state=99, mute=False, display=True, params) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None plot_history(n_history) Grafica Resultados. predict(X, y=None) None predict_proba(X, y=None) None restore(filename, random_state=99, name='Neural_model') None save_general(path, X_train, y_train) None save_model(path, name=None) None score(X, y) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_subpackages/mlearner.neural/#neural_sklearn","text":"Neural_sklearn(random_state=99, name='Neural_model_sklearn') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"Neural_sklearn"},{"location":"api_subpackages/mlearner.neural/#methods_1","text":"Evaluation_model(X, y, clases=[0, 1], save=True, n_splits=10, ROC=True, binary=True, path='checkpoints/', params) None Grid_model(X, y, n_splits=10, scoring='accuracy', Randomized=False, n_iter=20) None Pipeline_train(X, y, n_splits=10, Randomized=False, n_iter=20, threshold='median', clases=[0, 1], ROC=True, path='checkpoints/', eval=True) None buid_Pipeline(fn_clf, pipeline_preprocess, params) Example: def fn_clf(optimizer=tf.keras.optimizers.Adam(1e-3), kernel_initializer='glorot_uniform', dropout=0.2): model = tf.keras.models.Sequential() model.add(tf.keras.Input(shape=(8,))) model.add(tf.keras.layers.Dense(16, activation=\"relu\",kernel_initializer=kernel_initializer)) model.add(tf.keras.layers.Dropout(dropout)) model.add(tf.keras.layers.Dense(1,activation='sigmoid',kernel_initializer=kernel_initializer)) model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) return model params: { nb_epoch:100, batch_size:32, verbose:0 } build_param_grid(param_grid) None cuarentena(X, y, IDs) None fit(X, y) None fit_cv(X, y, n_splits=10, shuffle=True, scoring='accuracy', mute=False, display=True) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. line() None predict(X, y=None, binary=True) None predict_proba(X, y=None) None restore_pipeline(filename, random_state=99, name='Neural_model_sklearn') None save(path, name=None) None save_general(path, X_train, y_train) None score(X, y, binary=True) None set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. summary() None train_test(X, y, test_size=0.1) None","title":"Methods"},{"location":"api_subpackages/mlearner.nlp/","text":"mlearner version: 0.1.8 CountVectorizer CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling. Methods build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix. DCNN DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results) Methods add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. Properties activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables. Processor_data Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data) Methods apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Pdding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding boundary boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral' find_at find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David' find_capital find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God'] find_coin find_coin(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56' find_dates find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')] find_dollar find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56' find_domain find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org'] find_email find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com' find_emoji find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ())\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes'] find_hash find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona' find_nonalp find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')'] find_number find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047' find_phone_number find_phone_number(text) Spain Mobile numbers have ten digit. I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"698887776 is a phone number of Mark from 210,North Avenue\") >>> '698887776' find_punct find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')'] find_url find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence) find_year find_year(text) Extract year from 1940 till 2040. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947'] ip_add ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1'] lat_lon lat_lon(string, display=False) valid latitude & longitude Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude mac_add mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc'] neg_look_ahead neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love neg_look_behind neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp ngrams_top ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176 num_great num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974' num_less num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920' only_words only_words(text) Only Words - Discard Numbers. Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000' open_txt open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings. pick_only_key_sentence pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid'] pick_unique_sentence pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors'] pos_look_ahead pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love pos_look_behind pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp remove_tag remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics' search_string search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True subword subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2 unique_char unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(sentence) >>> 'hey this is long text son'","title":"Mlearner.nlp"},{"location":"api_subpackages/mlearner.nlp/#countvectorizer","text":"CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.","title":"CountVectorizer"},{"location":"api_subpackages/mlearner.nlp/#methods","text":"build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix.","title":"Methods"},{"location":"api_subpackages/mlearner.nlp/#dcnn","text":"DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results)","title":"DCNN"},{"location":"api_subpackages/mlearner.nlp/#methods_1","text":"add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope.","title":"Methods"},{"location":"api_subpackages/mlearner.nlp/#properties","text":"activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables.","title":"Properties"},{"location":"api_subpackages/mlearner.nlp/#processor_data","text":"Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data)","title":"Processor_data"},{"location":"api_subpackages/mlearner.nlp/#methods_2","text":"apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Pdding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding","title":"Methods"},{"location":"api_subpackages/mlearner.nlp/#boundary","text":"boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral'","title":"boundary"},{"location":"api_subpackages/mlearner.nlp/#find_at","text":"find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David'","title":"find_at"},{"location":"api_subpackages/mlearner.nlp/#find_capital","text":"find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God']","title":"find_capital"},{"location":"api_subpackages/mlearner.nlp/#find_coin","text":"find_coin(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"find_coin"},{"location":"api_subpackages/mlearner.nlp/#find_dates","text":"find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')]","title":"find_dates"},{"location":"api_subpackages/mlearner.nlp/#find_dollar","text":"find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"find_dollar"},{"location":"api_subpackages/mlearner.nlp/#find_domain","text":"find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org']","title":"find_domain"},{"location":"api_subpackages/mlearner.nlp/#find_email","text":"find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com'","title":"find_email"},{"location":"api_subpackages/mlearner.nlp/#find_emoji","text":"find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ())\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes']","title":"find_emoji"},{"location":"api_subpackages/mlearner.nlp/#find_hash","text":"find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona'","title":"find_hash"},{"location":"api_subpackages/mlearner.nlp/#find_nonalp","text":"find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')']","title":"find_nonalp"},{"location":"api_subpackages/mlearner.nlp/#find_number","text":"find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047'","title":"find_number"},{"location":"api_subpackages/mlearner.nlp/#find_phone_number","text":"find_phone_number(text) Spain Mobile numbers have ten digit. I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"698887776 is a phone number of Mark from 210,North Avenue\") >>> '698887776'","title":"find_phone_number"},{"location":"api_subpackages/mlearner.nlp/#find_punct","text":"find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')']","title":"find_punct"},{"location":"api_subpackages/mlearner.nlp/#find_url","text":"find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence)","title":"find_url"},{"location":"api_subpackages/mlearner.nlp/#find_year","text":"find_year(text) Extract year from 1940 till 2040. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947']","title":"find_year"},{"location":"api_subpackages/mlearner.nlp/#ip_add","text":"ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1']","title":"ip_add"},{"location":"api_subpackages/mlearner.nlp/#lat_lon","text":"lat_lon(string, display=False) valid latitude & longitude Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude","title":"lat_lon"},{"location":"api_subpackages/mlearner.nlp/#mac_add","text":"mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc']","title":"mac_add"},{"location":"api_subpackages/mlearner.nlp/#neg_look_ahead","text":"neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love","title":"neg_look_ahead"},{"location":"api_subpackages/mlearner.nlp/#neg_look_behind","text":"neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp","title":"neg_look_behind"},{"location":"api_subpackages/mlearner.nlp/#ngrams_top","text":"ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176","title":"ngrams_top"},{"location":"api_subpackages/mlearner.nlp/#num_great","text":"num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974'","title":"num_great"},{"location":"api_subpackages/mlearner.nlp/#num_less","text":"num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920'","title":"num_less"},{"location":"api_subpackages/mlearner.nlp/#only_words","text":"only_words(text) Only Words - Discard Numbers. Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000'","title":"only_words"},{"location":"api_subpackages/mlearner.nlp/#open_txt","text":"open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings.","title":"open_txt"},{"location":"api_subpackages/mlearner.nlp/#pick_only_key_sentence","text":"pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid']","title":"pick_only_key_sentence"},{"location":"api_subpackages/mlearner.nlp/#pick_unique_sentence","text":"pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors']","title":"pick_unique_sentence"},{"location":"api_subpackages/mlearner.nlp/#pos_look_ahead","text":"pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love","title":"pos_look_ahead"},{"location":"api_subpackages/mlearner.nlp/#pos_look_behind","text":"pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp","title":"pos_look_behind"},{"location":"api_subpackages/mlearner.nlp/#remove_tag","text":"remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics'","title":"remove_tag"},{"location":"api_subpackages/mlearner.nlp/#search_string","text":"search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True","title":"search_string"},{"location":"api_subpackages/mlearner.nlp/#subword","text":"subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2","title":"subword"},{"location":"api_subpackages/mlearner.nlp/#unique_char","text":"unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(sentence) >>> 'hey this is long text son'","title":"unique_char"},{"location":"api_subpackages/mlearner.plotly/","text":"mlearner version: 0.1.8 FeatureAnalyst FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias. plot_LDA plot_LDA(data, features) None plot_PCA plot_PCA(data, features) None plotly_histogram2 plotly_histogram2(X, columns, target) None","title":"Mlearner.plotly"},{"location":"api_subpackages/mlearner.plotly/#featureanalyst","text":"FeatureAnalyst(X, feature, target, targets=[2, 3, 13]) Analisis de la caracteristica respecto a las categorias.","title":"FeatureAnalyst"},{"location":"api_subpackages/mlearner.plotly/#plot_lda","text":"plot_LDA(data, features) None","title":"plot_LDA"},{"location":"api_subpackages/mlearner.plotly/#plot_pca","text":"plot_PCA(data, features) None","title":"plot_PCA"},{"location":"api_subpackages/mlearner.plotly/#plotly_histogram2","text":"plotly_histogram2(X, columns, target) None","title":"plotly_histogram2"},{"location":"api_subpackages/mlearner.preprocessing/","text":"mlearner version: 0.1.8 CategoricalEncoder CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings. Methods fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input. ClassTransformer_value ClassTransformer_value(columns, name='A/AH_cat', value=100) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None CopyFeatures CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None DataAnalyst DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/ Methods Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/', width=2) Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, width=2, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataExploratory DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe DataFrameSelector DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None DropFeatures DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped. ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped. FeatureSelector FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. FixSkewness FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. Keep Keep() Mantener columnas. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None LDA_add LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. LDA_selector LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. LabelEncoder LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array. Methods fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] MFD_OrientationClassTransformer MFD_OrientationClassTransformer(columns, name='MFDOCT', a=120, b=60, c=30, d=150) Transformer MFD Orientation. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None MeanCenterer MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. OneHotEncoder OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder. OrientationClassTransformer OrientationClassTransformer(columns, name='OCT', a=135, b=45) Transformer Orientation. Methods fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None PCA_add PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. PCA_selector PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered. ReplaceMulticlass ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced. StandardScaler StandardScaler( , copy=True, with_mean=True, with_std=True)* Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`. Methods fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. minmax_scaling minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Mlearner.preprocessing"},{"location":"api_subpackages/mlearner.preprocessing/#categoricalencoder","text":"CategoricalEncoder(encoding='onehot', categories='auto', dtype= , handle_unknown='error') Encode categorical features as a numeric array. The input to this transformer should be a matrix of integers or strings, denoting the values taken on by categorical (discrete) features. The features can be encoded using a one-hot aka one-of-K scheme ( encoding='onehot' , the default) or converted to ordinal integers ( encoding='ordinal' ). This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels. Read more in the :ref: User Guide <preprocessing_categorical_features> . Parameters encoding : str, 'onehot', 'onehot-dense' or 'ordinal' The type of encoding to use (default is 'onehot'): - 'onehot': encode the features using a one-hot aka one-of-K scheme (or also called 'dummy' encoding). This creates a binary column for each category and returns a sparse matrix. - 'onehot-dense': the same as 'onehot' but returns a dense array instead of a sparse matrix. - 'ordinal': encode the features as ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. categories : 'auto' or a list of lists/arrays of values. Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories are sorted before encoding the data (used categories can be found in the categories_ attribute). dtype : number type, default np.float64 Desired dtype of output. handle_unknown : 'error' (default) or 'ignore' Whether to raise an error or ignore if a unknown categorical feature is present during transform (default is to raise). When this is parameter is set to 'ignore' and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Ignoring unknown categories is not supported for encoding='ordinal' . Attributes categories_ : list of arrays The categories of each feature determined during fitting. When categories were specified manually, this holds the sorted categories (in order corresponding with output of transform ). Examples Given a dataset with three features and two samples, we let the encoder find the maximum value per feature and transform the data to a binary one-hot encoding. >>> from sklearn.preprocessing import CategoricalEncoder >>> enc = CategoricalEncoder(handle_unknown='ignore') >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ... # doctest: +ELLIPSIS CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>, encoding='onehot', handle_unknown='ignore') >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray() array([[ 1., 0., 0., 1., 0., 0., 1., 0., 0.], [ 0., 1., 1., 0., 0., 0., 0., 0., 0.]]) See also sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of integer ordinal features. The OneHotEncoder assumes that input features take on values in the range [0, max(feature)] instead of using the unique values. sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of dictionary items (also handles string-valued features). sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot encoding of dictionary items or strings.","title":"CategoricalEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods","text":"fit(X, y=None) Fit the CategoricalEncoder to X. Parameters X : array-like, shape [n_samples, n_feature] The data to determine the categories of each feature. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Transform X using one-hot encoding. Parameters X : array-like, shape [n_samples, n_features] The data to encode. Returns X_out : sparse matrix or a 2-d array Transformed input.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#classtransformer_value","text":"ClassTransformer_value(columns, name='A/AH_cat', value=100) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"ClassTransformer_value"},{"location":"api_subpackages/mlearner.preprocessing/#methods_1","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#copyfeatures","text":"CopyFeatures(columns=None, prefix='') Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CopyFeatures"},{"location":"api_subpackages/mlearner.preprocessing/#methods_2","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataanalyst","text":"DataAnalyst(data) Class for Preprocessed object for data analysis. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataAnalyst/","title":"DataAnalyst"},{"location":"api_subpackages/mlearner.preprocessing/#methods_3","text":"Xy_dataset(target=None) Separar datos del target en conjunto (X, y) boxplot(features=None, target=None, display=False, save_image=False, path='/', width=2) Funcion que realiza un BoxPlot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. categorical_vs_numerical() None corr_matrix(features=None, display=True, save_image=False, path='/') matriz de covarianza: Un valor positivo para r indica una asociacion positiva Un valor negativo para r indica una asociacion negativa. Cuanto mas cerca estar de 1cuanto mas se acercan los puntos de datos a una linea recta, la asociacion lineal es mas fuerte. Cuanto mas cerca este r de 0, lo que debilita la asociacion lineal. dispersion_categoria(features=None, target=None, density=True, display=False, width=2, save_image=False, path='/') Funcion que realiza un plot sobre la dispesion de cada categoria respecto a los grupos de target. Inputs: - data: Datos generales del dataset. - features: categorias a analizar. distribution_targets(target=None, display=True, save_image=False, path='/', palette='Set2') None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None sns_jointplot(feature1, feature2, target=None, categoria1=None, categoria2=None, display=True, save_image=False, path='/') None sns_pairplot(features=None, target=None, display=True, save_image=False, path='/', palette='husl') None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"api_subpackages/mlearner.preprocessing/#methods_4","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataexploratory","text":"DataExploratory(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataExploratory"},{"location":"api_subpackages/mlearner.preprocessing/#methods_5","text":"categorical_vs_numerical() None dtypes(X=None) retorno del tipo de datos por columna isNull() None load_data(filename, name='dataset', sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values(X=None) Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dataframeselector","text":"DataFrameSelector(attribute_names) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"DataFrameSelector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_6","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dropfeatures","text":"DropFeatures(columns_drop=None, random_state=99) This transformer drop features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropFeatures/","title":"DropFeatures"},{"location":"api_subpackages/mlearner.preprocessing/#methods_7","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"api_subpackages/mlearner.preprocessing/#methods_8","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"api_subpackages/mlearner.preprocessing/#methods_9","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"api_subpackages/mlearner.preprocessing/#methods_10","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#featureselector","text":"FeatureSelector(columns=None, random_state=99) This transformer select features. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureSelector/","title":"FeatureSelector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_11","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"api_subpackages/mlearner.preprocessing/#methods_12","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"api_subpackages/mlearner.preprocessing/#methods_13","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"api_subpackages/mlearner.preprocessing/#methods_14","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"api_subpackages/mlearner.preprocessing/#methods_15","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"api_subpackages/mlearner.preprocessing/#methods_16","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"api_subpackages/mlearner.preprocessing/#methods_17","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"api_subpackages/mlearner.preprocessing/#methods_18","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"api_subpackages/mlearner.preprocessing/#methods_19","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#fixskewness","text":"FixSkewness(columns=None, drop=True) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"api_subpackages/mlearner.preprocessing/#methods_20","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#keep","text":"Keep() Mantener columnas.","title":"Keep"},{"location":"api_subpackages/mlearner.preprocessing/#methods_21","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#lda_add","text":"LDA_add(columns=None, LDA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_add"},{"location":"api_subpackages/mlearner.preprocessing/#methods_22","text":"fit(X, y=None) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#lda_selector","text":"LDA_selector(columns=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"LDA_selector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_23","text":"fit(X, y) Selecting LDA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies LDA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#labelencoder","text":"LabelEncoder() Encode target labels with value between 0 and n_classes-1. This transformer should be used to encode target values, i.e. y , and not the input X . Read more in the :ref: User Guide <preprocessing_targets> . .. versionadded:: 0.12 Attributes classes_ : array of shape (n_class,) Holds the label for each class. Examples LabelEncoder can be used to normalize labels. >>> from sklearn import preprocessing >>> le = preprocessing.LabelEncoder() >>> le.fit([1, 2, 2, 6]) LabelEncoder() >>> le.classes_ array([1, 2, 6]) >>> le.transform([1, 1, 2, 6]) array([0, 0, 1, 2]...) >>> le.inverse_transform([0, 0, 1, 2]) array([1, 1, 2, 6]) It can also be used to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. >>> le = preprocessing.LabelEncoder() >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]) LabelEncoder() >>> list(le.classes_) ['amsterdam', 'paris', 'tokyo'] >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) array([2, 2, 1]...) >>> list(le.inverse_transform([2, 2, 1])) ['tokyo', 'tokyo', 'paris'] See also sklearn.preprocessing.OrdinalEncoder : Encode categorical features using an ordinal encoding scheme. sklearn.preprocessing.OneHotEncoder : Encode categorical features as a one-hot numeric array.","title":"LabelEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods_24","text":"fit(y) Fit label encoder Parameters y : array-like of shape (n_samples,) Target values. Returns self : returns an instance of self. fit_transform(y) Fit label encoder and return encoded labels Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples] get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(y) Transform labels back to original encoding. Parameters y : numpy array of shape [n_samples] Target values. Returns y : numpy array of shape [n_samples] set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(y) Transform labels to normalized encoding. Parameters y : array-like of shape [n_samples] Target values. Returns y : array-like of shape [n_samples]","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#mfd_orientationclasstransformer","text":"MFD_OrientationClassTransformer(columns, name='MFDOCT', a=120, b=60, c=30, d=150) Transformer MFD Orientation.","title":"MFD_OrientationClassTransformer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_25","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#meancenterer","text":"MeanCenterer(columns=None) Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_26","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[], Drop=True) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"api_subpackages/mlearner.preprocessing/#methods_27","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#orientationclasstransformer","text":"OrientationClassTransformer(columns, name='OCT', a=135, b=45) Transformer Orientation.","title":"OrientationClassTransformer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_28","text":"fit(X, y=None) None fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) None","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#pca_add","text":"PCA_add(columns=None, n_components=2, PCA_name=None, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_add"},{"location":"api_subpackages/mlearner.preprocessing/#methods_29","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#pca_selector","text":"PCA_selector(columns=None, n_components=2, random_state=99) Base class for all estimators in scikit-learn Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"PCA_selector"},{"location":"api_subpackages/mlearner.preprocessing/#methods_30","text":"fit(X, y=None) Selecting PCA columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Trransformer applies PCA. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#replacemulticlass","text":"ReplaceMulticlass(columns=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"api_subpackages/mlearner.preprocessing/#methods_31","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"api_subpackages/mlearner.preprocessing/#methods_32","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#standardscaler","text":"StandardScaler( , copy=True, with_mean=True, with_std=True)* Standardize features by removing the mean and scaling to unit variance The standard score of a sample x is calculated as: z = (x - u) / s where u is the mean of the training samples or zero if with_mean=False , and s is the standard deviation of the training samples or one if with_std=False . Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth: transform . Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. This scaler can also be applied to sparse CSR or CSC matrices by passing with_mean=False to avoid breaking the sparsity structure of the data. Read more in the :ref: User Guide <preprocessing_scaler> . Parameters copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). Attributes scale_ : ndarray or None, shape (n_features,) Per feature relative scaling of the data. This is calculated using np.sqrt(var_) . Equal to None when with_std=False . .. versionadded:: 0.17 scale_ mean_ : ndarray or None, shape (n_features,) The mean value for each feature in the training set. Equal to None when with_mean=False . var_ : ndarray or None, shape (n_features,) The variance for each feature in the training set. Used to compute scale_ . Equal to None when with_std=False . n_samples_seen_ : int or array, shape (n_features,) The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls. Examples >>> from sklearn.preprocessing import StandardScaler >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]] >>> scaler = StandardScaler() >>> print(scaler.fit(data)) StandardScaler() >>> print(scaler.mean_) [0.5 0.5] >>> print(scaler.transform(data)) [[-1. -1.] [-1. -1.] [ 1. 1.] [ 1. 1.]] >>> print(scaler.transform([[2, 2]])) [[3. 3.]] See also scale: Equivalent function without the estimator API. :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. Notes NaNs are treated as missing values: disregarded in fit, and maintained in transform. We use a biased estimator for the standard deviation, equivalent to `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to affect model performance. For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.","title":"StandardScaler"},{"location":"api_subpackages/mlearner.preprocessing/#methods_33","text":"fit(X, y=None) Compute the mean and std to be used for later scaling. Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y Ignored fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features) y : ndarray of shape (n_samples,), default=None Target values. **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. inverse_transform(X, copy=None) Scale back the data to the original representation Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not. Returns X_tr : array-like, shape [n_samples, n_features] Transformed array. partial_fit(X, y=None) Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when :meth: fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms for computing the sample variance: Analysis and recommendations.\" The American Statistician 37.3 (1983): 242-247: Parameters X : {array-like, sparse matrix}, shape [n_samples, n_features] The data used to compute the mean and standard deviation used for later scaling along the features axis. y : None Ignored. Returns self : object Transformer instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(X, copy=None) Perform standardization by centering and scaling Parameters X : array-like, shape [n_samples, n_features] The data used to scale along the features axis. copy : bool, optional (default: None) Copy the input X or not.","title":"Methods"},{"location":"api_subpackages/mlearner.preprocessing/#minmax_scaling","text":"minmax_scaling(X, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"api_subpackages/mlearner.stacking/","text":"mlearner version: 0.1.8","title":"Mlearner.stacking"},{"location":"api_subpackages/mlearner.training/","text":"mlearner version: 0.1.8 Training Training(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe. Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Mlearner.training"},{"location":"api_subpackages/mlearner.training/#training","text":"Training(model, random_state=99) Evaluacion de modelosself. Parameters model : Modelo a evaluearself. random_state : Asignacion de la semilla de aleatoriedad. Attributes See Also mlearner.load.DataLoad : Loading a dataset from a file or dataframe.","title":"Training"},{"location":"api_subpackages/mlearner.training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tecnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} - Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, save=False, logdir_report='') Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuantas predicciones son verdaderas y cuantas son falsas. Mas especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son mas bajos que las medidas de precision, ya que incorporan precision y recuerdo en su calculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(clf, data_eval, data_eval_target, targets=[0, 1], save=False, logdir_report='', display=True) Funcion que nos selecciona el thresholder mas optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, results_df, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que esta cometiendo su clasificador, sino mas importante aun, de los tipos de errores que se estan cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"api_subpackages/mlearner.utils/","text":"mlearner version: 0.1.8 ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes metodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None keras_checkpoint keras_checkpoint(model, checkpoint_path='', max_to_keep=5) Manantiene una save_counter para numerar los puestos de control. Parameters model: tf.keras.Model. Input model checkpoint_path: str Input model max_to_keep: int Maximo numero de checkpoints guardados Returns: ckpt_manager: tf.train.Checkpoint. Clase Checkpoint","title":"Mlearner.utils"},{"location":"api_subpackages/mlearner.utils/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes metodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"api_subpackages/mlearner.utils/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"},{"location":"api_subpackages/mlearner.utils/#keras_checkpoint","text":"keras_checkpoint(model, checkpoint_path='', max_to_keep=5) Manantiene una save_counter para numerar los puestos de control. Parameters model: tf.keras.Model. Input model checkpoint_path: str Input model max_to_keep: int Maximo numero de checkpoints guardados Returns: ckpt_manager: tf.train.Checkpoint. Clase Checkpoint","title":"keras_checkpoint"},{"location":"user_guide/clasifier/PipelineClasificators/","text":"PipelineClasificators PipelineClasificators(random_state=99) None Methods CatBoost(name='CBT') None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_GridSearch() None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5, select='XGBoost') None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine() XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(display=True) None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"PipelineClasificators"},{"location":"user_guide/clasifier/PipelineClasificators/#pipelineclasificators","text":"PipelineClasificators(random_state=99) None","title":"PipelineClasificators"},{"location":"user_guide/clasifier/PipelineClasificators/#methods","text":"CatBoost(name='CBT') None FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. KNearestNeighbors() LightBoost(name='LBT') None NaiveBayes() Naive Bayes assumes the data to be normally distributed which can be achieved by scaling using the MaxAbsScaler. Pipeline_GridSearch() None Pipeline_SelectModel(X, y, n_splits=5, select='XGBoost') None Pipeline_StackingClassifier(X, y, n_splits=5, select='XGBoost') None RandomForestClassifier() n_jobs: Parelizacion en la computacion. oob_score: True, muestreo aleatorio. n_estimadores = numero de arboles en el bosque max_features = numero maximo de caracteristicas consideradas para dividir un nodo max_depth = numero maximo de niveles en cada arbol de decision min_samples_split = numero minimo de puntos de datos colocados en un nodo antes de que el nodo se divida min_samples_leaf = numero minimo de puntos de datos permitidos en un nodo hoja bootstrap = metodo para muestrear puntos de datos (con o sin reemplazo) SupportVectorMachine() XGBoost(name='CBT') \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation dat. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ add_model(model) Incorporar modelo en la clase append_summary(model, X_train, X_test, y_train, y_test, name) None class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_Kfold_CV(model, X, X_test, y, y_test, n_splits=3, shuffle=True, mute=True) None eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_importances(display=True) None features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas func_acc(prob_pred, y_target) None get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(prob_pred, y_target, th=0.5) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/clasifier/TrainingUtilities/","text":"TrainingUtilities TrainingUtilities(random_state=99) None Methods Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"TrainingUtilities"},{"location":"user_guide/clasifier/TrainingUtilities/#trainingutilities","text":"TrainingUtilities(random_state=99) None","title":"TrainingUtilities"},{"location":"user_guide/clasifier/TrainingUtilities/#methods","text":"Seleccion_rasgos(model, X, Y, n, display=True) Funcion que nos retorna las categorias que mas influyen en el modelo. Inputs: - X, Y: dataset. - n: numero de categorias que queremos que formen parte de variables del modelo de RL. - display: Mostrar los resultados por pantalla Outputs: - z: Resumen de la inferencia de cada categoria en el modelo de RL. - valid: Categorias seleccionadas para formar parte del modelo de RL. opt_RandomForest_Classifier(X, Y, nmin=1, nmax=10000, num=4, display=True) Seleccion del numero de categorias a tener en cuenta para RF optimizacion_seleccion_rasgos(model, X, Y, n_splits, display=True) Seleccion del numero de categorias a tener en cuenta validacion_cruzada(model, X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/data/create_dataset/","text":"create_dataset create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"Create dataset"},{"location":"user_guide/data/create_dataset/#create_dataset","text":"create_dataset(config, n) Generate a Dataset. Attributes config : dict Dictionary for dataset configuration: p.e.: dict = { - `'A'` : data_uniform(0, 1, n), - `'B'` : data_normal(n), - `'C'` : data_normal(mu=5, sd=2, n=n), - `'D'` : data_gamma(a=5, n=n) } n : int number of data in the dataset. Returns data : Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/create_dataset/","title":"create_dataset"},{"location":"user_guide/data/data_gamma/","text":"data_gamma data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"Data gamma"},{"location":"user_guide/data/data_gamma/#data_gamma","text":"data_gamma(a=5, n=100) Generate a Gamma data distribution. Attributes a : int or float Parameter form. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_gamma/","title":"data_gamma"},{"location":"user_guide/data/data_normal/","text":"data_normal data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"Data normal"},{"location":"user_guide/data/data_normal/#data_normal","text":"data_normal(mu=0, sd=1, n=100) Generate a Normal data distribution. Attributes mu : int or float mean value. sd : int or float standard deviation. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_normal/","title":"data_normal"},{"location":"user_guide/data/data_uniform/","text":"data_uniform data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"Data uniform"},{"location":"user_guide/data/data_uniform/#data_uniform","text":"data_uniform(a, b, n) Generate a Uniform data distribution. Attributes a : int or float manimum value of the entire dataset. b : int or float maximum value of the entire dataset. n : int number of data in the dataset. Returns data : Uniform data distribution. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/data/data_uniform/","title":"data_uniform"},{"location":"user_guide/data/wine_data/","text":"Wine Dataset A function that loads the Wine dataset into NumPy arrays. from mlxtend.data import wine_data Overview The Wine dataset for classification. Samples 178 Features 13 Classes 3 Data Set Characteristics: Multivariate Attribute Characteristics: Integer, Real Associated Tasks: Classification Missing Values None column attribute 1) Class Label 2) Alcohol 3) Malic acid 4) Ash 5) Alcalinity of ash 6) Magnesium 7) Total phenols 8) Flavanoids 9) Nonflavanoid phenols 10) Proanthocyanins 11) Color intensity 12) Hue 13) OD280/OD315 of diluted wines 14) Proline class samples 0 59 1 71 2 48 References Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Source: https://archive.ics.uci.edu/ml/datasets/Wine Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Example 1 - Dataset overview from mlearner.data import wine_data X, y = wine_data() print('Dimensions: %s x %s' % (X.shape[0], X.shape[1])) print('\\n1st row') print(X.iloc[0]) Dimensions: 178 x 12 1st row alcohol 1.71 malic acid 2.43 ash 15.60 ash alcalinity 127.00 magnesium 2.80 total phenols 3.06 flavanoids 0.28 nonflavanoid phenols 2.29 proanthocyanins 5.64 color intensity 1.04 hue 3.92 OD280/OD315 of diluted wines 1065.00 Name: 14.23, dtype: float64 X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol malic acid ash ash alcalinity magnesium total phenols flavanoids nonflavanoid phenols proanthocyanins color intensity hue OD280/OD315 of diluted wines 14.23 1.71 2.43 15.6 127.0 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 13.20 1.78 2.14 11.2 100.0 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 13.16 2.36 2.67 18.6 101.0 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 14.37 1.95 2.50 16.8 113.0 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 13.24 2.59 2.87 21.0 118.0 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 14.20 1.76 2.45 15.2 112.0 3.27 3.39 0.34 1.97 6.75 1.05 2.85 1450 14.39 1.87 2.45 14.6 96.0 2.50 2.52 0.30 1.98 5.25 1.02 3.58 1290 14.06 2.15 2.61 17.6 121.0 2.60 2.51 0.31 1.25 5.05 1.06 3.58 1295 14.83 1.64 2.17 14.0 97.0 2.80 2.98 0.29 1.98 5.20 1.08 2.85 1045 13.86 1.35 2.27 16.0 98.0 2.98 3.15 0.22 1.85 7.22 1.01 3.55 1045 14.10 2.16 2.30 18.0 105.0 2.95 3.32 0.22 2.38 5.75 1.25 3.17 1510 14.12 1.48 2.32 16.8 95.0 2.20 2.43 0.26 1.57 5.00 1.17 2.82 1280 13.75 1.73 2.41 16.0 89.0 2.60 2.76 0.29 1.81 5.60 1.15 2.90 1320 14.75 1.73 2.39 11.4 91.0 3.10 3.69 0.43 2.81 5.40 1.25 2.73 1150 14.38 1.87 2.38 12.0 102.0 3.30 3.64 0.29 2.96 7.50 1.20 3.00 1547 13.63 1.81 2.70 17.2 112.0 2.85 2.91 0.30 1.46 7.30 1.28 2.88 1310 14.30 1.92 2.72 20.0 120.0 2.80 3.14 0.33 1.97 6.20 1.07 2.65 1280 13.83 1.57 2.62 20.0 115.0 2.95 3.40 0.40 1.72 6.60 1.13 2.57 1130 14.19 1.59 2.48 16.5 108.0 3.30 3.93 0.32 1.86 8.70 1.23 2.82 1680 13.64 3.10 2.56 15.2 116.0 2.70 3.03 0.17 1.66 5.10 0.96 3.36 845 14.06 1.63 2.28 16.0 126.0 3.00 3.17 0.24 2.10 5.65 1.09 3.71 780 12.93 3.80 2.65 18.6 102.0 2.41 2.41 0.25 1.98 4.50 1.03 3.52 770 13.71 1.86 2.36 16.6 101.0 2.61 2.88 0.27 1.69 3.80 1.11 4.00 1035 12.85 1.60 2.52 17.8 95.0 2.48 2.37 0.26 1.46 3.93 1.09 3.63 1015 13.50 1.81 2.61 20.0 96.0 2.53 2.61 0.28 1.66 3.52 1.12 3.82 845 13.05 2.05 3.22 25.0 124.0 2.63 2.68 0.47 1.92 3.58 1.13 3.20 830 13.39 1.77 2.62 16.1 93.0 2.85 2.94 0.34 1.45 4.80 0.92 3.22 1195 13.30 1.72 2.14 17.0 94.0 2.40 2.19 0.27 1.35 3.95 1.02 2.77 1285 13.87 1.90 2.80 19.4 107.0 2.95 2.97 0.37 1.76 4.50 1.25 3.40 915 14.02 1.68 2.21 16.0 96.0 2.65 2.33 0.26 1.98 4.70 1.04 3.59 1035 ... ... ... ... ... ... ... ... ... ... ... ... ... 13.32 3.24 2.38 21.5 92.0 1.93 0.76 0.45 1.25 8.42 0.55 1.62 650 13.08 3.90 2.36 21.5 113.0 1.41 1.39 0.34 1.14 9.40 0.57 1.33 550 13.50 3.12 2.62 24.0 123.0 1.40 1.57 0.22 1.25 8.60 0.59 1.30 500 12.79 2.67 2.48 22.0 112.0 1.48 1.36 0.24 1.26 10.80 0.48 1.47 480 13.11 1.90 2.75 25.5 116.0 2.20 1.28 0.26 1.56 7.10 0.61 1.33 425 13.23 3.30 2.28 18.5 98.0 1.80 0.83 0.61 1.87 10.52 0.56 1.51 675 12.58 1.29 2.10 20.0 103.0 1.48 0.58 0.53 1.40 7.60 0.58 1.55 640 13.17 5.19 2.32 22.0 93.0 1.74 0.63 0.61 1.55 7.90 0.60 1.48 725 13.84 4.12 2.38 19.5 89.0 1.80 0.83 0.48 1.56 9.01 0.57 1.64 480 12.45 3.03 2.64 27.0 97.0 1.90 0.58 0.63 1.14 7.50 0.67 1.73 880 14.34 1.68 2.70 25.0 98.0 2.80 1.31 0.53 2.70 13.00 0.57 1.96 660 13.48 1.67 2.64 22.5 89.0 2.60 1.10 0.52 2.29 11.75 0.57 1.78 620 12.36 3.83 2.38 21.0 88.0 2.30 0.92 0.50 1.04 7.65 0.56 1.58 520 13.69 3.26 2.54 20.0 107.0 1.83 0.56 0.50 0.80 5.88 0.96 1.82 680 12.85 3.27 2.58 22.0 106.0 1.65 0.60 0.60 0.96 5.58 0.87 2.11 570 12.96 3.45 2.35 18.5 106.0 1.39 0.70 0.40 0.94 5.28 0.68 1.75 675 13.78 2.76 2.30 22.0 90.0 1.35 0.68 0.41 1.03 9.58 0.70 1.68 615 13.73 4.36 2.26 22.5 88.0 1.28 0.47 0.52 1.15 6.62 0.78 1.75 520 13.45 3.70 2.60 23.0 111.0 1.70 0.92 0.43 1.46 10.68 0.85 1.56 695 12.82 3.37 2.30 19.5 88.0 1.48 0.66 0.40 0.97 10.26 0.72 1.75 685 13.58 2.58 2.69 24.5 105.0 1.55 0.84 0.39 1.54 8.66 0.74 1.80 750 13.40 4.60 2.86 25.0 112.0 1.98 0.96 0.27 1.11 8.50 0.67 1.92 630 12.20 3.03 2.32 19.0 96.0 1.25 0.49 0.40 0.73 5.50 0.66 1.83 510 12.77 2.39 2.28 19.5 86.0 1.39 0.51 0.48 0.64 9.90 0.57 1.63 470 14.16 2.51 2.48 20.0 91.0 1.68 0.70 0.44 1.24 9.70 0.62 1.71 660 13.71 5.65 2.45 20.5 95.0 1.68 0.61 0.52 1.06 7.70 0.64 1.74 740 13.40 3.91 2.48 23.0 102.0 1.80 0.75 0.43 1.41 7.30 0.70 1.56 750 13.27 4.28 2.26 20.0 120.0 1.59 0.69 0.43 1.35 10.20 0.59 1.56 835 13.17 2.59 2.37 20.0 120.0 1.65 0.68 0.53 1.46 9.30 0.60 1.62 840 14.13 4.10 2.74 24.5 96.0 2.05 0.76 0.56 1.35 9.20 0.61 1.60 560 178 rows \u00d7 12 columns import numpy as np print('Classes: %s' % np.unique(y)) print('Class distribution: %s' % np.bincount(y)) API","title":"Wine Dataset"},{"location":"user_guide/data/wine_data/#wine-dataset","text":"A function that loads the Wine dataset into NumPy arrays. from mlxtend.data import wine_data","title":"Wine Dataset"},{"location":"user_guide/data/wine_data/#overview","text":"The Wine dataset for classification. Samples 178 Features 13 Classes 3 Data Set Characteristics: Multivariate Attribute Characteristics: Integer, Real Associated Tasks: Classification Missing Values None column attribute 1) Class Label 2) Alcohol 3) Malic acid 4) Ash 5) Alcalinity of ash 6) Magnesium 7) Total phenols 8) Flavanoids 9) Nonflavanoid phenols 10) Proanthocyanins 11) Color intensity 12) Hue 13) OD280/OD315 of diluted wines 14) Proline class samples 0 59 1 71 2 48","title":"Overview"},{"location":"user_guide/data/wine_data/#references","text":"Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Source: https://archive.ics.uci.edu/ml/datasets/Wine Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. %load_ext autoreload %autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"References"},{"location":"user_guide/data/wine_data/#example-1-dataset-overview","text":"from mlearner.data import wine_data X, y = wine_data() print('Dimensions: %s x %s' % (X.shape[0], X.shape[1])) print('\\n1st row') print(X.iloc[0]) Dimensions: 178 x 12 1st row alcohol 1.71 malic acid 2.43 ash 15.60 ash alcalinity 127.00 magnesium 2.80 total phenols 3.06 flavanoids 0.28 nonflavanoid phenols 2.29 proanthocyanins 5.64 color intensity 1.04 hue 3.92 OD280/OD315 of diluted wines 1065.00 Name: 14.23, dtype: float64 X .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alcohol malic acid ash ash alcalinity magnesium total phenols flavanoids nonflavanoid phenols proanthocyanins color intensity hue OD280/OD315 of diluted wines 14.23 1.71 2.43 15.6 127.0 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 13.20 1.78 2.14 11.2 100.0 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 13.16 2.36 2.67 18.6 101.0 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 14.37 1.95 2.50 16.8 113.0 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 13.24 2.59 2.87 21.0 118.0 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 14.20 1.76 2.45 15.2 112.0 3.27 3.39 0.34 1.97 6.75 1.05 2.85 1450 14.39 1.87 2.45 14.6 96.0 2.50 2.52 0.30 1.98 5.25 1.02 3.58 1290 14.06 2.15 2.61 17.6 121.0 2.60 2.51 0.31 1.25 5.05 1.06 3.58 1295 14.83 1.64 2.17 14.0 97.0 2.80 2.98 0.29 1.98 5.20 1.08 2.85 1045 13.86 1.35 2.27 16.0 98.0 2.98 3.15 0.22 1.85 7.22 1.01 3.55 1045 14.10 2.16 2.30 18.0 105.0 2.95 3.32 0.22 2.38 5.75 1.25 3.17 1510 14.12 1.48 2.32 16.8 95.0 2.20 2.43 0.26 1.57 5.00 1.17 2.82 1280 13.75 1.73 2.41 16.0 89.0 2.60 2.76 0.29 1.81 5.60 1.15 2.90 1320 14.75 1.73 2.39 11.4 91.0 3.10 3.69 0.43 2.81 5.40 1.25 2.73 1150 14.38 1.87 2.38 12.0 102.0 3.30 3.64 0.29 2.96 7.50 1.20 3.00 1547 13.63 1.81 2.70 17.2 112.0 2.85 2.91 0.30 1.46 7.30 1.28 2.88 1310 14.30 1.92 2.72 20.0 120.0 2.80 3.14 0.33 1.97 6.20 1.07 2.65 1280 13.83 1.57 2.62 20.0 115.0 2.95 3.40 0.40 1.72 6.60 1.13 2.57 1130 14.19 1.59 2.48 16.5 108.0 3.30 3.93 0.32 1.86 8.70 1.23 2.82 1680 13.64 3.10 2.56 15.2 116.0 2.70 3.03 0.17 1.66 5.10 0.96 3.36 845 14.06 1.63 2.28 16.0 126.0 3.00 3.17 0.24 2.10 5.65 1.09 3.71 780 12.93 3.80 2.65 18.6 102.0 2.41 2.41 0.25 1.98 4.50 1.03 3.52 770 13.71 1.86 2.36 16.6 101.0 2.61 2.88 0.27 1.69 3.80 1.11 4.00 1035 12.85 1.60 2.52 17.8 95.0 2.48 2.37 0.26 1.46 3.93 1.09 3.63 1015 13.50 1.81 2.61 20.0 96.0 2.53 2.61 0.28 1.66 3.52 1.12 3.82 845 13.05 2.05 3.22 25.0 124.0 2.63 2.68 0.47 1.92 3.58 1.13 3.20 830 13.39 1.77 2.62 16.1 93.0 2.85 2.94 0.34 1.45 4.80 0.92 3.22 1195 13.30 1.72 2.14 17.0 94.0 2.40 2.19 0.27 1.35 3.95 1.02 2.77 1285 13.87 1.90 2.80 19.4 107.0 2.95 2.97 0.37 1.76 4.50 1.25 3.40 915 14.02 1.68 2.21 16.0 96.0 2.65 2.33 0.26 1.98 4.70 1.04 3.59 1035 ... ... ... ... ... ... ... ... ... ... ... ... ... 13.32 3.24 2.38 21.5 92.0 1.93 0.76 0.45 1.25 8.42 0.55 1.62 650 13.08 3.90 2.36 21.5 113.0 1.41 1.39 0.34 1.14 9.40 0.57 1.33 550 13.50 3.12 2.62 24.0 123.0 1.40 1.57 0.22 1.25 8.60 0.59 1.30 500 12.79 2.67 2.48 22.0 112.0 1.48 1.36 0.24 1.26 10.80 0.48 1.47 480 13.11 1.90 2.75 25.5 116.0 2.20 1.28 0.26 1.56 7.10 0.61 1.33 425 13.23 3.30 2.28 18.5 98.0 1.80 0.83 0.61 1.87 10.52 0.56 1.51 675 12.58 1.29 2.10 20.0 103.0 1.48 0.58 0.53 1.40 7.60 0.58 1.55 640 13.17 5.19 2.32 22.0 93.0 1.74 0.63 0.61 1.55 7.90 0.60 1.48 725 13.84 4.12 2.38 19.5 89.0 1.80 0.83 0.48 1.56 9.01 0.57 1.64 480 12.45 3.03 2.64 27.0 97.0 1.90 0.58 0.63 1.14 7.50 0.67 1.73 880 14.34 1.68 2.70 25.0 98.0 2.80 1.31 0.53 2.70 13.00 0.57 1.96 660 13.48 1.67 2.64 22.5 89.0 2.60 1.10 0.52 2.29 11.75 0.57 1.78 620 12.36 3.83 2.38 21.0 88.0 2.30 0.92 0.50 1.04 7.65 0.56 1.58 520 13.69 3.26 2.54 20.0 107.0 1.83 0.56 0.50 0.80 5.88 0.96 1.82 680 12.85 3.27 2.58 22.0 106.0 1.65 0.60 0.60 0.96 5.58 0.87 2.11 570 12.96 3.45 2.35 18.5 106.0 1.39 0.70 0.40 0.94 5.28 0.68 1.75 675 13.78 2.76 2.30 22.0 90.0 1.35 0.68 0.41 1.03 9.58 0.70 1.68 615 13.73 4.36 2.26 22.5 88.0 1.28 0.47 0.52 1.15 6.62 0.78 1.75 520 13.45 3.70 2.60 23.0 111.0 1.70 0.92 0.43 1.46 10.68 0.85 1.56 695 12.82 3.37 2.30 19.5 88.0 1.48 0.66 0.40 0.97 10.26 0.72 1.75 685 13.58 2.58 2.69 24.5 105.0 1.55 0.84 0.39 1.54 8.66 0.74 1.80 750 13.40 4.60 2.86 25.0 112.0 1.98 0.96 0.27 1.11 8.50 0.67 1.92 630 12.20 3.03 2.32 19.0 96.0 1.25 0.49 0.40 0.73 5.50 0.66 1.83 510 12.77 2.39 2.28 19.5 86.0 1.39 0.51 0.48 0.64 9.90 0.57 1.63 470 14.16 2.51 2.48 20.0 91.0 1.68 0.70 0.44 1.24 9.70 0.62 1.71 660 13.71 5.65 2.45 20.5 95.0 1.68 0.61 0.52 1.06 7.70 0.64 1.74 740 13.40 3.91 2.48 23.0 102.0 1.80 0.75 0.43 1.41 7.30 0.70 1.56 750 13.27 4.28 2.26 20.0 120.0 1.59 0.69 0.43 1.35 10.20 0.59 1.56 835 13.17 2.59 2.37 20.0 120.0 1.65 0.68 0.53 1.46 9.30 0.60 1.62 840 14.13 4.10 2.74 24.5 96.0 2.05 0.76 0.56 1.35 9.20 0.61 1.60 560 178 rows \u00d7 12 columns import numpy as np print('Classes: %s' % np.unique(y)) print('Class distribution: %s' % np.bincount(y))","title":"Example 1 - Dataset overview"},{"location":"user_guide/data/wine_data/#api","text":"","title":"API"},{"location":"user_guide/evaluation/EvaluationModels/","text":"EvaluationModels EvaluationModels(model, random_state=99) None Methods add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"EvaluationModels"},{"location":"user_guide/evaluation/EvaluationModels/#evaluationmodels","text":"EvaluationModels(model, random_state=99) None","title":"EvaluationModels"},{"location":"user_guide/evaluation/EvaluationModels/#methods","text":"add_model(filename) Load the model from disk class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/feature_selections/FeatureSelection/","text":"FeatureSelection FeatureSelection(random_state=99) None Methods LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"FeatureSelection"},{"location":"user_guide/feature_selections/FeatureSelection/#featureselection","text":"FeatureSelection(random_state=99) None","title":"FeatureSelection"},{"location":"user_guide/feature_selections/FeatureSelection/#methods","text":"LightGBM(X, y) LightGBM Normalization: No Impute missing values: No RandomForest(X, y, n_estimators=100) Random Forest Normalization: No Impute missing values: Yes Summary(X, y, cor_pearson=True, chi2=True, wrapper=True, embeded=True, RandomForest=True, LightGBM=True) Resumen de la seleccion de caracteristicas. chi2(X, y, k='all') Chi-2 Normalization: MinMaxScaler (values should be bigger than 0) Impute missing values: yes cor_pearson(X, y, k='all') Pearson Correlation Normalization: no Impute missing values: yes embeded(X, y) Embeded documentation for SelectFromModel: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html ### 3.1 Logistics Regression L1 Note Normalization: Yes Impute missing values: Yes transform_data(X, selector, features) Transformar el conjunto de entrenamiento al nuevo esquema proporcionado por el selector wrapper(X, y, k='all') Wrapper documentation for RFE: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Normalization: depend on the used model; yes for LR Impute missing values: depend on the used model; yes for LR","title":"Methods"},{"location":"user_guide/load/DataLoad/","text":"DataLoad DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ Methods load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: 'str, path object or file-like object' Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: 'str' Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: 'str, default None' Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"DataLoad"},{"location":"user_guide/load/DataLoad/#dataload","text":"DataLoad(data) Loading a dataset from a file or dataframe. Parameters data: pandas [n_columns, n_features]. pandas Dataframe. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/","title":"DataLoad"},{"location":"user_guide/load/DataLoad/#methods","text":"load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: 'str, path object or file-like object' Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv. If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: 'str' Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: 'str, default None' Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None reset() None","title":"Methods"},{"location":"user_guide/models/modelCatBoost/","text":"modelCatBoost modelCatBoost(name='CBT', random_state=99, args, * kwargs) None Methods FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"modelCatBoost"},{"location":"user_guide/models/modelCatBoost/#modelcatboost","text":"modelCatBoost(name='CBT', random_state=99, args, * kwargs) None","title":"modelCatBoost"},{"location":"user_guide/models/modelCatBoost/#methods","text":"FineTune_hyperopt(X, y, mute=False) None FineTune_sklearn(X, y, mute=False, n_splits=10, n_iter=2) https://www.kaggle.com/ksaaskil/pets-definitive-catboost-tuning Visualizer_Models(directs=None, visu_model=True) None add_cat_features(index_features) None copy( args, * kwargs) None dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwargs) None fit(X, y, use_best_model=True, plot=True, save_snapshot=False, verbose=0, args, * kwargs) None fit_cv(X, y, fold_count=4, shuffle=True, stratified=True, plot=True, verbose=100) None get_important_features(display=True) None hyperopt_objective(params) None index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None save_model(direct='./checkpoints', name='catboost_model') None update_model( kwargs) None","title":"Methods"},{"location":"user_guide/models/modelLightBoost/","text":"modelLightBoost modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelLightBoost"},{"location":"user_guide/models/modelLightBoost/#modellightboost","text":"modelLightBoost(name='LGB', random_state=99, train_dir='', args, * kwargs) Ejemplo multiclass: https://www.kaggle.com/nicapotato/multi-class-lgbm-cv-and-seed-diversification","title":"modelLightBoost"},{"location":"user_guide/models/modelLightBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, display_ROC=True, verbose=0, n_iter=10, replace_model=True, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None SeedDiversification_fs(X, y, params, n_iter=10, mute=False, logdir_report='', display=True, save_image=True) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, nfold=5, use_best_model=True, verbose=200, nosplit=False, early_stopping_rounds=150, num_boost_round=2000, kwargs) None func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None predict_proba(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/models/modelXGBoost/","text":"modelXGBoost modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"modelXGBoost"},{"location":"user_guide/models/modelXGBoost/#modelxgboost","text":"modelXGBoost(name='XGB', random_state=99, train_dir='', args, * kwargs) XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples. Parameters \"min_child_weight\": [ Minimum sum of instance weight (hessian) needed in a child. \"objective\": learning task. \"eval_metric\": Evaluation metrics for validation data. \"max_depth\": Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit \"max_delta_step\": /Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint. \"sampling_method\": The method to use to sample the training instances. \"subsample\": Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. \"eta\": tep size shrinkage used in update to prevents overfitting. \"gamma\": Minimum loss reduction required to make a further partition on a leaf node of the tree. \"lambda\": L2 regularization term on weights. Increasing this value will make model more conservative. \"alpha\": L1 regularization term on weights. Increasing this value will make model more conservative. \"tree_method\": he tree construction algorithm used in XGBoost. \"predictor\": The type of predictor algorithm to use. \"num_parallel_tree\": umber of parallel trees constructed during each iteration. ... Documentation https://xgboost.readthedocs.io/en/latest/ https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/","title":"modelXGBoost"},{"location":"user_guide/models/modelXGBoost/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' FineTune_SearchCV(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, ROC=False, randomized=True, cv=10, n_iter=10, replace_model=True, verbose=0, nosplit=False, finetune_dir='') None GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. SeedDiversification_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, n_iter=10, n_max=2020, cv=10, nosplit=False, finetuneseed_dir='', display=True, save_image=True, verbose=0) None add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. dataset(X, y, categorical_columns_indices=None, test_size=0.2, args, * kwarg) None eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas fit(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, mute=False, use_best_model=True, verbose=0, num_boost_round=100, nosplit=False, kwargs) None fit_cv(X=None, y=None, X_train=None, X_test=None, y_train=None, y_test=None, num_boost_round=75, nfold=5, use_best_model=True, verbose=2, nosplit=False, early_stopping_rounds=75, kwargs) https://xgboost.readthedocs.io/en/latest/parameter.html func_acc(prob_pred, y_target) None get_important_features(display=True) None get_model() None get_params_json() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] index_features(features) None load_model(direct='./checkpoints', name='catboost_model') None plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. pred_binary(X, args, * kwargs) None predict(X, args, * kwargs) None replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(direct='./checkpoints', name='catboost_model') Save the model to disk set_dataset_nosplit(X_train, X_test, y_train, y_test, categorical_columns_indices=None, args, * kwarg) None update_model( kwargs) None validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/nlp/CountVectorizer/","text":"CountVectorizer CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling. Methods build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix.","title":"CountVectorizer"},{"location":"user_guide/nlp/CountVectorizer/#countvectorizer","text":"CountVectorizer( , input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype= )* Convert a collection of text documents to a matrix of token counts This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix. If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Read more in the :ref: User Guide <text_feature_extraction> . Parameters input : string {'filename', 'file', 'content'}, default='content' If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be a sequence of items that can be of type string or byte. encoding : string, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode. decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'. strip_accents : {'ascii', 'unicode'}, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing. Both 'ascii' and 'unicode' use NFKD normalization from :func: unicodedata.normalize . lowercase : bool, default=True Convert all characters to lowercase before tokenizing. preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if analyzer is not callable . tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer == 'word' . stop_words : string {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref: stop_words ). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. token_pattern : string Regular expression denoting what constitutes a \"token\", only used if analyzer == 'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable . analyzer : string, {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. .. versionchanged:: 0.21 Since v0.21, if input is filename or file , the data is first read from the file and then passed to the given callable analyzer. max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, default=None Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : bool, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, default=np.int64 Type of the matrix returned by fit_transform() or transform(). Attributes vocabulary_ : dict A mapping of terms to feature indices. fixed_vocabulary_: boolean True if a fixed vocabulary of term to indices mapping is provided by the user stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given. Examples >>> from sklearn.feature_extraction.text import CountVectorizer >>> corpus = [ ... 'This is the first document.', ... 'This document is the second document.', ... 'And this is the third one.', ... 'Is this the first document?', ... ] >>> vectorizer = CountVectorizer() >>> X = vectorizer.fit_transform(corpus) >>> print(vectorizer.get_feature_names()) ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this'] >>> print(X.toarray()) [[0 1 1 1 0 0 1 0 1] [0 2 0 1 0 1 1 0 1] [1 0 0 1 1 0 1 1 1] [0 1 1 1 0 0 1 0 1]] >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) >>> X2 = vectorizer2.fit_transform(corpus) >>> print(vectorizer2.get_feature_names()) ['and this', 'document is', 'first document', 'is the', 'is this', 'second document', 'the first', 'the second', 'the third', 'third one', 'this document', 'this is', 'this the'] >>> print(X2.toarray()) [[0 0 1 1 0 0 1 0 0 0 0 1 0] [0 1 0 1 0 1 0 1 0 0 1 0 0] [1 0 0 1 0 0 0 0 1 1 0 1 0] [0 0 1 0 1 0 1 0 0 0 0 0 1]] See Also HashingVectorizer, TfidfVectorizer Notes The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.","title":"CountVectorizer"},{"location":"user_guide/nlp/CountVectorizer/#methods","text":"build_analyzer() Return a callable that handles preprocessing, tokenization and n-grams generation. Returns analyzer: callable A function to handle preprocessing, tokenization and n-grams generation. build_preprocessor() Return a function to preprocess the text before tokenization. Returns preprocessor: callable A function to preprocess the text before tokenization. build_tokenizer() Return a function that splits a string into a sequence of tokens. Returns tokenizer: callable A function to split a string into a sequence of tokens. decode(doc) Decode the input into a string of unicode symbols. The decoding strategy depends on the vectorizer parameters. Parameters doc : str The string to decode. Returns doc: str A string of unicode symbols. fit(raw_documents, y=None) Learn a vocabulary dictionary of all tokens in the raw documents. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns self fit_transform(raw_documents, y=None) Learn the vocabulary dictionary and return document-term matrix. This is equivalent to fit followed by transform, but more efficiently implemented. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : array of shape (n_samples, n_features) Document-term matrix. get_feature_names() Array mapping from feature integer indices to feature name. Returns feature_names : list A list of feature names. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. get_stop_words() Build or fetch the effective stop words list. Returns stop_words: list or None A list of stop words. inverse_transform(X) Return terms per document with nonzero entries in X. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) Document-term matrix. Returns X_inv : list of arrays of shape (n_samples,) List of arrays of terms. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : object Estimator instance. transform(raw_documents) Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor. Parameters raw_documents : iterable An iterable which yields either str, unicode or file objects. Returns X : sparse matrix of shape (n_samples, n_features) Document-term matrix.","title":"Methods"},{"location":"user_guide/nlp/DCNN/","text":"DCNN DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results) Methods add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. Properties activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables.","title":"DCNN"},{"location":"user_guide/nlp/DCNN/#dcnn","text":"DCNN( args, * kwargs) The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters vocab_size: Vocabulary size of the algorithm input text. emb_dim : int Embedding size. nb_filters : int Filter size for each layer Conv1D. FFN_units : int Units for dense layer. nb_classes : int Numbers of final categories. dropout_rate : float Dropout parameter. training : bool Trainning process activated. name : str Custom Model Name. weights_path: str Path load weight model. Attributes embedding : tf.keras.layers.Embedding Embedding layer for input vocabulary. bigram : tf.keras.layers.Conv1D 1D convolution layer, for two letters in a row. trigram : tf.keras.layers.Conv1D 1D convolution layer, for three letters in a row. fourgram : tf.keras.layers.Conv1D 1D convolution layer, for four letters in a row. pool : tf.keras.layers.GlobalMaxPool1D Max pooling operation for 1D temporal data. dense_1 : tf.keras.layers.Dense Regular densely-connected NN layer, concatenate 1D Convolutions. last_dense : tf.keras.layers.Dense Regular densely-connected NN layer, final decision. dropout : tf.keras.layers.Dropout Applies Dropout to dense_1. Examples: VOCAB_SIZE = tokenizer.vocab_size # 65540 EMB_DIM = 200 NB_FILTERS = 100 FFN_UNITS = 256 NB_CLASSES = 2#len(set(train_labels)) DROPOUT_RATE = 0.2 BATCH_SIZE = 32 NB_EPOCHS = 5 Dcnn = DCNN(vocab_size=VOCAB_SIZE, emb_dim=EMB_DIM, nb_filters=NB_FILTERS, FFN_units=FFN_UNITS, nb_classes=NB_CLASSES, dropout_rate=DROPOUT_RATE) if NB_CLASSES == 2: Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) else: Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"]) # Entrenamiento Dcnn.fit(train_inputs, train_labels, batch_size=BATCH_SIZE, epochs=NB_EPOCHS) # Evaluation results = Dcnn.evaluate(test_inputs, test_labels, batch_size=BATCH_SIZE) print(results)","title":"DCNN"},{"location":"user_guide/nlp/DCNN/#methods","text":"add_loss(losses, inputs=None) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The get_losses_for method allows to retrieve the losses relevant to a specific set of inputs. Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other than None is passed, it signals the losses are conditional on some of the layer's inputs, and thus they should only be run where these inputs are available. This is the case for activity regularization losses, for instance. If None is passed, the losses are assumed to be unconditional, and will apply across all dataflows of the layer (e.g. weight regularization losses). add_metric(value, aggregation=None, name=None) Adds metric tensor to the layer. Args: value: Metric tensor. aggregation: Sample-wise metric reduction function. If aggregation=None , it indicates that the metric tensor provided has been aggregated already. eg, bin_acc = BinaryAccuracy(name='acc') followed by model.add_metric(bin_acc(y_true, y_pred)) . If aggregation='mean', the given metric tensor will be sample-wise reduced using mean function. eg, model.add_metric(tf.reduce_sum(outputs), name='output_mean', aggregation='mean') . name: String metric name. Raises: ValueError: If aggregation is anything other than None or mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. (deprecated arguments) Warning: SOME ARGUMENTS ARE DEPRECATED: (inputs) . They will be removed in a future version. Instructions for updating: inputs is now automatically inferred Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. The get_updates_for method allows to retrieve the updates relevant to a specific set of inputs. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.add_weight method instead. add_weight(name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization= , aggregation= , kwargs) Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to self.dtype or float32 . initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint: Constraint instance (callable). partitioner: Partitioner to be passed to the Trackable API. use_resource: Whether to use ResourceVariable . synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs: Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: The created variable. Usually either a Variable or ResourceVariable instance. If partitioner is not None , a PartitionedVariable instance is returned. Raises: RuntimeError: If called with partitioned variable regularization and eager execution is enabled. ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . apply(inputs, args, * kwargs) Deprecated, do NOT use! (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use layer.__call__ method instead. This is an alias of self.__call__ . Arguments: inputs: Input tensor(s). args: additional positional arguments to be passed to self.call . *kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call model.build() in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, or TensorShape). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data. call(inputs, training) Calling the build function of the model. Parameters inputs: Tensor. Input Tensor. Training : bool Trainning process activated. Returns: output: Tensor. Output Tensor. compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, kwargs) Configures the model for training. Arguments: optimizer: String (name of optimizer) or optimizer instance. See tf.keras.optimizers . loss: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses . An objective function is any callable with the signature loss = fn(y_true, y_pred) , where y_true = ground truth values with shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] . y_pred = predicted values with shape = [batch_size, d0, .. dN] . It returns a weighted loss float tensor. If a custom Loss instance is used and reduction is set to NONE, return value has the shape [batch_size, d0, .. dN-1] ie. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a tf.keras.metrics.Metric instance. See tf.keras.metrics . Typically you will use metrics=['accuracy'] . A function is any callable with the signature result = fn(y_true, y_pred) . To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']} . You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']] . When you pass the strings 'accuracy' or 'acc', we convert this to one of tf.keras.metrics.BinaryAccuracy , tf.keras.metrics.CategoricalAccuracy , tf.keras.metrics.SparseCategoricalAccuracy based on the loss function used and the model output shape. We do a similar conversion for the strings 'crossentropy' and 'ce' as well. loss_weights: Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the weighted sum of all individual losses, weighted by the loss_weights coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients. sample_weight_mode: If you need to do timestep-wise sample weighting (2D weights), set this to \"temporal\" . None defaults to sample-wise weights (1D). If the model has multiple outputs, you can use a different sample_weight_mode on each output by passing a dictionary or a list of modes. weighted_metrics: List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing. **kwargs: Any additional arguments. For eager execution, pass run_eagerly=True . Raises: ValueError: In case of invalid arguments for optimizer , loss , metrics or sample_weight_mode . compute_mask(inputs, mask) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). evaluate(x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, return_dict=False) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from the iterator/dataset). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of a dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar. sample_weight: Optional Numpy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, instead pass sample weights as the third element of x . steps: Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, 'evaluate' will run until the dataset is exhausted. This argument is not supported with array inputs. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during evaluation. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: in case of invalid arguments. evaluate_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Evaluates the model on a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.evaluate, which supports generators. DEPRECATED: Model.evaluate now supports generators, so there is no longer any need to use this endpoint. fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights) . - A generator or keras.utils.Sequence returning (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). If x is a dataset, generator, or keras.utils.Sequence instance, y should not be specified (since targets will be obtained from x ). batch_size: Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See tf.keras.callbacks . validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: - tuple (x_val, y_val) of Numpy arrays or tensors - tuple (x_val, y_val, val_sample_weights) of Numpy arrays - dataset For the first two cases, batch_size must be provided. For the last case, validation_steps could be provided. Note that validation_data does not support all the data types that are supported in x , eg, dict, generator or keras.utils.Sequence . shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None . class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape (samples, sequence_length) , to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile() . This argument is not supported when x is a dataset, generator, or keras.utils.Sequence instance, instead provide the sample_weights as the third element of x . initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. validation_steps: Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of datasets, generators, or keras.utils.Sequence instances (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. Unpacking behavior for iterator-like inputs: A common pattern is to pass a tf.data.Dataset, generator, or tf.keras.utils.Sequence to the x argument of fit, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: RuntimeError: If the model was never compiled. ValueError: In case of mismatch between the provided input data and what the model expects. fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0) Fits the model on data yielded batch-by-batch by a Python generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.fit, which supports generators. DEPRECATED: Model.fit now supports generators, so there is no longer any need to use this endpoint. from_config(config, custom_objects=None) Instantiates a Model from its config (output of get_config() ). Arguments: config: Model config dictionary. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A model instance. Raises: ValueError: In case of improperly formatted config dict. get_config() Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Python dictionary. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_layer(name=None, index=None) Retrieves a layer based on either its name (unique) or index. If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up). Arguments: name: String, name of layer. index: Integer, index of layer. Returns: A layer instance. Raises: ValueError: In case of invalid layer name or index. get_losses_for(inputs) Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on inputs . get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on inputs . get_weights() Retrieves the weights of the model. Returns: A flat list of Numpy arrays. load_weights(filepath, by_name=False, skip_mismatch=False) Loads all layer weights, either from a TensorFlow or an HDF5 weight file. If by_name is False weights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights. If by_name is True, weights are loaded into layers only if they share the same name. This is useful for fine-tuning or transfer-learning models where some of the layers have changed. Only topological loading ( by_name=False ) is supported when loading weights from the TensorFlow format. Note that topological loading differs slightly between TensorFlow and HDF5 formats for user-defined classes inheriting from tf.keras.Model : HDF5 loads based on a flattened list of weights, while the TensorFlow format loads based on the object-local names of attributes to which layers are assigned in the Model 's constructor. Arguments: filepath: String, path to the weights file to load. For weight files in TensorFlow format, this is the file prefix (the same as was passed to save_weights ). by_name: Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format. skip_mismatch: Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True ). Returns: When loading a weight file in TensorFlow format, returns the same status object as tf.train.Checkpoint.restore . When graph building, restore ops are run automatically as soon as the network is built (on first call for user-defined classes inheriting from Model , immediately if it is already built). When loading weights in HDF5 format, returns None . Raises: ImportError: If h5py is not available and the weight file is in HDF5 format. ValueError: If skip_mismatch is set to True when by_name is False . make_predict_function() Creates a function that executes one step of inference. This method can be overridden to support custom inference logic. This method is called by Model.predict and Model.predict_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.predict_step . This function is cached the first time Model.predict or Model.predict_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return the outputs of the Model . make_test_function() Creates a function that executes one step of evaluation. This method can be overridden to support custom evaluation logic. This method is called by Model.evaluate and Model.test_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual evaluation logic to Model.test_step . This function is cached the first time Model.evaluate or Model.test_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_test_batch_end . make_train_function() Creates a function that executes one step of training. This method can be overridden to support custom training logic. This method is called by Model.fit and Model.train_on_batch . Typically, this method directly controls tf.function and tf.distribute.Strategy settings, and delegates the actual training logic to Model.train_step . This function is cached the first time Model.fit or Model.train_on_batch is called. The cache is cleared whenever Model.compile is called. Returns: Function. The function created by this method should accept a tf.data.Iterator , and return a dict containing values that will be passed to tf.keras.Callbacks.on_train_batch_end , such as {'loss': 0.2, 'accuracy': 0.7} . predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False) Generates output predictions for the input samples. Computation is done in batches. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., model(x) , or model(x, training=False) if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference. Arguments: x: Input samples. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A tf.data dataset. - A generator or keras.utils.Sequence instance. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given in the Unpacking behavior for iterator-like inputs section of Model.fit . batch_size: Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of dataset, generators, or keras.utils.Sequence instances (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . If x is a tf.data dataset and steps is None, predict will run until the input dataset is exhausted. callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during prediction. See callbacks . max_queue_size: Integer. Used for generator or keras.utils.Sequence input only. Maximum size for the generator queue. If unspecified, max_queue_size will default to 10. workers: Integer. Used for generator or keras.utils.Sequence input only. Maximum number of processes to spin up when using process-based threading. If unspecified, workers will default to 1. If 0, will execute the generator on the main thread. use_multiprocessing: Boolean. Used for generator or keras.utils.Sequence input only. If True , use process-based threading. If unspecified, use_multiprocessing will default to False . Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes. See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. predict_generator(generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0) Generates predictions for the input samples from a data generator. (deprecated) Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please use Model.predict, which supports generators. DEPRECATED: Model.predict now supports generators, so there is no longer any need to use this endpoint. predict_on_batch(x) Returns predictions for a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. predict_step(data) The logic for one inference step. This method can be overridden to support custom inference logic. This method is called by Model.make_predict_function . This method should contain the mathemetical logic for one step of inference. This typically includes the forward pass. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_predict_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: The result of one inference step, typically the output of calling the Model on data. reset_metrics() Resets the state of metrics. reset_states() None save(filepath, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None) Saves the model to Tensorflow SavedModel or a single HDF5 file. The savefile includes: - The model architecture, allowing to re-instantiate the model. - The model weights. - The state of the optimizer, allowing to resume training exactly where you left off. This allows you to save the entirety of the state of a model in a single file. Saved models can be reinstantiated via keras.models.load_model . The model returned by load_model is a compiled model ready to be used (unless the saved model was never compiled in the first place). Models built with the Sequential and Functional API can be saved to both the HDF5 and SavedModel formats. Subclassed models can only be saved with the SavedModel format. Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as \"dense_1/kernel:0\" . It is recommended that you use the layer properties to access specific variables, e.g. model.get_layer(\"dense_1\").kernel`. Arguments: filepath: String, path to SavedModel or H5 file to save the model. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. include_optimizer: If True, save optimizer's state together. save_format: Either 'tf' or 'h5', indicating whether to save the model to Tensorflow SavedModel or HDF5. Defaults to 'tf' in TF 2.X, and 'h5' in TF 1.X. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the signatures argument in tf.saved_model.save for details. options: Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel. Example: from keras.models import load_model model.save('my_model.h5') # creates a HDF5 file 'my_model.h5' del model # deletes the existing model # returns a compiled model # identical to the previous one model = load_model('my_model.h5') save_weights(filepath, overwrite=True, save_format=None) Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the save_format argument. When saving in HDF5 format, the weight file has: - layer_names (attribute), a list of strings (ordered names of model layers). - For every layer, a group named layer.name - For every such layer group, a group attribute weight_names , a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as tf.train.Checkpoint , including any Layer instances or Optimizer instances assigned to object attributes. For networks constructed from inputs and outputs using tf.keras.Model(inputs, outputs) , Layer instances used by the network are tracked/saved automatically. For user-defined classes which inherit from tf.keras.Model , Layer instances must be assigned to object attributes, typically in the constructor. See the documentation of tf.train.Checkpoint and tf.keras.Model for details. While the formats are the same, do not mix save_weights and tf.train.Checkpoint . Checkpoints saved by Model.save_weights should be loaded using Model.load_weights . Checkpoints saved using tf.train.Checkpoint.save should be restored using the corresponding tf.train.Checkpoint.restore . Prefer tf.train.Checkpoint over save_weights for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, self for save_weights , and greedily matching attribute names. For Model.save this is the Model , and for Checkpoint.save this is the Checkpoint even if the Checkpoint has a model attached. This means saving a tf.keras.Model using save_weights and loading into a tf.train.Checkpoint with a Model attached (or vice versa) will not match the Model 's variables. See the guide to training checkpoints for details on the TensorFlow format. Arguments: filepath: String, path to the file to save the weights to. When saving in TensorFlow format, this is the prefix used for checkpoint files (multiple files are generated). Note that the '.h5' suffix causes weights to be saved in HDF5 format. overwrite: Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt. save_format: Either 'tf' or 'h5'. A filepath ending in '.h5' or '.keras' will default to HDF5 if save_format is None . Otherwise None defaults to 'tf'. Raises: ImportError: If h5py is not available when attempting to save in HDF5 format. ValueError: For invalid/unknown format arguments. set_weights(weights) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: ValueError: If the provided weights list does not match the layer's specifications. summary(line_length=None, positions=None, print_fn=None) Prints a string summary of the network. Arguments: line_length: Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes). positions: Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.] . print_fn: Print function to use. Defaults to print . It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary. Raises: ValueError: if summary() is called before the model is built. test_on_batch(x, y=None, sample_weight=None, reset_metrics=True, return_dict=False) Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. test_step(data) The logic for one evaluation step. This method can be overridden to support custom evaluation logic. This method is called by Model.make_test_function . This function should contain the mathemetical logic for one step of evaluation. This typically includes the forward pass, loss calculation, and metrics updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_test_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. to_json( kwargs) Returns a JSON string containing the network configuration. To load a network from a JSON save file, use keras.models.model_from_json(json_string, custom_objects={}) . Arguments: **kwargs: Additional keyword arguments to be passed to json.dumps() . Returns: A JSON string. to_yaml( kwargs) Returns a yaml string containing the network configuration. To load a network from a yaml save file, use keras.models.model_from_yaml(yaml_string, custom_objects={}) . custom_objects should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Arguments: **kwargs: Additional keyword arguments to be passed to yaml.dump() . Returns: A YAML string. Raises: ImportError: if yaml module is not found. train_on_batch(x, y=None, sample_weight=None, class_weight=None, reset_metrics=True, return_dict=False) Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data x , it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with x (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile(). class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. reset_metrics: If True , the metrics returned will be only for this batch. If False , the metrics will be statefully accumulated across batches. return_dict: If True , loss and metric results are returned as a dict, with each key being the name of the metric. If False , they are returned as a list. Returns: Scalar training loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute model.metrics_names will give you the display labels for the scalar outputs. Raises: ValueError: In case of invalid user-provided arguments. train_step(data) The logic for one training step. This method can be overridden to support custom training logic. This method is called by Model.make_train_function . This method should contain the mathemetical logic for one step of training. This typically includes the forward pass, loss calculation, backpropagation, and metric updates. Configuration details for how this logic is run (e.g. tf.function and tf.distribute.Strategy settings), should be left to Model.make_train_function , which can also be overridden. Arguments: data: A nested structure of Tensor s. Returns: A dict containing values that will be passed to tf.keras.callbacks.CallbackList.on_train_batch_end . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . with_name_scope(method) Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) >>> mod.w Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope.","title":"Methods"},{"location":"user_guide/nlp/DCNN/#properties","text":"activity_regularizer Optional regularizer function for the output of this layer. distribute_strategy The tf.distribute.Strategy this model was created under. dtype Dtype used by the weights of the layer, set in the constructor. dynamic Whether the layer is dynamic (eager-only); set in the constructor. inbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. input Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. input_mask Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. input_shape Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. input_spec Gets the network's input specs. Returns: A list of InputSpec instances (one per input to the model) or a single instance if the model has only one input. layers None losses Losses which are associated with this Layer . Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing losses under a tf.GradientTape will propagate gradients back to the corresponding variables. Returns: A list of tensors. metrics Returns the model's metrics added using compile , add_metric APIs. Note: metrics are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> [m.name for m in model.metrics] [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> [m.name for m in model.metrics] ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.add_metric( ... tf.reduce_sum(output_2), name='mean', aggregation='mean') >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> [m.name for m in model.metrics] ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc', 'mean'] metrics_names Returns the model's display labels for all outputs. Note: metrics_names are available only after a keras.Model has been trained/evaluated on actual data. Examples: >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> outputs = tf.keras.layers.Dense(2)(inputs) >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"]) >>> model.metrics_names [] >>> x = np.random.random((2, 3)) >>> y = np.random.randint(0, 2, (2, 2)) >>> _ = model.fit(x, y, verbose=0) >>> model.metrics_names ['loss', 'mae'] >>> inputs = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2, name='out') >>> output_1 = d(inputs) >>> output_2 = d(inputs) >>> model = tf.keras.models.Model( ... inputs=inputs, outputs=[output_1, output_2]) >>> model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"]) >>> _ = model.fit(x, (y, y), verbose=0) >>> model.metrics_names ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae', 'out_1_acc'] name Name of the layer (string), set in the constructor. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables None non_trainable_weights List of all non-trainable weights tracked by this layer. Non-trainable weights are not updated during training. They are expected to be updated manually in call() . Returns: A list of non-trainable variables. outbound_nodes Deprecated, do NOT use! Only for compatibility with external Keras. output Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. output_mask Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. output_shape Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. run_eagerly Settable attribute indicating whether the model should run eagerly. Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls. By default, we will attempt to compile your model to a static graph to deliver the best execution performance. Returns: Boolean, whether the model should run eagerly. state_updates Returns the updates from all layers that are stateful. This is useful for separating training updates and state updates, e.g. when we need to update a layer's internal state during prediction. Returns: A list of update ops. stateful None submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Returns: A list of trainable variables. updates None variables Returns the list of all layer variables/weights. Alias of self.weights . Returns: A list of variables. weights Returns the list of all layer variables/weights. Returns: A list of variables.","title":"Properties"},{"location":"user_guide/nlp/Processor_data/","text":"Processor_data Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data) Methods apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Pdding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding","title":"Processor data"},{"location":"user_guide/nlp/Processor_data/#processor_data","text":"Processor_data(target_vocab_size=65536, language='en', value=0, padding='post', name='NLP') The DCNN class corresponds to the Neural Convolution Network algorithm for Natural Language Processing. Parameters mame: Instance class name Attributes clean: function Modulo limpieza de texto por medio de expresiones regulares Examples: cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"] data = pd.read_csv( TRAIN, header=None, names=cols, engine=\"python\", encoding=\"latin1\" ) data.drop([\"id\", \"date\", \"query\", \"user\"], axis=1, inplace=True) nlptrans = Processor() data_process = nlptrans.process_text(data)","title":"Processor_data"},{"location":"user_guide/nlp/Processor_data/#methods","text":"apply_non_breaking_prefix(text, language='en') clean words with a period at the end to make it easier for us to use. Parameters text: Text to apply cleaning. language: str Language a nonbreaking_prefix. options: en / es / fr. apply_padding(data, eval=False) El Pdding es una forma especial de enmascaramiento donde los pasos enmascarados se encuentran al comienzo o al comienzo de una secuencia. El padding proviene de la necesidad de codificar datos de secuencia en lotes contiguos: para que todas las secuencias en un lote se ajusten a una longitud estandar dada, es necesario rellenar o truncar algunas secuencias. clean(data) Clean text. encode_data(data, eval=False) Encoder all text process_text(data, eval=False) Procesador completo de texto: - Limpieza con expresiones regulares - Tokenizador - Padding","title":"Methods"},{"location":"user_guide/nlp/Text%20Helper%20Functions/","text":"FUNCIONES DE AYUDA AL PROCESADO DE TEXTO MLearner MLearner pretende ser una libreria de herramientas utiles para desarrollar algoritmos de Machine Learning e IA de manera mas facil e intuitiva. El desarrollo de esta libreria me ha servido para adquirir conocimientos de creacion de un proyecto de desarrollo software: Integracion Continua y Despliegue Continuo. Gestion de Repositorio a nivel de proyecto. Gestion de Repositorio OpenSource. Clean Code en desarrollo software. Frameworks Machine Learning y Deep Learning. Automatizacion de testing. Documentacion de codigo. Documentacion de la Libreria entorno Web. Empaquetacion y publicacion en Pypi. PyPI Para instalar MLearner ejecute: pip install mlearner Como alternativa, puede descargar el paquete directamente desde PyPI https://pypi.python.org/pypi/mlearner, posteriormente desarchivelo y navegue hasta la ruta del paquete, una vez alli ejecute el siguiente comando: python setup.py install Links Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner Text Helper Functions Implementation of functions for Natural language Processing from mlearner.nlp.helpers import * Overview Helpers List of implemented modules: URL Emoticons Email Hash Mention Number Phone Number Year Non Alphanumeric Punctuations Repetitive Character Dollar Number-Greater Number-Lesser Dates Only Words Only Numbers Boundaries Search Pick Sentence Duplicate Sentence Caps Words Length of Words Length of Characters Get ID Specific String Rows Hex code to Color Tags IP Address Mac Address Subword Latitude & Longitude PAN Phone Number Country Code Domain Importacion de Librerias import os import numpy as np import pandas as pd import matplotlib.pyplot as plt from mlearner import nlp from mlearner.preprocessing import DataAnalyst from mlearner.utils import keras_checkpoint import emoji %load_ext autoreload %autoreload 2 %matplotlib inline The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Carga del dataset SMS Spam Collection v.1 The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. The Grumbletext Web site is: http://www.grumbletext.co.uk/ file = \"data/SMSSpamCollection.txt\" file_csv = \"data/SMSSpamCollection.csv\" if not os.path.isfile(file_csv): text = nlp.open_txt(file).replace(\"\\t\", \"\\n\").split(\"\\n\") del text[-1] # Se borra la linea del final d = {'target': text[0::2], 'text': text[1::2]} sms = pd.DataFrame(d) sms.to_csv(\"data/SMSSpamCollection.csv\", index=False) else: sms = pd.read_csv(file_csv) sms.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... dataset = DataAnalyst.load_dataframe(sms) dataset.distribution_targets(target=[\"target\"]) Procesado del Texto data_clean = dataset.data.copy() URLs find_url(text) : Busqueda de URLs en el texto from mlearner.nlp.helpers import find_url data_clean['url'] = dataset.data['text'].apply(lambda x : find_url(x)) data_clean[data_clean[\"url\"]!=\"\"][[\"target\", \"url\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target url 15 spam http://wap. 305 spam http://img. 518 spam http://www.bubbletext.com 635 spam http://www.e-tlp.co.uk/expressoffer 833 spam http://www.e-tlp.co.uk/expressoffer Emojis find_emoji(text) : Busqueda de emojis en el texto. remove_emoji(text) : Borra los emoji del texto. from mlearner.nlp.helpers import find_emoji, remove_emoji sentence=\"I play () ... ()\" find_emoji(sentence) ['soccer_ball', 'beaming_face_with_smiling_eyes'] data_clean['emoji'] = dataset.data['text'].apply(lambda x : find_emoji(x)) data_clean['text'] = dataset.data['text'].apply(lambda x : remove_emoji(x)) Email find_email(text) : Extraccion de emails del texto. from mlearner.nlp.helpers import find_email data_clean['emails'] = dataset.data['text'].apply(lambda x : find_email(x)) data_clean[data_clean[\"emails\"]!=\"\"][[\"target\", \"emails\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target emails 135 spam 31p.msg@150p 136 ham yijue@hotmail.com 235 spam recd@thirtyeight 474 spam 31p.msg@150p 541 spam MonthlySubscription@50p Hash find_hash(text) : Busqueda de Hashtags en el texto from mlearner.nlp.helpers import find_hash data_clean['Hash'] = dataset.data['text'].apply(lambda x : find_hash(x)) data_clean[data_clean[\"Hash\"]!=\"\"][[\"target\", \"Hash\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Hash 471 spam 5000 1781 spam 150 1985 spam 5000 3895 spam 5000 data_clean[\"text\"].iloc[471] 'okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm' Mention find_at(text) : Busqueda de menciones \"@\" en el texto from mlearner.nlp.helpers import find_at data_clean['Mention'] = dataset.data['text'].apply(lambda x : find_at(x)) data_clean[data_clean[\"Mention\"]!=\"\"][[\"target\", \"Mention\"]].tail(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Mention 3299 spam 150p 3501 spam kiefer 4906 spam Warner kiosk kiosk 5104 spam netvision 5344 spam 150p Numbers find_number(text) : Busqueda de numeros en el texto. from mlearner.nlp.helpers import find_number data_clean['Numbers'] = dataset.data['text'].apply(lambda x : find_number(x)) data_clean[data_clean[\"Numbers\"]!=\"\"][[\"target\", \"Numbers\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Numbers 2 spam 2 21 2005 87121 08452810075 18 5 spam 3 1 50 7 ham 9 8 spam 900 09061701461 341 12 9 spam 11 08002986030 Phone Number find_phone_number(text) : Busqueda de numeros de telefono espa\u00f1oles en el texto. from mlearner.nlp.helpers import find_phone_number data_clean['phone_number'] = dataset.data['text'].apply(lambda x : find_phone_number(x)) find_phone_number(\"+34666999666\") [('+34', '6', '6')] Find Year find_year(text) : Busqueda de a\u00f1os de nacimiento en el texto [1940-2040] from mlearner.nlp.helpers import find_year data_clean['Years'] = dataset.data['text'].apply(lambda x : find_year(x)) data_clean[data_clean[\"Years\"]!=\"\"][[\"target\", \"Years\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Years 0 ham [] 1 ham [] 2 spam [2005] 3 ham [] 4 ham [] Non Alphanumeric characters find_nonalp(text) : Extraccion de caracteres no alfanumericos. from mlearner.nlp.helpers import find_nonalp data_clean['nonalp'] = dataset.data['text'].apply(lambda x : find_nonalp(x)) data_clean[data_clean[\"nonalp\"]!=\"\"][[\"target\", \"nonalp\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target nonalp 0 ham [,, ., ., ., ., ., ., ., .] 1 ham [., ., ., ., ., .] 2 spam [., (, ), &, ', '] 3 ham [., ., ., ., ., .] 4 ham [', ,] Retrieve punctuations from sentence find_punct(text) : Signos de puntuacion. from mlearner.nlp.helpers import find_punct data_clean['find_punct'] = dataset.data['text'].apply(lambda x : find_punct(x)) data_clean[data_clean[\"find_punct\"]!=\"\"][[\"target\", \"find_punct\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_punct 0 ham [,, ., ., ., ., ., ., ., .] 1 ham [., ., ., ., ., .] 2 spam [., (, ), &, ', '] 3 ham [., ., ., ., ., .] 4 ham [', ,] Unique Char unique_char(sentence) : Elimina los caracteres repetidos de una palabra. from mlearner.nlp.helpers import unique_char sentence=\"I lovee Machinee learning!\" unique_char(sentence) 'I love Machine learning!' Prices find_coin(text, symbol=\"$\") : Busqueda de precios en el texto from mlearner.nlp.helpers import find_coin data_clean['find_coin$'] = dataset.data['text'].apply(lambda x : find_coin(x, symbol=\"$\")) data_clean[data_clean[\"find_coin$\"]!=\"\"][[\"target\", \"find_coin$\"]].head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_coin$ 60 ham $1 123 spam $350 dataset.data['text'].iloc[60] \"Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\" Numbers great num_great(text) : Busqueda de numeros mayores a 930. from mlearner.nlp.helpers import num_great data_clean['num_great'] = dataset.data['text'].apply(lambda x : num_great(x)) data_clean[data_clean[\"num_great\"]!=\"\"][[\"target\", \"num_great\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target num_great 2 spam 2005 87121 8452810075 8 spam 9061701461 9 spam 8002986030 11 spam 87575 12 spam 81010 4403 Numbers less num_less(text) : Busqueda de numeros menores a 930. from mlearner.nlp.helpers import num_less data_clean['num_less'] = dataset.data['text'].apply(lambda x : num_less(x)) data_clean[data_clean[\"num_less\"]!=\"\"][[\"target\", \"num_less\"]].tail(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target num_less 5557 ham 4 2 2 4 5564 ham 2 5565 ham 6 5568 spam 2 5569 spam 2 2 Find Dates find_dates(text) : Busqueda de fechas [mm-dd-yyyy] from mlearner.nlp.helpers import find_dates sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) [('04', '28', '2020')] data_clean['find_dates'] = dataset.data['text'].apply(lambda x : find_dates(x)) Only Words only_words(text) : Eliminar los numeros del texto. from mlearner.nlp.helpers import only_words data_clean['only_words'] = dataset.data['text'].apply(lambda x : only_words(x)) data_clean[data_clean[\"only_words\"]!=\"\"][[\"target\", \"only_words\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target only_words 0 ham Go until jurong point crazy Available only in ... 1 ham Ok lar Joking wif u oni 2 spam Free entry in a wkly comp to win FA Cup final ... 3 ham U dun say so early hor U c already then say 4 ham Nah I don t think he goes to usf he lives arou... Search Key search_string(text, key) : Comprobar existencia de palabras en la frase. from mlearner.nlp.helpers import search_string data_clean['search_string'] = dataset.data['text'].apply(lambda x : search_string(x,' day ')) data_clean[data_clean[\"search_string\"]==True][[\"target\", \"search_string\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target search_string 163 ham True 252 ham True 315 ham True 359 ham True 365 ham True data_clean[\"text\"].iloc[163] \"I'm so in love with you. I'm excited each day i spend with you. You make me so happy.\" pick only key sentence pick_only_key_sentence(text, keyword) : Devuelve las frases que contiene la palabra (Keyword) seleccionada. from mlearner.nlp.helpers import pick_only_key_sentence data_clean['pick_only_key_sentence'] = dataset.data['text'].apply(lambda x : pick_only_key_sentence(x,' day ')) data_clean[data_clean[\"pick_only_key_sentence\"]!=\"\"][[\"target\", \"pick_only_key_sentence\"]].iloc[163:165] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target pick_only_key_sentence 163 ham [ I'm excited each day i spend with you] 164 spam [] pick unique sentence pick_unique_sentence(text) : Elimina frases duplicadas from mlearner.nlp.helpers import pick_unique_sentence sentence=\"I thank doctors\\nDoctors are working very hard in this pandemic situation\\nI thank doctors\" pick_unique_sentence(sentence) ['Doctors are working very hard in this pandemic situation', 'I thank doctors'] data_clean['pick_unique_sentence'] = dataset.data['text'].apply(lambda x : pick_unique_sentence(x)) Capital words find_capital(text) : Busqueda de palabras con primera letra en mayuscula. from mlearner.nlp.helpers import find_capital data_clean['find_capital'] = dataset.data['text'].apply(lambda x : find_capital(x)) data_clean[data_clean[\"find_capital\"]!=\"\"][[\"target\", \"find_capital\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_capital 0 ham [Go, Available, Cine] 1 ham [Ok, Joking] 2 spam [Free, FA, Cup, May, Text, FA] 3 ham [] 4 ham [Nah] Remove tag html remove_tag(text) : Elimina los tags de html. from mlearner.nlp.helpers import remove_tag sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) 'Markdown sentences can use for breaks and for italics' data_clean['remove_tag'] = dataset.data['text'].apply(lambda x : remove_tag(x)) Mac address mac_add(text) : Busqueda de Mac address en el texto from mlearner.nlp.helpers import mac_add sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc. Other details will be mentioned\" mac_add(sentence) ['00:24:17:b1:cc:cc'] data_clean['mac_add'] = dataset.data['text'].apply(lambda x : mac_add(x)) IP address ip_add(text) : Busqueda de IP address en el texto from mlearner.nlp.helpers import ip_add sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) ['125.16.100.1'] data_clean['ip_add'] = dataset.data['text'].apply(lambda x : ip_add(x)) Extract number of subwords subword(string, sub) : Devuelve el numero de veces que aparece la raiz de la palabra. from mlearner.nlp.helpers import subword sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others 2 data_clean['subword'] = dataset.data['text'].apply(lambda x : subword(x, \"on\")) data_clean[[\"target\", \"text\", \"subword\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text subword 0 ham Go until jurong point, crazy.. Available only ... 2 1 ham Ok lar... Joking wif u oni... 1 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 1 3 ham U dun say so early hor... U c already then say... 0 4 ham Nah I don't think he goes to usf, he lives aro... 1 Valid latitude & longitude lat_lon(string) : Devuelve si el dato de Latitud y longitud es correcto. from mlearner.nlp.helpers import lat_lon lat_lon('28.6466772,76.8130649', display=True) lat_lon('2324.3244,3423.432423', display=True) [28.6466772,76.8130649] is valid latitude & longitude [2324.3244,3423.432423] is not a valid latitude & longitude False data_clean['lat_lon'] = dataset.data['text'].apply(lambda x : lat_lon(x)) data_clean[[\"target\", \"text\", \"lat_lon\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text lat_lon 0 ham Go until jurong point, crazy.. Available only ... False 1 ham Ok lar... Joking wif u oni... False 2 spam Free entry in 2 a wkly comp to win FA Cup fina... False 3 ham U dun say so early hor... U c already then say... False 4 ham Nah I don't think he goes to usf, he lives aro... False Valid latitude & longitude find_domain(string) : Devuelve si el dato de Latitud y longitud es correcto. from mlearner.nlp.helpers import find_domain data_clean['find_domain'] = dataset.data['text'].apply(lambda x : find_domain(x)) data_clean[[\"target\", \"text\", \"find_domain\"]].iloc[10:15] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text find_domain 10 ham I'm gonna be home soon and i don't want to tal... [] 11 spam SIX chances to win CASH! From 100 to 20,000 po... [] 12 spam URGENT! You have won a 1 week FREE membership ... [www.dbuk] 13 ham I've been searching for the right words to tha... [] 14 ham I HAVE A DATE ON SUNDAY WITH WILL!! [] data_clean[\"text\"].iloc[12] 'URGENT! You have won a 1 week FREE membership in our \u00a3100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18' data_clean.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 target ham ham spam ham ham spam ham ham spam spam ... ham ham ham ham spam spam ham ham ham ham text Go until jurong point, crazy.. Available only ... Ok lar... Joking wif u oni... Free entry in 2 a wkly comp to win FA Cup fina... U dun say so early hor... U c already then say... Nah I don't think he goes to usf, he lives aro... FreeMsg Hey there darling it's been 3 week's n... Even my brother is not like to speak with me. ... As per your request 'Melle Melle (Oru Minnamin... WINNER!! As a valued network customer you have... Had your mobile 11 months or more? U R entitle... ... Ok lor... Sony ericsson salesman... I ask shuh... Ard 6 like dat lor. Why don't you wait 'til at least wednesday to ... Huh y lei... REMINDER FROM O2: To get 2.50 pounds free call... This is the 2nd time we have tried 2 contact u... Will \u00fc b going to esplanade fr home? Pity, * was in mood for that. So...any other s... The guy did some bitching but I acted like i'd... Rofl. Its true to its name url ... emoji [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] emails ... Hash ... Mention ... Numbers 2 21 2005 87121 08452810075 18 3 1 50 9 900 09061701461 341 12 11 08002986030 ... 2 6 2 2 50 2 2 2 750 2 087187272008 1 10 phone_number [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [(, 7, 0)] [] [] [] [] Years [] [] [2005] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] nonalp [,, ., ., ., ., ., ., ., .] [., ., ., ., ., .] [., (, ), &, ', '] [., ., ., ., ., .] [', ,] [', ', !, ', ?, !, ,, \u00a3, .] [., .] [', (, ), ', ., *] [!, !, \u00a3, !, ., ., .] [?, !] ... [., ., ., ., ., ., ., ., .] [.] [', ', .] [., ., .] [:, ., ,] [., \u00a3, ., ,, !, ., -, -, .] [\u00fc, ?] [,, *, ., ., ., ., ?] ['] [.] find_punct [,, ., ., ., ., ., ., ., .] [., ., ., ., ., .] [., (, ), &, ', '] [., ., ., ., ., .] [', ,] [', ', !, ', ?, !, ,, .] [., .] [', (, ), ', ., *] [!, !, !, ., ., .] [?, !] ... [., ., ., ., ., ., ., ., .] [.] [', ', .] [., ., .] [:, ., ,] [., ., ,, !, ., -, -, .] [?] [,, *, ., ., ., ., ?] ['] [.] find_coin$ ... num_great 2005 87121 8452810075 9061701461 8002986030 ... 87187272008 num_less 2 3 12 11 ... 2 6 2 2 2 find_dates [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] only_words Go until jurong point crazy Available only in ... Ok lar Joking wif u oni Free entry in a wkly comp to win FA Cup final ... U dun say so early hor U c already then say Nah I don t think he goes to usf he lives arou... FreeMsg Hey there darling it s been week s now... Even my brother is not like to speak with me T... As per your request Melle Melle Oru Minnaminun... WINNER As a valued network customer you have b... Had your mobile months or more U R entitled to... ... Ok lor Sony ericsson salesman I ask shuhui the... Ard like dat lor Why don t you wait til at least wednesday to s... Huh y lei REMINDER FROM To get pounds free call credit a... This is the time we have tried contact u U hav... Will \u00fc b going to esplanade fr home Pity was in mood for that So any other suggest... The guy did some bitching but I acted like i d... Rofl Its true to its name search_string False False False False False False False False False False ... False False False False False False False False False False pick_only_key_sentence [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] pick_unique_sentence [Go until jurong point, crazy.. Available only... [Ok lar... Joking wif u oni...] [Free entry in 2 a wkly comp to win FA Cup fin... [U dun say so early hor... U c already then sa... [Nah I don't think he goes to usf, he lives ar... [FreeMsg Hey there darling it's been 3 week's ... [Even my brother is not like to speak with me.... [As per your request 'Melle Melle (Oru Minnami... [WINNER!! As a valued network customer you hav... [Had your mobile 11 months or more? U R entitl... ... [Ok lor... Sony ericsson salesman... I ask shu... [Ard 6 like dat lor.] [Why don't you wait 'til at least wednesday to... [Huh y lei...] [REMINDER FROM O2: To get 2.50 pounds free cal... [This is the 2nd time we have tried 2 contact ... [Will \u00fc b going to esplanade fr home?] [Pity, * was in mood for that. So...any other ... [The guy did some bitching but I acted like i'... [Rofl. Its true to its name] find_capital [Go, Available, Cine] [Ok, Joking] [Free, FA, Cup, May, Text, FA] [] [Nah] [FreeMsg, Hey, Tb, XxX] [Even, They] [As, Melle, Melle, Oru, Minnaminunginte, Nurun... [WINNER, As, To, Claim, KL341, Valid] [Had, Update, Free, Call, The, Mobile, Update,... ... [Ok, Sony] [Ard] [Why] [Huh] [REMINDER, FROM, O2, To] [This, Pound, NOW1, Only, BT] [Will] [Pity, So] [The] [Rofl, Its] remove_tag Go until jurong point, crazy.. Available only ... Ok lar... Joking wif u oni... Free entry in 2 a wkly comp to win FA Cup fina... U dun say so early hor... U c already then say... Nah I don't think he goes to usf, he lives aro... FreeMsg Hey there darling it's been 3 week's n... Even my brother is not like to speak with me. ... As per your request 'Melle Melle (Oru Minnamin... WINNER!! As a valued network customer you have... Had your mobile 11 months or more? U R entitle... ... Ok lor... Sony ericsson salesman... I ask shuh... Ard 6 like dat lor. Why don't you wait 'til at least wednesday to ... Huh y lei... REMINDER FROM O2: To get 2.50 pounds free call... This is the 2nd time we have tried 2 contact u... Will \u00fc b going to esplanade fr home? Pity, * was in mood for that. So...any other s... The guy did some bitching but I acted like i'd... Rofl. Its true to its name mac_add [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [087187272008] [] [] [] [] ip_add [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] subword 2 1 1 0 1 0 0 0 1 2 ... 3 0 1 0 0 3 0 1 0 0 lat_lon False False False False False False False False False False ... False False False False False False False False False False find_domain [] [] [] [] [] [1.50] [] [] [] [] ... [] [] [] [] [2.50] [] [] [] [] [] 27 rows \u00d7 5574 columns","title":"FUNCIONES DE AYUDA AL PROCESADO DE TEXTO"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#funciones-de-ayuda-al-procesado-de-texto","text":"","title":"FUNCIONES DE AYUDA AL PROCESADO DE TEXTO"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#mlearner","text":"MLearner pretende ser una libreria de herramientas utiles para desarrollar algoritmos de Machine Learning e IA de manera mas facil e intuitiva. El desarrollo de esta libreria me ha servido para adquirir conocimientos de creacion de un proyecto de desarrollo software: Integracion Continua y Despliegue Continuo. Gestion de Repositorio a nivel de proyecto. Gestion de Repositorio OpenSource. Clean Code en desarrollo software. Frameworks Machine Learning y Deep Learning. Automatizacion de testing. Documentacion de codigo. Documentacion de la Libreria entorno Web. Empaquetacion y publicacion en Pypi.","title":"MLearner"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#pypi","text":"Para instalar MLearner ejecute: pip install mlearner Como alternativa, puede descargar el paquete directamente desde PyPI https://pypi.python.org/pypi/mlearner, posteriormente desarchivelo y navegue hasta la ruta del paquete, una vez alli ejecute el siguiente comando: python setup.py install","title":"PyPI"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#links","text":"Documentation: https://jaisenbe58r.github.io/MLearner/ Source code repository: https://github.com/jaisenbe58r/MLearner PyPI: https://pypi.python.org/pypi/mlearner","title":"Links"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#text-helper-functions","text":"Implementation of functions for Natural language Processing from mlearner.nlp.helpers import *","title":"Text Helper Functions"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#overview","text":"","title":"Overview"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#helpers","text":"List of implemented modules: URL Emoticons Email Hash Mention Number Phone Number Year Non Alphanumeric Punctuations Repetitive Character Dollar Number-Greater Number-Lesser Dates Only Words Only Numbers Boundaries Search Pick Sentence Duplicate Sentence Caps Words Length of Words Length of Characters Get ID Specific String Rows Hex code to Color Tags IP Address Mac Address Subword Latitude & Longitude PAN Phone Number Country Code Domain","title":"Helpers"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#importacion-de-librerias","text":"import os import numpy as np import pandas as pd import matplotlib.pyplot as plt from mlearner import nlp from mlearner.preprocessing import DataAnalyst from mlearner.utils import keras_checkpoint import emoji %load_ext autoreload %autoreload 2 %matplotlib inline The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Importacion de Librerias"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#carga-del-dataset","text":"SMS Spam Collection v.1 The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. The Grumbletext Web site is: http://www.grumbletext.co.uk/ file = \"data/SMSSpamCollection.txt\" file_csv = \"data/SMSSpamCollection.csv\" if not os.path.isfile(file_csv): text = nlp.open_txt(file).replace(\"\\t\", \"\\n\").split(\"\\n\") del text[-1] # Se borra la linea del final d = {'target': text[0::2], 'text': text[1::2]} sms = pd.DataFrame(d) sms.to_csv(\"data/SMSSpamCollection.csv\", index=False) else: sms = pd.read_csv(file_csv) sms.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... dataset = DataAnalyst.load_dataframe(sms) dataset.distribution_targets(target=[\"target\"])","title":"Carga del dataset"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#procesado-del-texto","text":"data_clean = dataset.data.copy()","title":"Procesado del Texto"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#urls","text":"find_url(text) : Busqueda de URLs en el texto from mlearner.nlp.helpers import find_url data_clean['url'] = dataset.data['text'].apply(lambda x : find_url(x)) data_clean[data_clean[\"url\"]!=\"\"][[\"target\", \"url\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target url 15 spam http://wap. 305 spam http://img. 518 spam http://www.bubbletext.com 635 spam http://www.e-tlp.co.uk/expressoffer 833 spam http://www.e-tlp.co.uk/expressoffer","title":"URLs"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#emojis","text":"find_emoji(text) : Busqueda de emojis en el texto. remove_emoji(text) : Borra los emoji del texto. from mlearner.nlp.helpers import find_emoji, remove_emoji sentence=\"I play () ... ()\" find_emoji(sentence) ['soccer_ball', 'beaming_face_with_smiling_eyes'] data_clean['emoji'] = dataset.data['text'].apply(lambda x : find_emoji(x)) data_clean['text'] = dataset.data['text'].apply(lambda x : remove_emoji(x))","title":"Emojis"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#email","text":"find_email(text) : Extraccion de emails del texto. from mlearner.nlp.helpers import find_email data_clean['emails'] = dataset.data['text'].apply(lambda x : find_email(x)) data_clean[data_clean[\"emails\"]!=\"\"][[\"target\", \"emails\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target emails 135 spam 31p.msg@150p 136 ham yijue@hotmail.com 235 spam recd@thirtyeight 474 spam 31p.msg@150p 541 spam MonthlySubscription@50p","title":"Email"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#hash","text":"find_hash(text) : Busqueda de Hashtags en el texto from mlearner.nlp.helpers import find_hash data_clean['Hash'] = dataset.data['text'].apply(lambda x : find_hash(x)) data_clean[data_clean[\"Hash\"]!=\"\"][[\"target\", \"Hash\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Hash 471 spam 5000 1781 spam 150 1985 spam 5000 3895 spam 5000 data_clean[\"text\"].iloc[471] 'okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm'","title":"Hash"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#mention","text":"find_at(text) : Busqueda de menciones \"@\" en el texto from mlearner.nlp.helpers import find_at data_clean['Mention'] = dataset.data['text'].apply(lambda x : find_at(x)) data_clean[data_clean[\"Mention\"]!=\"\"][[\"target\", \"Mention\"]].tail(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Mention 3299 spam 150p 3501 spam kiefer 4906 spam Warner kiosk kiosk 5104 spam netvision 5344 spam 150p","title":"Mention"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#numbers","text":"find_number(text) : Busqueda de numeros en el texto. from mlearner.nlp.helpers import find_number data_clean['Numbers'] = dataset.data['text'].apply(lambda x : find_number(x)) data_clean[data_clean[\"Numbers\"]!=\"\"][[\"target\", \"Numbers\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Numbers 2 spam 2 21 2005 87121 08452810075 18 5 spam 3 1 50 7 ham 9 8 spam 900 09061701461 341 12 9 spam 11 08002986030","title":"Numbers"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#phone-number","text":"find_phone_number(text) : Busqueda de numeros de telefono espa\u00f1oles en el texto. from mlearner.nlp.helpers import find_phone_number data_clean['phone_number'] = dataset.data['text'].apply(lambda x : find_phone_number(x)) find_phone_number(\"+34666999666\") [('+34', '6', '6')]","title":"Phone Number"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#find-year","text":"find_year(text) : Busqueda de a\u00f1os de nacimiento en el texto [1940-2040] from mlearner.nlp.helpers import find_year data_clean['Years'] = dataset.data['text'].apply(lambda x : find_year(x)) data_clean[data_clean[\"Years\"]!=\"\"][[\"target\", \"Years\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target Years 0 ham [] 1 ham [] 2 spam [2005] 3 ham [] 4 ham []","title":"Find Year"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#non-alphanumeric-characters","text":"find_nonalp(text) : Extraccion de caracteres no alfanumericos. from mlearner.nlp.helpers import find_nonalp data_clean['nonalp'] = dataset.data['text'].apply(lambda x : find_nonalp(x)) data_clean[data_clean[\"nonalp\"]!=\"\"][[\"target\", \"nonalp\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target nonalp 0 ham [,, ., ., ., ., ., ., ., .] 1 ham [., ., ., ., ., .] 2 spam [., (, ), &, ', '] 3 ham [., ., ., ., ., .] 4 ham [', ,]","title":"Non Alphanumeric characters"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#retrieve-punctuations-from-sentence","text":"find_punct(text) : Signos de puntuacion. from mlearner.nlp.helpers import find_punct data_clean['find_punct'] = dataset.data['text'].apply(lambda x : find_punct(x)) data_clean[data_clean[\"find_punct\"]!=\"\"][[\"target\", \"find_punct\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_punct 0 ham [,, ., ., ., ., ., ., ., .] 1 ham [., ., ., ., ., .] 2 spam [., (, ), &, ', '] 3 ham [., ., ., ., ., .] 4 ham [', ,]","title":"Retrieve punctuations from sentence"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#unique-char","text":"unique_char(sentence) : Elimina los caracteres repetidos de una palabra. from mlearner.nlp.helpers import unique_char sentence=\"I lovee Machinee learning!\" unique_char(sentence) 'I love Machine learning!'","title":"Unique Char"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#prices","text":"find_coin(text, symbol=\"$\") : Busqueda de precios en el texto from mlearner.nlp.helpers import find_coin data_clean['find_coin$'] = dataset.data['text'].apply(lambda x : find_coin(x, symbol=\"$\")) data_clean[data_clean[\"find_coin$\"]!=\"\"][[\"target\", \"find_coin$\"]].head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_coin$ 60 ham $1 123 spam $350 dataset.data['text'].iloc[60] \"Your gonna have to pick up a $1 burger for yourself on your way home. I can't even move. Pain is killing me.\"","title":"Prices"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#numbers-great","text":"num_great(text) : Busqueda de numeros mayores a 930. from mlearner.nlp.helpers import num_great data_clean['num_great'] = dataset.data['text'].apply(lambda x : num_great(x)) data_clean[data_clean[\"num_great\"]!=\"\"][[\"target\", \"num_great\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target num_great 2 spam 2005 87121 8452810075 8 spam 9061701461 9 spam 8002986030 11 spam 87575 12 spam 81010 4403","title":"Numbers great"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#numbers-less","text":"num_less(text) : Busqueda de numeros menores a 930. from mlearner.nlp.helpers import num_less data_clean['num_less'] = dataset.data['text'].apply(lambda x : num_less(x)) data_clean[data_clean[\"num_less\"]!=\"\"][[\"target\", \"num_less\"]].tail(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target num_less 5557 ham 4 2 2 4 5564 ham 2 5565 ham 6 5568 spam 2 5569 spam 2 2","title":"Numbers less"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#find-dates","text":"find_dates(text) : Busqueda de fechas [mm-dd-yyyy] from mlearner.nlp.helpers import find_dates sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) [('04', '28', '2020')] data_clean['find_dates'] = dataset.data['text'].apply(lambda x : find_dates(x))","title":"Find Dates"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#only-words","text":"only_words(text) : Eliminar los numeros del texto. from mlearner.nlp.helpers import only_words data_clean['only_words'] = dataset.data['text'].apply(lambda x : only_words(x)) data_clean[data_clean[\"only_words\"]!=\"\"][[\"target\", \"only_words\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target only_words 0 ham Go until jurong point crazy Available only in ... 1 ham Ok lar Joking wif u oni 2 spam Free entry in a wkly comp to win FA Cup final ... 3 ham U dun say so early hor U c already then say 4 ham Nah I don t think he goes to usf he lives arou...","title":"Only Words"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#search-key","text":"search_string(text, key) : Comprobar existencia de palabras en la frase. from mlearner.nlp.helpers import search_string data_clean['search_string'] = dataset.data['text'].apply(lambda x : search_string(x,' day ')) data_clean[data_clean[\"search_string\"]==True][[\"target\", \"search_string\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target search_string 163 ham True 252 ham True 315 ham True 359 ham True 365 ham True data_clean[\"text\"].iloc[163] \"I'm so in love with you. I'm excited each day i spend with you. You make me so happy.\"","title":"Search Key"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#pick-only-key-sentence","text":"pick_only_key_sentence(text, keyword) : Devuelve las frases que contiene la palabra (Keyword) seleccionada. from mlearner.nlp.helpers import pick_only_key_sentence data_clean['pick_only_key_sentence'] = dataset.data['text'].apply(lambda x : pick_only_key_sentence(x,' day ')) data_clean[data_clean[\"pick_only_key_sentence\"]!=\"\"][[\"target\", \"pick_only_key_sentence\"]].iloc[163:165] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target pick_only_key_sentence 163 ham [ I'm excited each day i spend with you] 164 spam []","title":"pick only key sentence"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#pick-unique-sentence","text":"pick_unique_sentence(text) : Elimina frases duplicadas from mlearner.nlp.helpers import pick_unique_sentence sentence=\"I thank doctors\\nDoctors are working very hard in this pandemic situation\\nI thank doctors\" pick_unique_sentence(sentence) ['Doctors are working very hard in this pandemic situation', 'I thank doctors'] data_clean['pick_unique_sentence'] = dataset.data['text'].apply(lambda x : pick_unique_sentence(x))","title":"pick unique sentence"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#capital-words","text":"find_capital(text) : Busqueda de palabras con primera letra en mayuscula. from mlearner.nlp.helpers import find_capital data_clean['find_capital'] = dataset.data['text'].apply(lambda x : find_capital(x)) data_clean[data_clean[\"find_capital\"]!=\"\"][[\"target\", \"find_capital\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target find_capital 0 ham [Go, Available, Cine] 1 ham [Ok, Joking] 2 spam [Free, FA, Cup, May, Text, FA] 3 ham [] 4 ham [Nah]","title":"Capital words"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#remove-tag-html","text":"remove_tag(text) : Elimina los tags de html. from mlearner.nlp.helpers import remove_tag sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) 'Markdown sentences can use for breaks and for italics' data_clean['remove_tag'] = dataset.data['text'].apply(lambda x : remove_tag(x))","title":"Remove tag html"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#mac-address","text":"mac_add(text) : Busqueda de Mac address en el texto from mlearner.nlp.helpers import mac_add sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc. Other details will be mentioned\" mac_add(sentence) ['00:24:17:b1:cc:cc'] data_clean['mac_add'] = dataset.data['text'].apply(lambda x : mac_add(x))","title":"Mac address"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#ip-address","text":"ip_add(text) : Busqueda de IP address en el texto from mlearner.nlp.helpers import ip_add sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) ['125.16.100.1'] data_clean['ip_add'] = dataset.data['text'].apply(lambda x : ip_add(x))","title":"IP address"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#extract-number-of-subwords","text":"subword(string, sub) : Devuelve el numero de veces que aparece la raiz de la palabra. from mlearner.nlp.helpers import subword sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others 2 data_clean['subword'] = dataset.data['text'].apply(lambda x : subword(x, \"on\")) data_clean[[\"target\", \"text\", \"subword\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text subword 0 ham Go until jurong point, crazy.. Available only ... 2 1 ham Ok lar... Joking wif u oni... 1 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 1 3 ham U dun say so early hor... U c already then say... 0 4 ham Nah I don't think he goes to usf, he lives aro... 1","title":"Extract number of subwords"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#valid-latitude-longitude","text":"lat_lon(string) : Devuelve si el dato de Latitud y longitud es correcto. from mlearner.nlp.helpers import lat_lon lat_lon('28.6466772,76.8130649', display=True) lat_lon('2324.3244,3423.432423', display=True) [28.6466772,76.8130649] is valid latitude & longitude [2324.3244,3423.432423] is not a valid latitude & longitude False data_clean['lat_lon'] = dataset.data['text'].apply(lambda x : lat_lon(x)) data_clean[[\"target\", \"text\", \"lat_lon\"]].head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text lat_lon 0 ham Go until jurong point, crazy.. Available only ... False 1 ham Ok lar... Joking wif u oni... False 2 spam Free entry in 2 a wkly comp to win FA Cup fina... False 3 ham U dun say so early hor... U c already then say... False 4 ham Nah I don't think he goes to usf, he lives aro... False","title":"Valid latitude &amp; longitude"},{"location":"user_guide/nlp/Text%20Helper%20Functions/#valid-latitude-longitude_1","text":"find_domain(string) : Devuelve si el dato de Latitud y longitud es correcto. from mlearner.nlp.helpers import find_domain data_clean['find_domain'] = dataset.data['text'].apply(lambda x : find_domain(x)) data_clean[[\"target\", \"text\", \"find_domain\"]].iloc[10:15] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } target text find_domain 10 ham I'm gonna be home soon and i don't want to tal... [] 11 spam SIX chances to win CASH! From 100 to 20,000 po... [] 12 spam URGENT! You have won a 1 week FREE membership ... [www.dbuk] 13 ham I've been searching for the right words to tha... [] 14 ham I HAVE A DATE ON SUNDAY WITH WILL!! [] data_clean[\"text\"].iloc[12] 'URGENT! You have won a 1 week FREE membership in our \u00a3100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18' data_clean.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 9 ... 5564 5565 5566 5567 5568 5569 5570 5571 5572 5573 target ham ham spam ham ham spam ham ham spam spam ... ham ham ham ham spam spam ham ham ham ham text Go until jurong point, crazy.. Available only ... Ok lar... Joking wif u oni... Free entry in 2 a wkly comp to win FA Cup fina... U dun say so early hor... U c already then say... Nah I don't think he goes to usf, he lives aro... FreeMsg Hey there darling it's been 3 week's n... Even my brother is not like to speak with me. ... As per your request 'Melle Melle (Oru Minnamin... WINNER!! As a valued network customer you have... Had your mobile 11 months or more? U R entitle... ... Ok lor... Sony ericsson salesman... I ask shuh... Ard 6 like dat lor. Why don't you wait 'til at least wednesday to ... Huh y lei... REMINDER FROM O2: To get 2.50 pounds free call... This is the 2nd time we have tried 2 contact u... Will \u00fc b going to esplanade fr home? Pity, * was in mood for that. So...any other s... The guy did some bitching but I acted like i'd... Rofl. Its true to its name url ... emoji [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] emails ... Hash ... Mention ... Numbers 2 21 2005 87121 08452810075 18 3 1 50 9 900 09061701461 341 12 11 08002986030 ... 2 6 2 2 50 2 2 2 750 2 087187272008 1 10 phone_number [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [(, 7, 0)] [] [] [] [] Years [] [] [2005] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] nonalp [,, ., ., ., ., ., ., ., .] [., ., ., ., ., .] [., (, ), &, ', '] [., ., ., ., ., .] [', ,] [', ', !, ', ?, !, ,, \u00a3, .] [., .] [', (, ), ', ., *] [!, !, \u00a3, !, ., ., .] [?, !] ... [., ., ., ., ., ., ., ., .] [.] [', ', .] [., ., .] [:, ., ,] [., \u00a3, ., ,, !, ., -, -, .] [\u00fc, ?] [,, *, ., ., ., ., ?] ['] [.] find_punct [,, ., ., ., ., ., ., ., .] [., ., ., ., ., .] [., (, ), &, ', '] [., ., ., ., ., .] [', ,] [', ', !, ', ?, !, ,, .] [., .] [', (, ), ', ., *] [!, !, !, ., ., .] [?, !] ... [., ., ., ., ., ., ., ., .] [.] [', ', .] [., ., .] [:, ., ,] [., ., ,, !, ., -, -, .] [?] [,, *, ., ., ., ., ?] ['] [.] find_coin$ ... num_great 2005 87121 8452810075 9061701461 8002986030 ... 87187272008 num_less 2 3 12 11 ... 2 6 2 2 2 find_dates [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] only_words Go until jurong point crazy Available only in ... Ok lar Joking wif u oni Free entry in a wkly comp to win FA Cup final ... U dun say so early hor U c already then say Nah I don t think he goes to usf he lives arou... FreeMsg Hey there darling it s been week s now... Even my brother is not like to speak with me T... As per your request Melle Melle Oru Minnaminun... WINNER As a valued network customer you have b... Had your mobile months or more U R entitled to... ... Ok lor Sony ericsson salesman I ask shuhui the... Ard like dat lor Why don t you wait til at least wednesday to s... Huh y lei REMINDER FROM To get pounds free call credit a... This is the time we have tried contact u U hav... Will \u00fc b going to esplanade fr home Pity was in mood for that So any other suggest... The guy did some bitching but I acted like i d... Rofl Its true to its name search_string False False False False False False False False False False ... False False False False False False False False False False pick_only_key_sentence [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] pick_unique_sentence [Go until jurong point, crazy.. Available only... [Ok lar... Joking wif u oni...] [Free entry in 2 a wkly comp to win FA Cup fin... [U dun say so early hor... U c already then sa... [Nah I don't think he goes to usf, he lives ar... [FreeMsg Hey there darling it's been 3 week's ... [Even my brother is not like to speak with me.... [As per your request 'Melle Melle (Oru Minnami... [WINNER!! As a valued network customer you hav... [Had your mobile 11 months or more? U R entitl... ... [Ok lor... Sony ericsson salesman... I ask shu... [Ard 6 like dat lor.] [Why don't you wait 'til at least wednesday to... [Huh y lei...] [REMINDER FROM O2: To get 2.50 pounds free cal... [This is the 2nd time we have tried 2 contact ... [Will \u00fc b going to esplanade fr home?] [Pity, * was in mood for that. So...any other ... [The guy did some bitching but I acted like i'... [Rofl. Its true to its name] find_capital [Go, Available, Cine] [Ok, Joking] [Free, FA, Cup, May, Text, FA] [] [Nah] [FreeMsg, Hey, Tb, XxX] [Even, They] [As, Melle, Melle, Oru, Minnaminunginte, Nurun... [WINNER, As, To, Claim, KL341, Valid] [Had, Update, Free, Call, The, Mobile, Update,... ... [Ok, Sony] [Ard] [Why] [Huh] [REMINDER, FROM, O2, To] [This, Pound, NOW1, Only, BT] [Will] [Pity, So] [The] [Rofl, Its] remove_tag Go until jurong point, crazy.. Available only ... Ok lar... Joking wif u oni... Free entry in 2 a wkly comp to win FA Cup fina... U dun say so early hor... U c already then say... Nah I don't think he goes to usf, he lives aro... FreeMsg Hey there darling it's been 3 week's n... Even my brother is not like to speak with me. ... As per your request 'Melle Melle (Oru Minnamin... WINNER!! As a valued network customer you have... Had your mobile 11 months or more? U R entitle... ... Ok lor... Sony ericsson salesman... I ask shuh... Ard 6 like dat lor. Why don't you wait 'til at least wednesday to ... Huh y lei... REMINDER FROM O2: To get 2.50 pounds free call... This is the 2nd time we have tried 2 contact u... Will \u00fc b going to esplanade fr home? Pity, * was in mood for that. So...any other s... The guy did some bitching but I acted like i'd... Rofl. Its true to its name mac_add [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [087187272008] [] [] [] [] ip_add [] [] [] [] [] [] [] [] [] [] ... [] [] [] [] [] [] [] [] [] [] subword 2 1 1 0 1 0 0 0 1 2 ... 3 0 1 0 0 3 0 1 0 0 lat_lon False False False False False False False False False False ... False False False False False False False False False False find_domain [] [] [] [] [] [1.50] [] [] [] [] ... [] [] [] [] [2.50] [] [] [] [] [] 27 rows \u00d7 5574 columns","title":"Valid latitude &amp; longitude"},{"location":"user_guide/nlp/boundary/","text":"boundary boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral'","title":"Boundary"},{"location":"user_guide/nlp/boundary/#boundary","text":"boundary(text) Extracting word with boundary Parameters text: str Text selected to apply transformation Examples: sentence=\"Most tweets are neutral in twitter\" boundary(sentence) >>> 'neutral'","title":"boundary"},{"location":"user_guide/nlp/find_at/","text":"find_at find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David'","title":"Find at"},{"location":"user_guide/nlp/find_at/#find_at","text":"find_at(text) @ - Used to mention someone in tweets Parameters text: str Text selected to apply transformation Examples: sentence=\"@David,can you help me out\" find_at(sentence) >>> 'David'","title":"find_at"},{"location":"user_guide/nlp/find_capital/","text":"find_capital find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God']","title":"Find capital"},{"location":"user_guide/nlp/find_capital/#find_capital","text":"find_capital(text) Extract words starting with capital letter. Some words like names,place or universal object are usually mentioned in a text starting with CAPS. Parameters text: str Text selected to apply transformation. Examples: sentence=\"World is affected by corona crisis. No one other than God can save us from it\" find_capital(sentence) >>> ['World', 'No', 'God']","title":"find_capital"},{"location":"user_guide/nlp/find_dates/","text":"find_dates find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')]","title":"Find dates"},{"location":"user_guide/nlp/find_dates/#find_dates","text":"find_dates(text) Find Dates. mm-dd-yyyy format Parameters text: str Text selected to apply transformation Examples: sentence=\"Todays date is 04/28/2020 for format mm/dd/yyyy, not 28/04/2020\" find_dates(sentence) >>> [('04', '28', '2020')]","title":"find_dates"},{"location":"user_guide/nlp/find_dollar/","text":"find_dollar find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"Find dollar"},{"location":"user_guide/nlp/find_dollar/#find_dollar","text":"find_dollar(text, symbol='$') Find prices in text Parameters text: str Text selected to apply transformation symbol: str Coin symbol Examples: sentence=\"this shirt costs $20.56\" find_dollar(sentence) >>> '$20.56'","title":"find_dollar"},{"location":"user_guide/nlp/find_domain/","text":"find_domain find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org']","title":"Find domain"},{"location":"user_guide/nlp/find_domain/#find_domain","text":"find_domain(string) Search domains in the text. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: sentence=\"WHO provides valid information about covid in their site who.int. UNICEF supports disadvantageous childrens. know more in unicef.org\" find_domain(sentence) >>> ['who.int', 'unicef.org']","title":"find_domain"},{"location":"user_guide/nlp/find_email/","text":"find_email find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com'","title":"Find email"},{"location":"user_guide/nlp/find_email/#find_email","text":"find_email(text) Extract email from text. Parameters text: str Text selected to apply transformation Examples: sentence=\"My gmail is abc99@gmail.com\" find_email(sentence) >>> 'abc99@gmail.com'","title":"find_email"},{"location":"user_guide/nlp/find_emoji/","text":"find_emoji find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ()\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes']","title":"Find emoji"},{"location":"user_guide/nlp/find_emoji/#find_emoji","text":"find_emoji(text) Find and convert emoji to text. Parameters text: str Text selected to apply transformation Examples: sentence=\"I love () very much ()\" find_emoji(sentence) >>> ['soccer_ball', 'beaming_face_with_smiling_eyes']","title":"find_emoji"},{"location":"user_guide/nlp/find_hash/","text":"find_hash find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona'","title":"Find hash"},{"location":"user_guide/nlp/find_hash/#find_hash","text":"find_hash(text) This value is especially to denote trends in twitter. Parameters text: str Text selected to apply transformation Examples: sentence=\"#Corona is trending now in the world\" find_hash(sentence) >>> 'Corona'","title":"find_hash"},{"location":"user_guide/nlp/find_nonalp/","text":"find_nonalp find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')']","title":"Find nonalp"},{"location":"user_guide/nlp/find_nonalp/#find_nonalp","text":"find_nonalp(text) Extract Non Alphanumeric characters. Parameters text: str Text selected to apply transformation Examples: sentence=\"Twitter has lots of @ and # in posts.(general tweet)\" find_nonalp(sentence) >>> ['@', '#', '.', '(', ')']","title":"find_nonalp"},{"location":"user_guide/nlp/find_number/","text":"find_number find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047'","title":"Find number"},{"location":"user_guide/nlp/find_number/#find_number","text":"find_number(text) Pick only number from sentence Parameters text: str Text selected to apply transformation Examples: sentence=\"2833047 people are affected by corona now\" find_number(sentence) >>> '2833047'","title":"find_number"},{"location":"user_guide/nlp/find_phone_number/","text":"find_phone_number find_phone_number(text) Indian Mobile numbers have ten digit.I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"9998887776 is a phone number of Mark from 210,North Avenue\") >>> '9998887776'","title":"Find phone number"},{"location":"user_guide/nlp/find_phone_number/#find_phone_number","text":"find_phone_number(text) Indian Mobile numbers have ten digit.I will write that pattern below. Parameters text: str Text selected to apply transformation Examples: find_phone_number(\"9998887776 is a phone number of Mark from 210,North Avenue\") >>> '9998887776'","title":"find_phone_number"},{"location":"user_guide/nlp/find_punct/","text":"find_punct find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')']","title":"Find punct"},{"location":"user_guide/nlp/find_punct/#find_punct","text":"find_punct(text) Retrieve punctuations from sentence. Parameters text: str Text selected to apply transformation Examples: example=\"Corona virus have kiled #24506 confirmed cases now.#Corona is un(tolerable)\" print(find_punct(example)) >>> ['#', '.', '#', '(', ')']","title":"find_punct"},{"location":"user_guide/nlp/find_url/","text":"find_url find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence)","title":"Find url"},{"location":"user_guide/nlp/find_url/#find_url","text":"find_url(string) Search URL in text. Parameters string: str Text selected to apply transformation Examples: sentence=\"I love spending time at https://www.kaggle.com/\" find_url(sentence)","title":"find_url"},{"location":"user_guide/nlp/find_year/","text":"find_year find_year(text) Extract year from 1940 till 2020. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947']","title":"Find year"},{"location":"user_guide/nlp/find_year/#find_year","text":"find_year(text) Extract year from 1940 till 2020. Parameters text: str Text selected to apply transformation Examples: sentence=\"India got independence on 1947.\" find_year(sentence) >>> ['1947']","title":"find_year"},{"location":"user_guide/nlp/ip_add/","text":"ip_add ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1']","title":"Ip add"},{"location":"user_guide/nlp/ip_add/#ip_add","text":"ip_add(string) Extract IP address from text. Parameters string: str Text selected to apply transformation. Examples: sentence=\"An example of ip address is 125.16.100.1\" ip_add(sentence) >>> ['125.16.100.1']","title":"ip_add"},{"location":"user_guide/nlp/lat_lon/","text":"lat_lon lat_lon(string) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude","title":"Lat lon"},{"location":"user_guide/nlp/lat_lon/#lat_lon","text":"lat_lon(string) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. Examples: lat_lon('28.6466772,76.8130649') lat_lon('2324.3244,3423.432423') >>> [28.6466772,76.8130649] is valid latitude & longitude >>> [2324.3244,3423.432423] is not a valid latitude & longitude","title":"lat_lon"},{"location":"user_guide/nlp/mac_add/","text":"mac_add mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc']","title":"Mac add"},{"location":"user_guide/nlp/mac_add/#mac_add","text":"mac_add(string) Extract Mac address from text. https://stackoverflow.com/questions/26891833/python-regex-extract-mac-addresses-from-string/2689237 Parameters string: str Text selected to apply transformation. Examples: sentence=\"MAC ADDRESSES of this laptop - 00:24:17:b1:cc:cc . Other details will be mentioned\" mac_add(sentence) >>> ['00:24:17:b1:cc:cc']","title":"mac_add"},{"location":"user_guide/nlp/neg_look_ahead/","text":"neg_look_ahead neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love","title":"Neg look ahead"},{"location":"user_guide/nlp/neg_look_ahead/#neg_look_ahead","text":"neg_look_ahead(string, A, B) Negative look ahead will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is A(?!B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(2, 6) Matched word:love","title":"neg_look_ahead"},{"location":"user_guide/nlp/neg_look_behind/","text":"neg_look_behind neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp","title":"Neg look behind"},{"location":"user_guide/nlp/neg_look_behind/#neg_look_behind","text":"neg_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does not match against the forthcoming input. The syntax is \"A(?<!=B)\" where \"A\"is actual expression and \"B\" is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: neg_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that doesnt come after \"love\" >>> position:(26, 29) Matched word: nlp","title":"neg_look_behind"},{"location":"user_guide/nlp/ngrams_top/","text":"ngrams_top ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176","title":"Ngrams top"},{"location":"user_guide/nlp/ngrams_top/#ngrams_top","text":"ngrams_top(corpus, ngram_range, n=None, idiom='english') List the top n words in a vocabulary according to occurrence in a text corpus. Examples: ngrams_top(df['text'],(1,1),n=10) >>> text count >>> 0 just 2278 >>> 1 day 2115 >>> 2 good 1578 >>> 3 like 1353 >>> 4 http 1247 >>> 5 work 1150 >>> 6 today 1147 >>> 7 love 1145 >>> 8 going 1103 >>> 9 got 1085 ngrams_top(df['text'],(2,2),n=10) >>> text count >>> 0 mother day 358 >>> 1 twitpic com 334 >>> 2 http twitpic 332 >>> 3 mothers day 279 >>> 4 happy mother 275 >>> 5 just got 219 >>> 6 happy mother 199 >>> 7 http bit 180 >>> 8 bit ly 180 >>> 9 good morning 176","title":"ngrams_top"},{"location":"user_guide/nlp/num_great/","text":"num_great num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974'","title":"Num great"},{"location":"user_guide/nlp/num_great/#num_great","text":"num_great(text) Number greater than 930 Parameters text: str Text selected to apply transformation Examples: sentence=\"It is expected to be more than 935 corona death and 29974 observation cases across 29 states in india\" num_great(sentence) >>> '935 29974'","title":"num_great"},{"location":"user_guide/nlp/num_less/","text":"num_less num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920'","title":"Num less"},{"location":"user_guide/nlp/num_less/#num_less","text":"num_less(text) Number less than 930. Parameters text: str Text selected to apply transformation Examples: sentence=\"There are some countries where less than 920 cases exist with 1100 observations\" num_less(sentence) >>> '920'","title":"num_less"},{"location":"user_guide/nlp/only_words/","text":"only_words only_words(text) Only Numbers Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000'","title":"Only words"},{"location":"user_guide/nlp/only_words/#only_words","text":"only_words(text) Only Numbers Parameters text: str Text selected to apply transformation Examples: sentence=\"the world population has grown from 1650 million to 6000 million\" only_numbers(sentence) >>> '1650 6000'","title":"only_words"},{"location":"user_guide/nlp/open_txt/","text":"open_txt open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings.","title":"Open txt"},{"location":"user_guide/nlp/open_txt/#open_txt","text":"open_txt(filename, encoding='utf-8') Function to open a .txt and return list of phrases. Parameters filename: Path where the file is hosted. encoding: Unicode and text encodings.","title":"open_txt"},{"location":"user_guide/nlp/pick_only_key_sentence/","text":"pick_only_key_sentence pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid']","title":"Pick only key sentence"},{"location":"user_guide/nlp/pick_only_key_sentence/#pick_only_key_sentence","text":"pick_only_key_sentence(text, keyword) If we want to get all sentence with particular keyword. We can use below function. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"People are fighting with covid these days. Economy has fallen down.How will we survice covid\" pick_only_key_sentence(sentence,'covid') >>> ['People are fighting with covid these days', 'How will we survice covid']","title":"pick_only_key_sentence"},{"location":"user_guide/nlp/pick_unique_sentence/","text":"pick_unique_sentence pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors']","title":"Pick unique sentence"},{"location":"user_guide/nlp/pick_unique_sentence/#pick_unique_sentence","text":"pick_unique_sentence(text) Most webscrapped data contains duplicated sentence. This function could retrieve unique ones. Parameters text: str Text selected to apply transformation. keyword: str Word to search within the phrase. Examples: sentence=\"I thank doctors Doctors are working very hard in this pandemic situation I thank doctors\" pick_unique_sentence(sentence) >>> ['Doctors are working very hard in this pandemic situation', 'I thank doctors']","title":"pick_unique_sentence"},{"location":"user_guide/nlp/pos_look_ahead/","text":"pos_look_ahead pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love","title":"Pos look ahead"},{"location":"user_guide/nlp/pos_look_ahead/#pos_look_ahead","text":"pos_look_ahead(string, A, B) Positive look ahead will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_ahead(\"I love kaggle. I love DL\",\"love\",\"DL\") >>> position:(17, 21) Matched word:love","title":"pos_look_ahead"},{"location":"user_guide/nlp/pos_look_behind/","text":"pos_look_behind pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp","title":"Pos look behind"},{"location":"user_guide/nlp/pos_look_behind/#pos_look_behind","text":"pos_look_behind(string, A, B) Positive look behind will succeed if passed non-consuming expression does match against the forthcoming input. The syntax is A(?<=B) where A is actual expression and B is non-consuming expression. Parameters string: str Text selected to apply transformation. A, B: str A is actual expression and B is non-consuming expression. Examples: pos_look_behind(\"i love nlp.everyone likes nlp\",\"love\",\"nlp\") # the word \"nlp\" that do come after \"love\" >>> position:(7, 10) Matched word: nlp","title":"pos_look_behind"},{"location":"user_guide/nlp/remove_emoji/","text":"","title":"Remove emoji"},{"location":"user_guide/nlp/remove_tag/","text":"remove_tag remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics'","title":"Remove tag"},{"location":"user_guide/nlp/remove_tag/#remove_tag","text":"remove_tag(string) Most of web scrapped data contains html tags. It can be removed from below re script Parameters text: str Text selected to apply transformation. Examples: sentence=\"Markdown sentences can use <br> for breaks and <i></i> for italics\" remove_tag(sentence) >>> 'Markdown sentences can use for breaks and for italics'","title":"remove_tag"},{"location":"user_guide/nlp/search_string/","text":"search_string search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True","title":"Search string"},{"location":"user_guide/nlp/search_string/#search_string","text":"search_string(text, key) Is the key word present in the sentence? Parameters text: str Text selected to apply transformation key: str Word to search within the phrase Examples: sentence=\"Happy Mothers day to all Moms\" search_string(sentence,'day') >>> True","title":"search_string"},{"location":"user_guide/nlp/subword/","text":"subword subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2","title":"Subword"},{"location":"user_guide/nlp/subword/#subword","text":"subword(string, sub) Extract number of subwords from sentences and words. Parameters string: str Text selected to apply transformation. sub: str subwords from sentences Examples: sentence = 'Fundamentalism and constructivism are important skills' subword(sentence,'ism') # change subword and try for others >>> 2","title":"subword"},{"location":"user_guide/nlp/unique_char/","text":"unique_char unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(rep,sentence) >>> 'hey this is long text son'","title":"Unique char"},{"location":"user_guide/nlp/unique_char/#unique_char","text":"unique_char(sentence) Retrieve punctuations from sentence. If you want to change match repetitive characters to n numbers, chage the return line in the rep function to grp[0:n]. Parameters sentence: str Text selected to apply transformation Examples: sentence=\"heyyy this is loong textttt sooon\" unique_char(rep,sentence) >>> 'hey this is long text son'","title":"unique_char"},{"location":"user_guide/preprocessing/DataAnalyst/","text":"Preprocessing Data analysis - EDA Dataset preprocessing examples from mlearner.preprocessing import DataAnalyst import numpy as np import pandas as pd from mlearner.preprocessing import DataAnalyst %matplotlib inline Load data in base_preprocess class filename = \"mlearner/data/data/breast-cancer-wisconsin-data.txt\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sample code number Clump Thickness Uniformity of Cell Size Uniformity of Cell Shape Marginal Adhesion Single Epithelial Cell Size Bare Nuclei Bland Chromatin Normal Nucleoli Mitoses Class 0 1000025 5 1 1 1 2 1 3 1 1 2 1 1002945 5 4 4 5 7 10 3 2 1 2 2 1015425 3 1 1 1 2 2 3 1 1 2 3 1016277 6 8 8 1 3 4 3 7 1 2 4 1017023 4 1 1 3 2 1 3 1 1 2 dataset.dtypes() Sample code number int64 Clump Thickness int64 Uniformity of Cell Size int64 Uniformity of Cell Shape int64 Marginal Adhesion int64 Single Epithelial Cell Size int64 Bare Nuclei object Bland Chromatin int64 Normal Nucleoli int64 Mitoses int64 Class int64 dtype: object target = [\"Class\"] Method: boxplot dataset.boxplot(features=[\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: dispersion_categoria dataset.dispersion_categoria(target=target) Method: sns_jointplot dataset.sns_jointplot(feature1=[\"Bland Chromatin\"], feature2=[\"Clump Thickness\"], target=target, categoria1 = [2]) Method: sns_pairplot dataset.sns_pairplot(features= [\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: distribution_targets dataset.distribution_targets(target=target) Method: corr_matrix dataset.corr_matrix() Method: not_type_object API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataAnalyst/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataAnalyst/#data-analysis-eda","text":"Dataset preprocessing examples from mlearner.preprocessing import DataAnalyst import numpy as np import pandas as pd from mlearner.preprocessing import DataAnalyst %matplotlib inline Load data in base_preprocess class filename = \"mlearner/data/data/breast-cancer-wisconsin-data.txt\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sample code number Clump Thickness Uniformity of Cell Size Uniformity of Cell Shape Marginal Adhesion Single Epithelial Cell Size Bare Nuclei Bland Chromatin Normal Nucleoli Mitoses Class 0 1000025 5 1 1 1 2 1 3 1 1 2 1 1002945 5 4 4 5 7 10 3 2 1 2 2 1015425 3 1 1 1 2 2 3 1 1 2 3 1016277 6 8 8 1 3 4 3 7 1 2 4 1017023 4 1 1 3 2 1 3 1 1 2 dataset.dtypes() Sample code number int64 Clump Thickness int64 Uniformity of Cell Size int64 Uniformity of Cell Shape int64 Marginal Adhesion int64 Single Epithelial Cell Size int64 Bare Nuclei object Bland Chromatin int64 Normal Nucleoli int64 Mitoses int64 Class int64 dtype: object target = [\"Class\"] Method: boxplot dataset.boxplot(features=[\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: dispersion_categoria dataset.dispersion_categoria(target=target) Method: sns_jointplot dataset.sns_jointplot(feature1=[\"Bland Chromatin\"], feature2=[\"Clump Thickness\"], target=target, categoria1 = [2]) Method: sns_pairplot dataset.sns_pairplot(features= [\"Bland Chromatin\", \"Clump Thickness\"], target=target) Method: distribution_targets dataset.distribution_targets(target=target) Method: corr_matrix dataset.corr_matrix() Method: not_type_object","title":"Data analysis - EDA"},{"location":"user_guide/preprocessing/DataAnalyst/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/","text":"Preprocessing Data Cleaner - Transformers Dataset preprocessing examples import numpy as np import pandas as pd from mlearner.preprocessing import minmax_scaling, MeanCenterer, FeatureDropper from mlearner.preprocessing import FillNaTransformer_median, FillNaTransformer_mean from mlearner.preprocessing import FillNaTransformer_idmax, FillNaTransformer_any from mlearner.preprocessing import FillNaTransformer_all, FillNaTransformer_value from mlearner.preprocessing import FillNaTransformer_backward, FillNaTransformer_forward from mlearner.preprocessing import FixSkewness, OneHotEncoder, DropOutliers, ReplaceTransformer from mlearner.preprocessing import ReplaceMulticlass, ExtractCategories, DataAnalyst, DataExploratory Load data in DataAnalyst class from mlearner.preprocessing import DataAnalyst filename = \"mlearner/data/data/titanic3.csv\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object dataset.data[\"age\"] = dataset.data[\"age\"].astype(np.float32) dataset.Xy_dataset(target=[\"survived\"]) Apply Transformers Drop Nan dataset.X.shape (1309, 13) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Transformer FillNaTransformer_median class from mlearner.preprocessing import FillNaTransformer_median mc = FillNaTransformer_median(columns=[\"body\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\pandas\\core\\frame.py:3140: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy self[k1] = value[k2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_mean class from mlearner.preprocessing import FillNaTransformer_mean mc = FillNaTransformer_median(columns=[\"age\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 embarked 2 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_idmax class from mlearner.preprocessing import FillNaTransformer_idmax mc = FillNaTransformer_idmax(columns=[\"embarked\"]).fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) C:\\Users\\AUTIS\\Google Drive\\10_MachineLearning_JS\\MachineLearning\\05_MLearner\\MLearner\\mlearner\\preprocessing\\replace_na.py:244: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy X[col] = X[col].fillna(X[col].value_counts().idxmax()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_value class from mlearner.preprocessing import FillNaTransformer_value mc = FillNaTransformer_value(columns=[\"home.dest\"]).fit(dataset.X, value=\"Empty\") dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_backward class from mlearner.preprocessing import FillNaTransformer_backward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_backward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 59 home.dest 27 boat 9 body 2 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 embarked 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_forward class from mlearner.preprocessing import FillNaTransformer_forward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_forward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 3 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_all class from mlearner.preprocessing import FillNaTransformer_all mc = FillNaTransformer_median().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_any class from mlearner.preprocessing import FillNaTransformer_any mc = FillNaTransformer_any().fit(dataset.X) AUX = mc.transform(dataset.X) dataset.missing_values(AUX) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Extract targets Transformer ExtractCategories class from mlearner.preprocessing import ExtractCategories Extracting subsets of data from a selected category mc = ExtractCategories(categories=[1], target=[\"survived\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) data_survived.shape (500, 14) Drop Features Transformer FeatureDropper class from mlearner.preprocessing import FeatureDropper mc = FeatureDropper(drop=[\"name\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape, data_survived.shape ((1309, 14), (1309, 13)) Drop Outliers Transformer DropOutliers class from mlearner.preprocessing import DropOutliers mc = DropOutliers(features=[\"age\", \"body\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape[0], data_survived.shape[0] C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal keep = (tmp_a >= first_edge) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal keep &= (tmp_a <= last_edge) (1309, 116) Scaling Transformer minmax_scaling class from mlearner.preprocessing import minmax_scaling dataset.X[\"age\"].max(), dataset.X[\"age\"].min() (80.0, 0.1667) new = minmax_scaling(dataset.X, columns=[\"age\"]) new.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.361169 1 0.009395 2 0.022964 3 0.373695 4 0.311064 Fix Skewnesss Transformer FixSkewness class from mlearner.preprocessing import FixSkewness import matplotlib.pyplot as plt mc = FixSkewness(columns=[\"age\"]).fit(dataset.X) plt.hist(dataset.data[\"age\"]) plt.hist(mc.transform(dataset.X)[\"age\"]) (array([ 76., 85., 336., 319., 208., 148., 71., 51., 11., 4.]), array([ 0.1667 , 8.15003, 16.13336, 24.11669, 32.10002, 40.08335, 48.06668, 56.05001, 64.03334, 72.01667, 80. ], dtype=float32), <a list of 10 Patch objects>) One-Hot-Encoder Transformer OneHotEncoder class from mlearner.preprocessing import OneHotEncoder mc = OneHotEncoder(columns=[\"pclass\"]).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[[\"pclass_1\", \"pclass_2\", \"pclass_3\"]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass_1 pclass_2 pclass_3 0 1 0 0 1 1 0 0 2 1 0 0 3 1 0 0 4 1 0 0 Replace map Transformer ReplaceTransformer class from mlearner.preprocessing import ReplaceTransformer mapping = {\"C\": 0, \"S\": 1, \"Q\": 3} mc = ReplaceTransformer(columns=[\"embarked\"], mapping=mapping).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[\"embarked\"].unique() array([1, 0, 3], dtype=int64) ### Replace Multiclass Transformer ReplaceMulticlass class from mlearner.preprocessing import ReplaceMulticlass dataset.X.columns.tolist() ['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] col = ['pclass', 'sex', 'sibsp', 'parch', 'ticket'] mc = ReplaceMulticlass(columns=col).fit(dataset.X) data_X = mc.transform(dataset.X) data_X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 0 Allen, Miss. Elisabeth Walton 0 29.0000 0 0 0 211.3375 B5 S 2 171.0 St Louis, MO 1 0 Allison, Master. Hudson Trevor 1 0.9167 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 2 0 Allison, Miss. Helen Loraine 0 2.0000 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 3 0 Allison, Mr. Hudson Joshua Creighton 1 30.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON 4 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) 0 25.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#data-cleaner-transformers","text":"Dataset preprocessing examples import numpy as np import pandas as pd from mlearner.preprocessing import minmax_scaling, MeanCenterer, FeatureDropper from mlearner.preprocessing import FillNaTransformer_median, FillNaTransformer_mean from mlearner.preprocessing import FillNaTransformer_idmax, FillNaTransformer_any from mlearner.preprocessing import FillNaTransformer_all, FillNaTransformer_value from mlearner.preprocessing import FillNaTransformer_backward, FillNaTransformer_forward from mlearner.preprocessing import FixSkewness, OneHotEncoder, DropOutliers, ReplaceTransformer from mlearner.preprocessing import ReplaceMulticlass, ExtractCategories, DataAnalyst, DataExploratory Load data in DataAnalyst class from mlearner.preprocessing import DataAnalyst filename = \"mlearner/data/data/titanic3.csv\" dataset = DataAnalyst.load_data(filename, sep=\",\") dataset.data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object dataset.data[\"age\"] = dataset.data[\"age\"].astype(np.float32) dataset.Xy_dataset(target=[\"survived\"])","title":"Data Cleaner - Transformers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#apply-transformers","text":"","title":"Apply Transformers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-nan","text":"dataset.X.shape (1309, 13) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Transformer FillNaTransformer_median class from mlearner.preprocessing import FillNaTransformer_median mc = FillNaTransformer_median(columns=[\"body\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\pandas\\core\\frame.py:3140: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy self[k1] = value[k2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_mean class from mlearner.preprocessing import FillNaTransformer_mean mc = FillNaTransformer_median(columns=[\"age\"]).fit(dataset.X) dataset.missing_values(mc.transform(dataset.X)) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 embarked 2 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_idmax class from mlearner.preprocessing import FillNaTransformer_idmax mc = FillNaTransformer_idmax(columns=[\"embarked\"]).fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) C:\\Users\\AUTIS\\Google Drive\\10_MachineLearning_JS\\MachineLearning\\05_MLearner\\MLearner\\mlearner\\preprocessing\\replace_na.py:244: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy X[col] = X[col].fillna(X[col].value_counts().idxmax()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 home.dest 564 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_value class from mlearner.preprocessing import FillNaTransformer_value mc = FillNaTransformer_value(columns=[\"home.dest\"]).fit(dataset.X, value=\"Empty\") dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 1014 boat 823 fare 1 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 embarked 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_backward class from mlearner.preprocessing import FillNaTransformer_backward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_backward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total cabin 59 home.dest 27 boat 9 body 2 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 embarked 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_forward class from mlearner.preprocessing import FillNaTransformer_forward dataset.Xy_dataset(target=[\"survived\"]) mc = FillNaTransformer_forward().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 3 pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_all class from mlearner.preprocessing import FillNaTransformer_all mc = FillNaTransformer_median().fit(dataset.X) dataset.X = mc.transform(dataset.X) dataset.missing_values(dataset.X) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13) Transformer FillNaTransformer_any class from mlearner.preprocessing import FillNaTransformer_any mc = FillNaTransformer_any().fit(dataset.X) AUX = mc.transform(dataset.X) dataset.missing_values(AUX) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total pclass 0 name 0 sex 0 age 0 sibsp 0 parch 0 ticket 0 fare 0 cabin 0 embarked 0 boat 0 body 0 home.dest 0 dataset.X.shape (1309, 13)","title":"Drop Nan"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#extract-targets","text":"Transformer ExtractCategories class from mlearner.preprocessing import ExtractCategories Extracting subsets of data from a selected category mc = ExtractCategories(categories=[1], target=[\"survived\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) data_survived.shape (500, 14)","title":"Extract targets"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-features","text":"Transformer FeatureDropper class from mlearner.preprocessing import FeatureDropper mc = FeatureDropper(drop=[\"name\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape, data_survived.shape ((1309, 14), (1309, 13))","title":"Drop Features"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#drop-outliers","text":"Transformer DropOutliers class from mlearner.preprocessing import DropOutliers mc = DropOutliers(features=[\"age\", \"body\"]).fit(dataset.data) data_survived = mc.transform(dataset.data) dataset.data.shape[0], data_survived.shape[0] C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:839: RuntimeWarning: invalid value encountered in greater_equal keep = (tmp_a >= first_edge) C:\\Users\\AUTIS\\Anaconda3\\envs\\Tensorflow 2.0\\lib\\site-packages\\numpy\\lib\\histograms.py:840: RuntimeWarning: invalid value encountered in less_equal keep &= (tmp_a <= last_edge) (1309, 116)","title":"Drop Outliers"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#scaling","text":"Transformer minmax_scaling class from mlearner.preprocessing import minmax_scaling dataset.X[\"age\"].max(), dataset.X[\"age\"].min() (80.0, 0.1667) new = minmax_scaling(dataset.X, columns=[\"age\"]) new.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age 0 0.361169 1 0.009395 2 0.022964 3 0.373695 4 0.311064","title":"Scaling"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#fix-skewnesss","text":"Transformer FixSkewness class from mlearner.preprocessing import FixSkewness import matplotlib.pyplot as plt mc = FixSkewness(columns=[\"age\"]).fit(dataset.X) plt.hist(dataset.data[\"age\"]) plt.hist(mc.transform(dataset.X)[\"age\"]) (array([ 76., 85., 336., 319., 208., 148., 71., 51., 11., 4.]), array([ 0.1667 , 8.15003, 16.13336, 24.11669, 32.10002, 40.08335, 48.06668, 56.05001, 64.03334, 72.01667, 80. ], dtype=float32), <a list of 10 Patch objects>)","title":"Fix Skewnesss"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#one-hot-encoder","text":"Transformer OneHotEncoder class from mlearner.preprocessing import OneHotEncoder mc = OneHotEncoder(columns=[\"pclass\"]).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[[\"pclass_1\", \"pclass_2\", \"pclass_3\"]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass_1 pclass_2 pclass_3 0 1 0 0 1 1 0 0 2 1 0 0 3 1 0 0 4 1 0 0","title":"One-Hot-Encoder"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#replace-map","text":"Transformer ReplaceTransformer class from mlearner.preprocessing import ReplaceTransformer mapping = {\"C\": 0, \"S\": 1, \"Q\": 3} mc = ReplaceTransformer(columns=[\"embarked\"], mapping=mapping).fit(dataset.X) data_X = mc.transform(dataset.X) data_X[\"embarked\"].unique() array([1, 0, 3], dtype=int64) ### Replace Multiclass Transformer ReplaceMulticlass class from mlearner.preprocessing import ReplaceMulticlass dataset.X.columns.tolist() ['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] col = ['pclass', 'sex', 'sibsp', 'parch', 'ticket'] mc = ReplaceMulticlass(columns=col).fit(dataset.X) data_X = mc.transform(dataset.X) data_X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 0 Allen, Miss. Elisabeth Walton 0 29.0000 0 0 0 211.3375 B5 S 2 171.0 St Louis, MO 1 0 Allison, Master. Hudson Trevor 1 0.9167 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 2 0 Allison, Miss. Helen Loraine 0 2.0000 1 1 1 151.5500 C22 C26 S 11 171.0 Montreal, PQ / Chesterville, ON 3 0 Allison, Mr. Hudson Joshua Creighton 1 30.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON 4 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) 0 25.0000 1 1 1 151.5500 C22 C26 S 11 135.0 Montreal, PQ / Chesterville, ON","title":"Replace map"},{"location":"user_guide/preprocessing/DataCleaner%20-%20Transformers/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DataCleaner/","text":"DataCleaner DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/ Methods categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"DataCleaner"},{"location":"user_guide/preprocessing/DataCleaner/#datacleaner","text":"DataCleaner(data) Class to preprocessed object for data cleaning. Attributes data: pd.DataFrame of Dataset Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DataCleaner/","title":"DataCleaner"},{"location":"user_guide/preprocessing/DataCleaner/#methods","text":"categorical_vs_numerical() None dtypes() retorno del tipo de datos por columna isNull() None load_data(filename, sep=';', decimal=',', params) Loading a dataset from a csv file. Parameters filename: str, path object or file-like object Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: file://localhost/path/to/table.csv . If you want to pass in a path object, pandas accepts any os.PathLike. By file-like object, we refer to objects with a read() method, such as a file handler (e.g. via builtin open function) or StringIO. seps: str Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python's builtin sniffer tool, csv.Sniffer. delimiter: str, default None Alias for sep. Attributes n: lenght of dataset. start: start iterator. end: end iterator. num: current iterator. Returns data: Pandas DataFrame, [n_samples, n_classes] Dataframe from dataset. Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/load/DataLoad/ load_dataframe(data) None missing_values() Numero de valores vacios en el dataframe. not_type_object() Deteccion de de categorias con type \"object\" reset() None type_object() Deteccion de de categorias con type \"object\" view_features() Mostrar features del dataframe","title":"Methods"},{"location":"user_guide/preprocessing/DataExploratory/","text":"Preprocessing Data exploratory - EDA Dataset preprocessing examples from mlearner.preprocessing import DataExploratory import numpy as np import pandas as pd from mlearner.preprocessing import DataExploratory Load data in DataExploratory class filename = \"mlearner/data/data/titanic3.csv\" dataset = DataExploratory.load_data(filename, sep=\",\") dataset.data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 5 1 1 Anderson, Mr. Harry male 48 0 0 19952 26.5500 E12 S 3 NaN New York, NY 6 1 1 Andrews, Miss. Kornelia Theodosia female 63 1 0 13502 77.9583 D7 S 10 NaN Hudson, NY 7 1 0 Andrews, Mr. Thomas Jr male 39 0 0 112050 0.0000 A36 S NaN NaN Belfast, NI 8 1 1 Appleton, Mrs. Edward Dale (Charlotte Lamson) female 53 2 0 11769 51.4792 C101 S D NaN Bayside, Queens, NY 9 1 0 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5042 NaN C NaN 22.0 Montevideo, Uruguay 10 1 0 Astor, Col. John Jacob male 47 1 0 PC 17757 227.5250 C62 C64 C NaN 124.0 New York, NY 11 1 1 Astor, Mrs. John Jacob (Madeleine Talmadge Force) female 18 1 0 PC 17757 227.5250 C62 C64 C 4 NaN New York, NY 12 1 1 Aubart, Mme. Leontine Pauline female 24 0 0 PC 17477 69.3000 B35 C 9 NaN Paris, France 13 1 1 Barber, Miss. Ellen \"Nellie\" female 26 0 0 19877 78.8500 NaN S 6 NaN NaN 14 1 1 Barkworth, Mr. Algernon Henry Wilson male 80 0 0 27042 30.0000 A23 S B NaN Hessle, Yorks 15 1 0 Baumann, Mr. John D male NaN 0 0 PC 17318 25.9250 NaN S NaN NaN New York, NY 16 1 0 Baxter, Mr. Quigg Edmond male 24 0 1 PC 17558 247.5208 B58 B60 C NaN NaN Montreal, PQ 17 1 1 Baxter, Mrs. James (Helene DeLaudeniere Chaput) female 50 0 1 PC 17558 247.5208 B58 B60 C 6 NaN Montreal, PQ 18 1 1 Bazzani, Miss. Albina female 32 0 0 11813 76.2917 D15 C 8 NaN NaN 19 1 0 Beattie, Mr. Thomson male 36 0 0 13050 75.2417 C6 C A NaN Winnipeg, MN 20 1 1 Beckwith, Mr. Richard Leonard male 37 1 1 11751 52.5542 D35 S 5 NaN New York, NY 21 1 1 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) female 47 1 1 11751 52.5542 D35 S 5 NaN New York, NY 22 1 1 Behr, Mr. Karl Howell male 26 0 0 111369 30.0000 C148 C 5 NaN New York, NY 23 1 1 Bidois, Miss. Rosalie female 42 0 0 PC 17757 227.5250 NaN C 4 NaN NaN 24 1 1 Bird, Miss. Ellen female 29 0 0 PC 17483 221.7792 C97 S 8 NaN NaN 25 1 0 Birnbaum, Mr. Jakob male 25 0 0 13905 26.0000 NaN C NaN 148.0 San Francisco, CA 26 1 1 Bishop, Mr. Dickinson H male 25 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 27 1 1 Bishop, Mrs. Dickinson H (Helen Walton) female 19 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 28 1 1 Bissette, Miss. Amelia female 35 0 0 PC 17760 135.6333 C99 S 8 NaN NaN 29 1 1 Bjornstrom-Steffansson, Mr. Mauritz Hakan male 28 0 0 110564 26.5500 C52 S D NaN Stockholm, Sweden / Washington, DC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1279 3 0 Vestrom, Miss. Hulda Amanda Adolfina female 14 0 0 350406 7.8542 NaN S NaN NaN NaN 1280 3 0 Vovk, Mr. Janko male 22 0 0 349252 7.8958 NaN S NaN NaN NaN 1281 3 0 Waelens, Mr. Achille male 22 0 0 345767 9.0000 NaN S NaN NaN Antwerp, Belgium / Stanton, OH 1282 3 0 Ware, Mr. Frederick male NaN 0 0 359309 8.0500 NaN S NaN NaN NaN 1283 3 0 Warren, Mr. Charles William male NaN 0 0 C.A. 49867 7.5500 NaN S NaN NaN NaN 1284 3 0 Webber, Mr. James male NaN 0 0 SOTON/OQ 3101316 8.0500 NaN S NaN NaN NaN 1285 3 0 Wenzel, Mr. Linhart male 32.5 0 0 345775 9.5000 NaN S NaN 298.0 NaN 1286 3 1 Whabee, Mrs. George Joseph (Shawneene Abi-Saab) female 38 0 0 2688 7.2292 NaN C C NaN NaN 1287 3 0 Widegren, Mr. Carl/Charles Peter male 51 0 0 347064 7.7500 NaN S NaN NaN NaN 1288 3 0 Wiklund, Mr. Jakob Alfred male 18 1 0 3101267 6.4958 NaN S NaN 314.0 NaN 1289 3 0 Wiklund, Mr. Karl Johan male 21 1 0 3101266 6.4958 NaN S NaN NaN NaN 1290 3 1 Wilkes, Mrs. James (Ellen Needs) female 47 1 0 363272 7.0000 NaN S NaN NaN NaN 1291 3 0 Willer, Mr. Aaron (\"Abi Weller\") male NaN 0 0 3410 8.7125 NaN S NaN NaN NaN 1292 3 0 Willey, Mr. Edward male NaN 0 0 S.O./P.P. 751 7.5500 NaN S NaN NaN NaN 1293 3 0 Williams, Mr. Howard Hugh \"Harry\" male NaN 0 0 A/5 2466 8.0500 NaN S NaN NaN NaN 1294 3 0 Williams, Mr. Leslie male 28.5 0 0 54636 16.1000 NaN S NaN 14.0 NaN 1295 3 0 Windelov, Mr. Einar male 21 0 0 SOTON/OQ 3101317 7.2500 NaN S NaN NaN NaN 1296 3 0 Wirz, Mr. Albert male 27 0 0 315154 8.6625 NaN S NaN 131.0 NaN 1297 3 0 Wiseman, Mr. Phillippe male NaN 0 0 A/4. 34244 7.2500 NaN S NaN NaN NaN 1298 3 0 Wittevrongel, Mr. Camille male 36 0 0 345771 9.5000 NaN S NaN NaN NaN 1299 3 0 Yasbeck, Mr. Antoni male 27 1 0 2659 14.4542 NaN C C NaN NaN 1300 3 1 Yasbeck, Mrs. Antoni (Selini Alexander) female 15 1 0 2659 14.4542 NaN C NaN NaN NaN 1301 3 0 Youseff, Mr. Gerious male 45.5 0 0 2628 7.2250 NaN C NaN 312.0 NaN 1302 3 0 Yousif, Mr. Wazli male NaN 0 0 2647 7.2250 NaN C NaN NaN NaN 1303 3 0 Yousseff, Mr. Gerious male NaN 0 0 2627 14.4583 NaN C NaN NaN NaN 1304 3 0 Zabour, Miss. Hileni female 14.5 1 0 2665 14.4542 NaN C NaN 328.0 NaN 1305 3 0 Zabour, Miss. Thamine female NaN 1 0 2665 14.4542 NaN C NaN NaN NaN 1306 3 0 Zakarian, Mr. Mapriededer male 26.5 0 0 2656 7.2250 NaN C NaN 304.0 NaN 1307 3 0 Zakarian, Mr. Ortin male 27 0 0 2670 7.2250 NaN C NaN NaN NaN 1308 3 0 Zimmerman, Mr. Leo male 29 0 0 315082 7.8750 NaN S NaN NaN NaN 1309 rows \u00d7 14 columns Method: dtypes dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object Method: missing_values dataset.missing_values() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 survived 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Method: isNull dataset.isNull() Cuidado que existen valores nulos True Method: view_features dataset.view_features() ['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] Method: categorical_vs_numerical _, _ = dataset.categorical_vs_numerical() Number of categorical features: 9 Number of numerical features: 5 Method: type_object dataset.type_object() ['name', 'sex', 'age', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'home.dest'] Method: not_type_object dataset.not_type_object() ['pclass', 'survived', 'sibsp', 'parch', 'body'] API","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataExploratory/#preprocessing","text":"","title":"Preprocessing"},{"location":"user_guide/preprocessing/DataExploratory/#data-exploratory-eda","text":"Dataset preprocessing examples from mlearner.preprocessing import DataExploratory import numpy as np import pandas as pd from mlearner.preprocessing import DataExploratory Load data in DataExploratory class filename = \"mlearner/data/data/titanic3.csv\" dataset = DataExploratory.load_data(filename, sep=\",\") dataset.data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pclass survived name sex age sibsp parch ticket fare cabin embarked boat body home.dest 0 1 1 Allen, Miss. Elisabeth Walton female 29 0 0 24160 211.3375 B5 S 2 NaN St Louis, MO 1 1 1 Allison, Master. Hudson Trevor male 0.9167 1 2 113781 151.5500 C22 C26 S 11 NaN Montreal, PQ / Chesterville, ON 2 1 0 Allison, Miss. Helen Loraine female 2 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 3 1 0 Allison, Mr. Hudson Joshua Creighton male 30 1 2 113781 151.5500 C22 C26 S NaN 135.0 Montreal, PQ / Chesterville, ON 4 1 0 Allison, Mrs. Hudson J C (Bessie Waldo Daniels) female 25 1 2 113781 151.5500 C22 C26 S NaN NaN Montreal, PQ / Chesterville, ON 5 1 1 Anderson, Mr. Harry male 48 0 0 19952 26.5500 E12 S 3 NaN New York, NY 6 1 1 Andrews, Miss. Kornelia Theodosia female 63 1 0 13502 77.9583 D7 S 10 NaN Hudson, NY 7 1 0 Andrews, Mr. Thomas Jr male 39 0 0 112050 0.0000 A36 S NaN NaN Belfast, NI 8 1 1 Appleton, Mrs. Edward Dale (Charlotte Lamson) female 53 2 0 11769 51.4792 C101 S D NaN Bayside, Queens, NY 9 1 0 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5042 NaN C NaN 22.0 Montevideo, Uruguay 10 1 0 Astor, Col. John Jacob male 47 1 0 PC 17757 227.5250 C62 C64 C NaN 124.0 New York, NY 11 1 1 Astor, Mrs. John Jacob (Madeleine Talmadge Force) female 18 1 0 PC 17757 227.5250 C62 C64 C 4 NaN New York, NY 12 1 1 Aubart, Mme. Leontine Pauline female 24 0 0 PC 17477 69.3000 B35 C 9 NaN Paris, France 13 1 1 Barber, Miss. Ellen \"Nellie\" female 26 0 0 19877 78.8500 NaN S 6 NaN NaN 14 1 1 Barkworth, Mr. Algernon Henry Wilson male 80 0 0 27042 30.0000 A23 S B NaN Hessle, Yorks 15 1 0 Baumann, Mr. John D male NaN 0 0 PC 17318 25.9250 NaN S NaN NaN New York, NY 16 1 0 Baxter, Mr. Quigg Edmond male 24 0 1 PC 17558 247.5208 B58 B60 C NaN NaN Montreal, PQ 17 1 1 Baxter, Mrs. James (Helene DeLaudeniere Chaput) female 50 0 1 PC 17558 247.5208 B58 B60 C 6 NaN Montreal, PQ 18 1 1 Bazzani, Miss. Albina female 32 0 0 11813 76.2917 D15 C 8 NaN NaN 19 1 0 Beattie, Mr. Thomson male 36 0 0 13050 75.2417 C6 C A NaN Winnipeg, MN 20 1 1 Beckwith, Mr. Richard Leonard male 37 1 1 11751 52.5542 D35 S 5 NaN New York, NY 21 1 1 Beckwith, Mrs. Richard Leonard (Sallie Monypeny) female 47 1 1 11751 52.5542 D35 S 5 NaN New York, NY 22 1 1 Behr, Mr. Karl Howell male 26 0 0 111369 30.0000 C148 C 5 NaN New York, NY 23 1 1 Bidois, Miss. Rosalie female 42 0 0 PC 17757 227.5250 NaN C 4 NaN NaN 24 1 1 Bird, Miss. Ellen female 29 0 0 PC 17483 221.7792 C97 S 8 NaN NaN 25 1 0 Birnbaum, Mr. Jakob male 25 0 0 13905 26.0000 NaN C NaN 148.0 San Francisco, CA 26 1 1 Bishop, Mr. Dickinson H male 25 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 27 1 1 Bishop, Mrs. Dickinson H (Helen Walton) female 19 1 0 11967 91.0792 B49 C 7 NaN Dowagiac, MI 28 1 1 Bissette, Miss. Amelia female 35 0 0 PC 17760 135.6333 C99 S 8 NaN NaN 29 1 1 Bjornstrom-Steffansson, Mr. Mauritz Hakan male 28 0 0 110564 26.5500 C52 S D NaN Stockholm, Sweden / Washington, DC ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1279 3 0 Vestrom, Miss. Hulda Amanda Adolfina female 14 0 0 350406 7.8542 NaN S NaN NaN NaN 1280 3 0 Vovk, Mr. Janko male 22 0 0 349252 7.8958 NaN S NaN NaN NaN 1281 3 0 Waelens, Mr. Achille male 22 0 0 345767 9.0000 NaN S NaN NaN Antwerp, Belgium / Stanton, OH 1282 3 0 Ware, Mr. Frederick male NaN 0 0 359309 8.0500 NaN S NaN NaN NaN 1283 3 0 Warren, Mr. Charles William male NaN 0 0 C.A. 49867 7.5500 NaN S NaN NaN NaN 1284 3 0 Webber, Mr. James male NaN 0 0 SOTON/OQ 3101316 8.0500 NaN S NaN NaN NaN 1285 3 0 Wenzel, Mr. Linhart male 32.5 0 0 345775 9.5000 NaN S NaN 298.0 NaN 1286 3 1 Whabee, Mrs. George Joseph (Shawneene Abi-Saab) female 38 0 0 2688 7.2292 NaN C C NaN NaN 1287 3 0 Widegren, Mr. Carl/Charles Peter male 51 0 0 347064 7.7500 NaN S NaN NaN NaN 1288 3 0 Wiklund, Mr. Jakob Alfred male 18 1 0 3101267 6.4958 NaN S NaN 314.0 NaN 1289 3 0 Wiklund, Mr. Karl Johan male 21 1 0 3101266 6.4958 NaN S NaN NaN NaN 1290 3 1 Wilkes, Mrs. James (Ellen Needs) female 47 1 0 363272 7.0000 NaN S NaN NaN NaN 1291 3 0 Willer, Mr. Aaron (\"Abi Weller\") male NaN 0 0 3410 8.7125 NaN S NaN NaN NaN 1292 3 0 Willey, Mr. Edward male NaN 0 0 S.O./P.P. 751 7.5500 NaN S NaN NaN NaN 1293 3 0 Williams, Mr. Howard Hugh \"Harry\" male NaN 0 0 A/5 2466 8.0500 NaN S NaN NaN NaN 1294 3 0 Williams, Mr. Leslie male 28.5 0 0 54636 16.1000 NaN S NaN 14.0 NaN 1295 3 0 Windelov, Mr. Einar male 21 0 0 SOTON/OQ 3101317 7.2500 NaN S NaN NaN NaN 1296 3 0 Wirz, Mr. Albert male 27 0 0 315154 8.6625 NaN S NaN 131.0 NaN 1297 3 0 Wiseman, Mr. Phillippe male NaN 0 0 A/4. 34244 7.2500 NaN S NaN NaN NaN 1298 3 0 Wittevrongel, Mr. Camille male 36 0 0 345771 9.5000 NaN S NaN NaN NaN 1299 3 0 Yasbeck, Mr. Antoni male 27 1 0 2659 14.4542 NaN C C NaN NaN 1300 3 1 Yasbeck, Mrs. Antoni (Selini Alexander) female 15 1 0 2659 14.4542 NaN C NaN NaN NaN 1301 3 0 Youseff, Mr. Gerious male 45.5 0 0 2628 7.2250 NaN C NaN 312.0 NaN 1302 3 0 Yousif, Mr. Wazli male NaN 0 0 2647 7.2250 NaN C NaN NaN NaN 1303 3 0 Yousseff, Mr. Gerious male NaN 0 0 2627 14.4583 NaN C NaN NaN NaN 1304 3 0 Zabour, Miss. Hileni female 14.5 1 0 2665 14.4542 NaN C NaN 328.0 NaN 1305 3 0 Zabour, Miss. Thamine female NaN 1 0 2665 14.4542 NaN C NaN NaN NaN 1306 3 0 Zakarian, Mr. Mapriededer male 26.5 0 0 2656 7.2250 NaN C NaN 304.0 NaN 1307 3 0 Zakarian, Mr. Ortin male 27 0 0 2670 7.2250 NaN C NaN NaN NaN 1308 3 0 Zimmerman, Mr. Leo male 29 0 0 315082 7.8750 NaN S NaN NaN NaN 1309 rows \u00d7 14 columns Method: dtypes dataset.dtypes() pclass int64 survived int64 name object sex object age object sibsp int64 parch int64 ticket object fare object cabin object embarked object boat object body float64 home.dest object dtype: object Method: missing_values dataset.missing_values() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } total body 1188 cabin 1014 boat 823 home.dest 564 age 263 embarked 2 fare 1 pclass 0 survived 0 name 0 sex 0 sibsp 0 parch 0 ticket 0 Method: isNull dataset.isNull() Cuidado que existen valores nulos True Method: view_features dataset.view_features() ['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'] Method: categorical_vs_numerical _, _ = dataset.categorical_vs_numerical() Number of categorical features: 9 Number of numerical features: 5 Method: type_object dataset.type_object() ['name', 'sex', 'age', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'home.dest'] Method: not_type_object dataset.not_type_object() ['pclass', 'survived', 'sibsp', 'parch', 'body']","title":"Data exploratory - EDA"},{"location":"user_guide/preprocessing/DataExploratory/#api","text":"","title":"API"},{"location":"user_guide/preprocessing/DropOutliers/","text":"DropOutliers DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"DropOutliers"},{"location":"user_guide/preprocessing/DropOutliers/#dropoutliers","text":"DropOutliers(features=[], display=False) Drop Outliers from dataframe Attributes features: list or tuple list of features to drop outliers [n_columns] display: boolean` Show histogram with changes made. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/DropOutliers/","title":"DropOutliers"},{"location":"user_guide/preprocessing/DropOutliers/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"user_guide/preprocessing/ExtractCategories/","text":"ExtractCategories ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ExtractCategories"},{"location":"user_guide/preprocessing/ExtractCategories/#extractcategories","text":"ExtractCategories(categories=None, target=None) This transformer filters the selected dataset categories. Attributes categories: list of categories that you want to keep. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ExtractCategories"},{"location":"user_guide/preprocessing/ExtractCategories/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make filters the selected dataset categories. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FeatureDropper/","text":"FeatureDropper FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/ Methods fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"FeatureDropper"},{"location":"user_guide/preprocessing/FeatureDropper/#featuredropper","text":"FeatureDropper(drop=[]) Column drop according to the selected feature. Attributes drop: list of features to drop [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FeatureDropper/","title":"FeatureDropper"},{"location":"user_guide/preprocessing/FeatureDropper/#methods","text":"fit(X, y=None, fit_params) Gets the columns that not drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X, fit_params) Features drop. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns dropped.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_all/","text":"FillNaTransformer_all FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer all"},{"location":"user_guide/preprocessing/FillNaTransformer_all/#fillnatransformer_all","text":"FillNaTransformer_all() This transformer delete row that there is all NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_all/","title":"FillNaTransformer_all"},{"location":"user_guide/preprocessing/FillNaTransformer_all/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_any/","text":"FillNaTransformer_any FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/ Methods fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer any"},{"location":"user_guide/preprocessing/FillNaTransformer_any/#fillnatransformer_any","text":"FillNaTransformer_any() This transformer delete row that there is some NaN. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_any/","title":"FillNaTransformer_any"},{"location":"user_guide/preprocessing/FillNaTransformer_any/#methods","text":"fit(X, y=None, fit_params) Not implemented. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) This transformer delete row that there is some NaN Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/","text":"FillNaTransformer_backward FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer backward"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/#fillnatransformer_backward","text":"FillNaTransformer_backward(columns=None) This transformer handles missing values closer backward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_backward/","title":"FillNaTransformer_backward"},{"location":"user_guide/preprocessing/FillNaTransformer_backward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/","text":"FillNaTransformer_forward FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer forward"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/#fillnatransformer_forward","text":"FillNaTransformer_forward(columns=None) This transformer handles missing values closer forward. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_forward/","title":"FillNaTransformer_forward"},{"location":"user_guide/preprocessing/FillNaTransformer_forward/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/","text":"FillNaTransformer_idmax FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer idmax"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/#fillnatransformer_idmax","text":"FillNaTransformer_idmax(columns=None) This transformer handles missing values for idmax. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_idmax/","title":"FillNaTransformer_idmax"},{"location":"user_guide/preprocessing/FillNaTransformer_idmax/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/","text":"FillNaTransformer_mean FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer mean"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/#fillnatransformer_mean","text":"FillNaTransformer_mean(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_mean/","title":"FillNaTransformer_mean"},{"location":"user_guide/preprocessing/FillNaTransformer_mean/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_median/","text":"FillNaTransformer_median FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer median"},{"location":"user_guide/preprocessing/FillNaTransformer_median/#fillnatransformer_median","text":"FillNaTransformer_median(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_median/","title":"FillNaTransformer_median"},{"location":"user_guide/preprocessing/FillNaTransformer_median/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FillNaTransformer_value/","text":"FillNaTransformer_value FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/ Methods fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"FillNaTransformer value"},{"location":"user_guide/preprocessing/FillNaTransformer_value/#fillnatransformer_value","text":"FillNaTransformer_value(columns=None) This transformer handles missing values. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FillNaTransformer_value/","title":"FillNaTransformer_value"},{"location":"user_guide/preprocessing/FillNaTransformer_value/#methods","text":"fit(X, y=None, value=None, fit_params) Gets the columns to make a replace missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) this transformer handles missing values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/FixSkewness/","text":"FixSkewness FixSkewness(columns=None) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/ Methods fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"FixSkewness"},{"location":"user_guide/preprocessing/FixSkewness/#fixskewness","text":"FixSkewness(columns=None) This transformer applies log to skewed features. Attributes columns: npandas [n_columns] Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/FixSkewness/","title":"FixSkewness"},{"location":"user_guide/preprocessing/FixSkewness/#methods","text":"fit(X, y=None, fit_params) Selecting skewed columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"user_guide/preprocessing/MeanCenterer/","text":"MeanCenterer MeanCenterer() Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause Methods fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"MeanCenterer"},{"location":"user_guide/preprocessing/MeanCenterer/#meancenterer","text":"MeanCenterer() Column centering of pandas Dataframe. Attributes col_means: numpy.ndarray [n_columns] or pandas [n_columns] mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/MeanCenterer/ adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/mean_centering.py Author: Sebastian Raschka License: BSD 3 clause","title":"MeanCenterer"},{"location":"user_guide/preprocessing/MeanCenterer/#methods","text":"fit(X, y=None) Gets the column means for mean centering. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Centers a pandas. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns centered.","title":"Methods"},{"location":"user_guide/preprocessing/OneHotEncoder/","text":"OneHotEncoder OneHotEncoder(columns=None, numerical=[]) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/ Methods fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"OneHotEncoder"},{"location":"user_guide/preprocessing/OneHotEncoder/#onehotencoder","text":"OneHotEncoder(columns=None, numerical=[]) This transformer applies One-Hot-Encoder to features. Attributes numerical: pandas [n_columns]. numerical columns to be treated as categorical. columns: pandas [n_columns]. columns to use (if None then all categorical variables are included). Examples For usage examples, please see: https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/OneHotEncoder/","title":"OneHotEncoder"},{"location":"user_guide/preprocessing/OneHotEncoder/#methods","text":"fit(X, y=None, fit_params) Selecting OneHotEncoder columns from the dataset. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Trransformer applies log to skewed features. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {DAtaframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns encoder.","title":"Methods"},{"location":"user_guide/preprocessing/ReplaceMulticlass/","text":"ReplaceMulticlass ReplaceMulticlass(columns=None, mapping=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceMulticlass"},{"location":"user_guide/preprocessing/ReplaceMulticlass/#replacemulticlass","text":"ReplaceMulticlass(columns=None, mapping=None) This transformer replace some categorical values with others. Attributes columns: list of columns to transformer [n_columns] Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceMulticlass/","title":"ReplaceMulticlass"},{"location":"user_guide/preprocessing/ReplaceMulticlass/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace to categorical values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/ReplaceTransformer/","text":"ReplaceTransformer ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/ Methods fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"ReplaceTransformer"},{"location":"user_guide/preprocessing/ReplaceTransformer/#replacetransformer","text":"ReplaceTransformer(columns=None, mapping=None) This transformer replace some values with others. Attributes columns: list of columns to transformer [n_columns] mapping: dict`, for example: mapping = {\"yes\": 1, \"no\": 0} Examples For usage examples, please see https://jaisenbe58r.github.io/MLearner/user_guide/preprocessing/ReplaceTransformer/","title":"ReplaceTransformer"},{"location":"user_guide/preprocessing/ReplaceTransformer/#methods","text":"fit(X, y=None, fit_params) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X. Parameters X : numpy array of shape [n_samples, n_features] Training set. y : numpy array of shape [n_samples] Target values. Returns X_new : numpy array of shape [n_samples, n_features_new] Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self transform(X) Gets the columns to make a replace values. Parameters X : {Dataframe}, shape = [n_samples, n_features] Dataframe of samples, where n_samples is the number of samples and n_features is the number of features. Returns X_transform : {Dataframe}, shape = [n_samples, n_features] A copy of the input Dataframe with the columns replaced.","title":"Methods"},{"location":"user_guide/preprocessing/minmax_scaling/","text":"minmax_scaling minmax_scaling(array, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"Minmax scaling"},{"location":"user_guide/preprocessing/minmax_scaling/#minmax_scaling","text":"minmax_scaling(array, columns, min_val=0, max_val=1) In max scaling of pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see [http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.](http://jaisenbe58r.github.io/mlearner/user_guide/preprocessing/minmax_scaling/.) adapted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/preprocessing/scaling.py Author: Sebastian Raschka <sebastianraschka.com> License: BSD 3 clause","title":"minmax_scaling"},{"location":"user_guide/training/Training/","text":"Training Training(model, random_state=99) None Methods FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Training"},{"location":"user_guide/training/Training/#training","text":"Training(model, random_state=99) None","title":"Training"},{"location":"user_guide/training/Training/#methods","text":"FineTune(model, X, y, params, refit='Accuracy', cv=3, verbose=0, randomized=True, n_iter=100, mute=False) Tcnica de Ajuste fino de hiperparametros. Model: Modelo a Optimizar. params: diccionario de parametros con el grid. scoring: Metricas. scoring = {'AUC': 'roc_auc', 'Accuracy': acc_scorer} * Anotador de metricas: acc_score = make_scorer(accuracy_score, mean_squared_error) refit: Metrica de importancia para optimizar el modelo'Accuracy' GridSearchCV_Evaluating(model, param, max_param, min_score=0.5) https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py KFold_CrossValidation(model, X, y, n_splits=10, ROC=False, shuffle=True, mute=False, logdir_report='', display=True, save_image=True, verbose=0) Validacion cruzada respecto a \"n_plits\" del KFolds. add_model(model, random_state=99) Incorporar modelo en la clase class_report(y_true, predictions, clases, logdir_report) Un informe de clasificacion se utiliza para medir la calidad de las predicciones de un algoritmo de clasificacion. Cuntas predicciones son verdaderas y cuntas son falsas. Ms especificamente, los Positivos verdaderos, los Positivos falsos, los negativos verdaderos y los negativos falsos se utilizan para predecir las metricas de un informe de clasificacion. El informe muestra las principales metricas de clasificacion de precision, recuperacion y puntaje f1 por clase. Las metricas se calculan utilizando verdaderos y falsos positivos, verdaderos y falsos negativos. Positivo y negativo en este caso son nombres genericos para las clases predichas. Hay cuatro formas de verificar si las predicciones son correctas o incorrectas: TN / Verdadero negativo: cuando un caso fue negativo y se pronostico negativo TP / Verdadero Positivo: cuando un caso fue positivo y predicho positivo FN / Falso negativo: cuando un caso fue positivo pero predicho negativo FP / Falso Positivo: cuando un caso fue negativo pero predicho positivo La precision es la capacidad de un clasificador de no etiquetar una instancia positiva que en realidad es negativa. Para cada clase se define como la relacion de positivos verdaderos a la suma de positivos verdaderos y falsos. TP - Positivos verdaderos FP - Positivos falsos Precision: precision de las predicciones positivas. Precision = TP / (TP + FP) Recordar es la capacidad de un clasificador para encontrar todas las instancias positivas. Para cada clase se define como la relacion entre los verdaderos positivos y la suma de los verdaderos positivos y los falsos negativos. FN - Falsos negativos Recordar: fraccion de positivos identificados correctamente. Recuperacion = TP / (TP + FN) El puntaje F 1 es una media armonica ponderada de precision y recuperacion de modo que el mejor puntaje es 1.0 y el peor es 0.0. En terminos generales, los puntajes de F 1 son ms bajos que las medidas de precision, ya que incorporan precision y recuerdo en su clculo. Como regla general, el promedio ponderado de F 1 debe usarse para comparar modelos clasificadores, no la precision global. Puntuacion F1 = 2 * (recuperacion * precision) / (recuperacion + precision) compute_roc_auc(model, index, X, y) Computo para todas las pruebas de KFold confusion_matrix(y_true, y_pred) None create_ROC(lm, X_test, Y_test, targets=[0, 1], logdir_report='', display=True, save_image=True) Se crea la curva ROC de las predicciones de conjunto de test. Outputs: - df: Dataframe de los datos de metricas ROC. - auc: Area por debajo de la curfa ROC (efectividad de las predicciones). create_ROC_pro(fprs, tprs, X, y, targets=[0, 1], logdir_report='', display=True, save_image=True) Plot the Receiver Operating Characteristic from a list of true positive rates and false positive rates. eval_FineTune(X, y) https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/random_forest_explained/Improving%20Random%20Forest%20Part%202.ipynb eval_train(model, X, y, name='Performance') None evaluacion_rf_2features(data_eval, data_eval_target, targets=[0, 1], logdir_report='', display=True) Funcion que nos selecciona el thresholder ms optimo: Inputs: data_eval: Conjunto de evaluacion. probs: Matriz de probabilidades que viene dado por el \"lm.predict_proba\" del podelo de RL. thresholders: Intervalo de corte para determinar la categoria. viene dado por el \"metrics.roc_curve\" de sckitlearn. display: si queremos mostrar los datos graficados por pantalla. Outputs: th: Thresholder seleccionado. res: El accurancy con ese thresholder. df: Dataframe con el resultado del test. evaluate(model, X, y) Evalucion del modelo Fine-Tune features_important(X, y, logdir='', display=True, save_image=False) Explorar las features mas significativas get_model() None heatmap_params(parameters, metric='mean_test_Accuracy') parametres a relacionar: parameters = [\"n_estimators\", \"min_samples_split\"] plot_Histograma(predict, correct, incorrect, logdir_report, categorias=[0, 1], display=True, save_image=True) None plot_confusion_matrix(y_true, y_pred, classes, num_clases, logdir_report, normalize=False, title=None, cmap= , name='cm_normalizada') Una matriz de confusion es un resumen de los resultados de prediccion sobre un problema de clasificacion. El numero de predicciones correctas e incorrectas se resume con valores de conteo y se desglosa por clase. Esta es la clave de la matriz de confusion. La matriz de confusion muestra las formas en que su modelo de clasificacion se confunde cuando hace predicciones. Le da una idea no solo de los errores que est cometiendo su clasificador, sino ms importante aun, de los tipos de errores que se estn cometiendo. Es este desglose el que supera la limitacion del uso de la precision de clasificacion solo. replace_multiclass(targets) None restore_model(filename) Load the model from disk save_model(filename) Save the model to disk validacion_cruzada(X, Y, n_splits, shuffle=True, scoring='accuracy') Validacion cruzada del dataset introducido como input. Inputs: - cv = Numero de iteraciones. Outputs: - score: Media de los acc de todas las iteraciones.","title":"Methods"},{"location":"user_guide/utils/ParamsManager/","text":"ParamsManager ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes mtodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo Methods export_params(filename) None get_params() None update_params( kwargs) None","title":"ParamsManager"},{"location":"user_guide/utils/ParamsManager/#paramsmanager","text":"ParamsManager(params_file, key_read='info_json') Objeto utilizado para leer los datos de un fichero json de configuracion. Parameters params_file: fichero .json desde el directorio de trabajo. key_read: argumento \"key\" del fichero .json sobre el que queremos actuar. Este objeto nos proporciona los siguientes mtodos: get_params(): obtener parametros a partir del \"key\" especificado update_params(): actualizacion de los valores de un \"key\" especificado. export_params(): Escribir parametros en un archivo","title":"ParamsManager"},{"location":"user_guide/utils/ParamsManager/#methods","text":"export_params(filename) None get_params() None update_params( kwargs) None","title":"Methods"}]}